{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier code for training the datasets\n",
    "\n",
    "## Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['time_step', 'x', 'y', 'z', 's_1_0', 's_1_1', 's_1_2', 's_1_3', 's_1_4',\n",
      "       's_1_5', 's_1_6', 's_1_7', 's_1_8', 's_1_9', 's_1_10', 's_1_11',\n",
      "       's_1_12', 's_1_13', 's_1_14', 's_1_15', 's_2_0', 's_2_1', 's_2_2',\n",
      "       's_2_3', 's_2_4', 's_2_5', 's_2_6', 's_2_7', 's_2_8', 's_2_9', 's_2_10',\n",
      "       's_2_11', 's_2_12', 's_2_13', 's_2_14', 's_2_15'],\n",
      "      dtype='object')\n",
      "11.8\n",
      "GPU: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB#\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import sys\n",
    "#df = pd.read_csv('/its/home/drs25/Documents/GitHub/TactileSensor/Code/Data collection/robot/movementLeftFoot.csv')\n",
    "#df=(df-df.std())/(df.mean())\n",
    "path=\"/its/home/drs25/GonkRobot/Data/raw/\"\n",
    "if sys.platform.startswith('win'):\n",
    "    path=\"C:/Users/dexte/Documents/GitHub/TactileSensor/Code/Data collection/robot/New_feet/raw/\"\n",
    "df = pd.read_csv(path+'carpet_d1.1_raw_L.csv')\n",
    "#df=(df-df.std())/(df.mean())\n",
    "\n",
    "print(df.keys())\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.version.cuda)\n",
    "print(\"GPU:\",torch.cuda.is_available())\n",
    "def get_foot_image(processed): #get the image of an array\n",
    "    grid=np.zeros((3,5))\n",
    "    grid[2][4]=processed[0]\n",
    "    grid[2][3]=processed[1]\n",
    "    grid[2][2]=processed[2]\n",
    "    grid[2][1]=processed[3]\n",
    "    grid[2][0]=processed[4]\n",
    "\n",
    "    grid[1][3]=processed[8]\n",
    "    grid[1][2]=processed[9]\n",
    "    grid[1][1]=processed[6]\n",
    "    grid[1][0]=processed[5]\n",
    "\n",
    "    grid[0][4]=processed[10]\n",
    "    grid[0][3]=processed[11]\n",
    "    grid[0][2]=processed[12]\n",
    "    grid[0][1]=processed[13]\n",
    "    grid[0][0]=processed[14]\n",
    "    grid=(grid-np.min(grid))/(np.max(grid)-np.min(grid))\n",
    "    return grid\n",
    "\n",
    "def genDataSet(var,texture=False,days=True,left=True,right=True):\n",
    "    if right and not left:\n",
    "        X,y=sort_data(\"wood_d1_raw_R.csv\",var)\n",
    "    elif left and not right:\n",
    "        X,y=sort_data(\"wood_d1_raw_L.csv\",var)\n",
    "    if right:\n",
    "        X1,y1=sort_data(\"wood_d2.1_raw_R.csv\",var)\n",
    "        X=np.concatenate((X,X1),axis=0)\n",
    "        y=np.concatenate((y,y1),axis=0)\n",
    "        X1,y1=sort_data(\"wood_d2.3_raw_R.csv\",var)\n",
    "        X=np.concatenate((X,X1),axis=0)\n",
    "        y=np.concatenate((y,y1),axis=0)\n",
    "        X1,y1=sort_data(\"wood_d2_raw_R.csv\",var)\n",
    "        X=np.concatenate((X,X1),axis=0)\n",
    "        y=np.concatenate((y,y1),axis=0)\n",
    "    if left:\n",
    "        if not right:\n",
    "            X1,y1=sort_data(\"wood_d1_raw_L.csv\",var)\n",
    "            X=np.concatenate((X,X1),axis=0)\n",
    "            y=np.concatenate((y,y1),axis=0)\n",
    "        X1,y1=sort_data(\"wood_d2.1_raw_L.csv\",var)\n",
    "        X=np.concatenate((X,X1),axis=0)\n",
    "        y=np.concatenate((y,y1),axis=0)\n",
    "    if texture:\n",
    "        if left:\n",
    "            X1,y1=sort_data(\"carpet_d1.1_raw_L.csv\",var)\n",
    "            X=np.concatenate((X,X1),axis=0)\n",
    "            y=np.concatenate((y,y1),axis=0)\n",
    "            X1,y1=sort_data(\"carpet_d1_raw_L.csv\",var)\n",
    "            X=np.concatenate((X,X1),axis=0)\n",
    "            y=np.concatenate((y,y1),axis=0)\n",
    "            X1,y1=sort_data(\"concrete_d2_raw_L.csv\",var)\n",
    "            X=np.concatenate((X,X1),axis=0)\n",
    "            y=np.concatenate((y,y1),axis=0)\n",
    "            X1,y1=sort_data(\"concrete_d2.1_raw_L.csv\",var)\n",
    "            X=np.concatenate((X,X1),axis=0)\n",
    "            y=np.concatenate((y,y1),axis=0)\n",
    "            X1,y1=sort_data(\"RoughRock_d3_raw_L.csv\",var)\n",
    "            X=np.concatenate((X,X1),axis=0)\n",
    "            y=np.concatenate((y,y1),axis=0)\n",
    "            X1,y1=sort_data(\"Smooth_d4_raw_L.csv\",var)\n",
    "            X=np.concatenate((X,X1),axis=0)\n",
    "            y=np.concatenate((y,y1),axis=0)\n",
    "        if right:\n",
    "            X1,y1=sort_data(\"carpet_d1_raw_R.csv\",var)\n",
    "            X=np.concatenate((X,X1),axis=0)\n",
    "            y=np.concatenate((y,y1),axis=0)\n",
    "            X1,y1=sort_data(\"carpet_d1.1_raw_R.csv\",var)\n",
    "            X=np.concatenate((X,X1),axis=0)\n",
    "            y=np.concatenate((y,y1),axis=0)\n",
    "            X1,y1=sort_data(\"concrete_d2_raw_R.csv\",var)\n",
    "            X=np.concatenate((X,X1),axis=0)\n",
    "            y=np.concatenate((y,y1),axis=0)\n",
    "            X1,y1=sort_data(\"concrete_d2.1_raw_R.csv\",var)\n",
    "            X=np.concatenate((X,X1),axis=0)\n",
    "            y=np.concatenate((y,y1),axis=0)\n",
    "            X1,y1=sort_data(\"RoughRock_d3_raw_R.csv\",var)\n",
    "            X=np.concatenate((X,X1),axis=0)\n",
    "            y=np.concatenate((y,y1),axis=0)\n",
    "            X1,y1=sort_data(\"Smooth_d4_raw_R.csv\",var)\n",
    "            X=np.concatenate((X,X1),axis=0)\n",
    "            y=np.concatenate((y,y1),axis=0)\n",
    "        if days:\n",
    "            if left:\n",
    "                X1,y1=sort_data(\"Carpet_d4_raw_L.csv\",var)\n",
    "                X=np.concatenate((X,X1),axis=0)\n",
    "                y=np.concatenate((y,y1),axis=0)\n",
    "            if right:\n",
    "                X1,y1=sort_data(\"Carpet_d4_raw_R.csv\",var)\n",
    "                X=np.concatenate((X,X1),axis=0)\n",
    "                y=np.concatenate((y,y1),axis=0)\n",
    "    return X,y\n",
    "\n",
    "def sort_data(name,vibration=True,dir=\"all\"):\n",
    "    df = pd.read_csv(path+name)\n",
    "    df=pd.DataFrame(df).fillna(0)\n",
    "    if vibration:\n",
    "        ar=[]\n",
    "        for key in list(df.keys()):\n",
    "            if key not in [\"time_step\",\"x\",\"y\",\"z\"]:\n",
    "                ar.append(df[key])\n",
    "        x=np.array(ar)\n",
    "    else: #return without vibration data\n",
    "        ar=[]\n",
    "        for key in list(df.keys()):\n",
    "            if key not in [\"time_step\",\"x\",\"y\",\"z\",\"s_1_7\",\"s_2_7\"]:\n",
    "                ar.append(df[key])\n",
    "        x=np.array(ar)\n",
    "    if dir==\"left\":\n",
    "        ar=[]\n",
    "        for key in list(df.keys()):\n",
    "            if key not in [\"time_step\",\"x\",\"y\",\"z\"] and \"s_1\" in key:\n",
    "                ar.append(df[key])\n",
    "        x=np.array(ar)\n",
    "    elif dir==\"right\":\n",
    "        ar=[]\n",
    "        for key in list(df.keys()):\n",
    "            if key not in [\"x\",\"y\",\"z\"] and \"s_2\" in key:\n",
    "                ar.append(df[key])\n",
    "        x=np.array(ar)\n",
    "    x=x.T #transpose to have layers\n",
    "    y=np.array([df['x'],df['y'],df['z']])\n",
    "    y=y.T\n",
    "    nan_indices = np.where(np.isnan(y))\n",
    "    y[nan_indices]=0\n",
    "    print(\"X data:\",x.shape,\"/ny data:\",y.shape)\n",
    "    return x,y\n",
    "\n",
    "def getReductionMatrix(filename):\n",
    "    x,y=sort_data(filename,vibration=True,dir=\"all\") #loop through file name\n",
    "    reduction = np.average(x,axis=0)\n",
    "    return reduction\n",
    "\n",
    "def gen_temporal_data(X_,y_,T):\n",
    "    X=X_.copy()\n",
    "    temp_x=np.zeros((X.shape[0]-T,X.shape[1]*T))\n",
    "    temp_y=np.zeros((X.shape[0]-T,y_.shape[1]))\n",
    "    for j in range(len(y_)-T): #loop through classes\n",
    "        ar=[X[j+k] for k in range(T)]\n",
    "        temp_x[j]=np.concatenate(ar,axis=0)\n",
    "        temp_y[j]=y_[j]\n",
    "    x=temp_x\n",
    "    y=temp_y\n",
    "    return x, y\n",
    "\n",
    "def gen_temporal_data_2(X_,y_,T):\n",
    "    X=X_.copy()\n",
    "    temp_x=np.zeros((X.shape[0]-T,T,X.shape[1]))\n",
    "    temp_y=np.zeros((X.shape[0]-T,y_.shape[1]))\n",
    "    for j in range(len(y_)-T): #loop through classes\n",
    "        ar=[X[j+k] for k in range(T)]\n",
    "        temp_x[j]=np.array(ar)\n",
    "        temp_y[j]=y_[j]\n",
    "    x=temp_x\n",
    "    y=temp_y\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def reduction_matrix(x_standard):\n",
    "    return np.average(x_standard,axis=0)\n",
    "\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "\n",
    "        # Create a list to store the layers of the neural network\n",
    "        layers = []\n",
    "\n",
    "        # Add the input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())  # You can use other activation functions as well\n",
    "        #print(layers[0].weight.dtype)\n",
    "        # Add the hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            layers.append(nn.ReLU())  # You can use other activation functions as well\n",
    "\n",
    "        # Add the output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "\n",
    "        # Combine all layers into a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass of the network\n",
    "        return self.model(x)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagation\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def train(X_train,Y_train,X_test,Y_test,batch_size=64,num_epochs = 100,learning_rate = 0.01,acc_check=0):\n",
    "    # Split your dataset into training and validation sets\n",
    "    # train_data, val_data = ...\n",
    "    lstm_model=LSTMModel(X_train.shape[2],500,2,Y_train.shape[1]).to(device)\n",
    "    # Create data loaders for training and validation\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    history_train=[]\n",
    "    history_test=[]\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        lstm_model.train()\n",
    "        total_loss=0\n",
    "        for i in range(0,len(X_train)-batch_size,batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = lstm_model(X_train[i:i+batch_size])\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, Y_train[i:i+batch_size])\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        history_train.append(total_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
    "    return history_train, history_test, lstm_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regress orientation\n",
    "\n",
    "This section takes in a temporal window of tactile data and classifies which texture it is belonging to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (4950, 32) /ny data: (4950, 3)\n",
      "X data: (4950, 32) /ny data: (4950, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (4950, 32) /ny data: (4950, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (4950, 32) /ny data: (4950, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (4950, 32) /ny data: (4950, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (4950, 32) /ny data: (4950, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "X data: (4950, 32) /ny data: (4950, 3)\n",
      "X data: (3600, 32) /ny data: (3600, 3)\n",
      "(17100, 32) (17100, 32) (24300, 32) (8550, 32)\n",
      "(17100, 3) (17100, 3) (24300, 3) (8550, 3)\n"
     ]
    }
   ],
   "source": [
    "var=1\n",
    "X,y=sort_data(\"wood_d1_raw_R.csv\",var)\n",
    "X1,y1=sort_data(\"wood_d1_raw_L.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"wood_d2.1_raw_L.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"wood_d2.1_raw_R.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"wood_d2.3_raw_R.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"wood_d2_raw_R.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "\n",
    "wood_X=X.copy()\n",
    "wood_y=y.copy()\n",
    "\n",
    "X,y=sort_data(\"carpet_d1.1_raw_L.csv\",var)\n",
    "X1,y1=sort_data(\"carpet_d1.1_raw_R.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"carpet_d1_raw_L.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"carpet_d1_raw_R.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "\n",
    "carpet_x=X.copy()\n",
    "carpet_y=y.copy()\n",
    "\n",
    "X,y=sort_data(\"concrete_d2_raw_L.csv\",var)\n",
    "X1,y1=sort_data(\"concrete_d2_raw_R.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"concrete_d2.1_raw_L.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "X1,y1=sort_data(\"concrete_d2.1_raw_R.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "\n",
    "concrete_X=X.copy()\n",
    "concrete_y=y.copy()\n",
    "\n",
    "X,y=sort_data(\"Smooth_d4_raw_L.csv\",var)\n",
    "X1,y1=sort_data(\"Smooth_d4_raw_R.csv\",var)\n",
    "X=np.concatenate((X,X1),axis=0)\n",
    "y=np.concatenate((y,y1),axis=0)\n",
    "\n",
    "smooth_X=X.copy()\n",
    "smooth_y=y.copy()\n",
    "print(concrete_X.shape,carpet_x.shape,wood_X.shape,smooth_X.shape)\n",
    "print(concrete_y.shape,carpet_y.shape,wood_y.shape,smooth_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at differences in readings\n",
    "\n",
    "datasets=[concrete_X.flatten(),carpet_x.flatten(),wood_X.flatten(),smooth_X.flatten()]\n",
    "labels=[\"Rough concrete\",\"Carpet\",\"Wood\",\"Smooth concrete\"]\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylabel('Pressures')\n",
    "bplot = ax.boxplot(datasets,\n",
    "                   patch_artist=True,  # fill with color\n",
    "                   tick_labels=labels)  # will be used to label x-ticks\n",
    "ax.set_title(\"Pressure readings across textures\")        \n",
    "plt.tight_layout()\n",
    "plt.savefig()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forrest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runRF(t=80):\n",
    "    X=np.concatenate([concrete_X,carpet_x,wood_X,smooth_X])\n",
    "    y=np.concatenate([concrete_y,carpet_y,wood_y,smooth_y])\n",
    "\n",
    "    X,y=gen_temporal_data(X,y,t)\n",
    "    print(X.shape,y.shape)\n",
    "    X=(X-np.mean(X))/(np.std(X))\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    rf_classifier = RandomForestRegressor(n_estimators=25)\n",
    "\n",
    "    # Train the classifier on the training data\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    predictions_train = rf_classifier.predict(X_train)\n",
    "    predictions_test = rf_classifier.predict(X_test)\n",
    "\n",
    "    mse1 = mean_squared_error(y_train, predictions_train)\n",
    "    mse2 = mean_squared_error(y_test, predictions_test)\n",
    "    print(\"Train:\",mse1,\"Test:\",mse2)\n",
    "    return mse1,mse2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runRidge(t=80):\n",
    "    X=np.concatenate([concrete_X,carpet_x,wood_X,smooth_X])\n",
    "    y=np.concatenate([concrete_y,carpet_y,wood_y,smooth_y])\n",
    "    X,y=gen_temporal_data(X,y,t)\n",
    "    X=(X-np.mean(X))/(np.std(X))\n",
    "    print(X.shape,y.shape)\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    ridge = Ridge(alpha=1)\n",
    "\n",
    "    # Train the classifier on the training data\n",
    "    ridge.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    predictions_train = ridge.predict(X_train)\n",
    "    predictions_test = ridge.predict(X_test)\n",
    "\n",
    "    mse1 = mean_squared_error(y_train, predictions_train)\n",
    "    mse2 = mean_squared_error(y_test, predictions_test)\n",
    "    print(\"Train:\",mse1,\"Test:\",mse2)\n",
    "    return mse1,mse2\n",
    "#runRidge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runFNN(t=80):\n",
    "    X=np.concatenate([concrete_X,carpet_x,wood_X,smooth_X])\n",
    "    y=np.concatenate([concrete_y,carpet_y,wood_y,smooth_y])\n",
    "    X,y=gen_temporal_data(X,y,t)\n",
    "    X=(X-np.mean(X))/(np.std(X))\n",
    "    y=(y-np.mean(y))/(np.std(y))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train=torch.tensor(X_train,dtype=torch.float32).to(device)\n",
    "    y_train=torch.tensor(y_train,dtype=torch.float32).to(device)\n",
    "    model = SimpleNN(len(X_train[0]), [500,50], 3).to(device)\n",
    "\n",
    "    # Define the loss function and the optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    num_epochs=5000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_train)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, y_train)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Print the average loss for the epoch\n",
    "        average_loss = total_loss \n",
    "        if epoch%100==0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "\n",
    "    predictions_train = model(X_train)\n",
    "    predictions_test = model(torch.tensor(X_test,dtype=torch.float32).to(device))\n",
    "\n",
    "    mse1 = mean_squared_error(y_train.cpu().detach().numpy(), predictions_train.cpu().detach().numpy())\n",
    "    mse2 = mean_squared_error(y_test, predictions_test.cpu().detach().numpy())\n",
    "    print(\"Train:\",mse1,\"Test:\",mse2)\n",
    "    return mse1,mse2\n",
    "#runFNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runLSTM(t=80):\n",
    "    X=np.concatenate([concrete_X,carpet_x,wood_X,smooth_X])\n",
    "    y=np.concatenate([concrete_y,carpet_y,wood_y,smooth_y])\n",
    "    X,y=gen_temporal_data_2(X,y,t)\n",
    "    X=(X-np.mean(X))/(np.std(X))\n",
    "    y=(y-np.mean(y))/(np.std(y))\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train=torch.tensor(X_train,dtype=torch.float32).to(device)\n",
    "    y_train=torch.tensor(y_train,dtype=torch.float32).to(device)\n",
    "    X_test=torch.tensor(X_test,dtype=torch.float32)\n",
    "    y_test=torch.tensor(y_test,dtype=torch.float32)\n",
    "\n",
    "    lossTrain,lossTest,model=train(X_train,y_train,X_test,y_test,num_epochs = 300,learning_rate = 0.01)\n",
    "\n",
    "    plt.plot(lossTrain)\n",
    "    plt.plot(lossTest)\n",
    "    plt.show()\n",
    "\n",
    "    predictions_train = model(X_train)\n",
    "    predictions_test = model(X_test.to(device))\n",
    "\n",
    "    mse1 = mean_squared_error(y_train.cpu().detach().numpy(), predictions_train.cpu().detach().numpy())\n",
    "    mse2 = mean_squared_error(y_test.cpu().detach().numpy(), predictions_test.cpu().detach().numpy())\n",
    "    print(\"Train:\",mse1,\"Test:\",mse2)\n",
    "    return mse1,mse2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T plotting experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67049, 32) (67049, 3)\n",
      "Train: 1.012439659739717 Test: 0.9647149271352875\n",
      "(67047, 96) (67047, 3)\n",
      "Train: 0.9127308672761898 Test: 0.9301178519887445\n",
      "(67045, 160) (67045, 3)\n",
      "Train: 0.8601876395576274 Test: 0.8626858503443758\n",
      "(67043, 224) (67043, 3)\n",
      "Train: 0.8209592083348962 Test: 0.7942078288221648\n",
      "(67041, 288) (67041, 3)\n",
      "Train: 0.77539727929914 Test: 0.805756331666771\n",
      "(67039, 352) (67039, 3)\n",
      "Train: 0.7548088130719693 Test: 0.755453959261982\n",
      "(67037, 416) (67037, 3)\n",
      "Train: 0.7322636333851547 Test: 0.7279043120231742\n",
      "(67035, 480) (67035, 3)\n",
      "Train: 0.7077879454937782 Test: 0.709683378686568\n",
      "(67033, 544) (67033, 3)\n",
      "Train: 0.6854699469124776 Test: 0.699021199227127\n",
      "(67031, 608) (67031, 3)\n",
      "Train: 0.6749237393137705 Test: 0.6575968859763054\n",
      "(67029, 672) (67029, 3)\n",
      "Train: 0.6480347820962182 Test: 0.700527771235668\n",
      "(67027, 736) (67027, 3)\n",
      "Train: 0.6355430420176117 Test: 0.6936527037684854\n",
      "(67025, 800) (67025, 3)\n",
      "Train: 0.6378941892898297 Test: 0.6310650464704591\n",
      "(67023, 864) (67023, 3)\n",
      "Train: 0.615045752384432 Test: 0.667995152409751\n",
      "(67021, 928) (67021, 3)\n",
      "Train: 0.6119230044086067 Test: 0.6217657196450118\n",
      "(67019, 992) (67019, 3)\n",
      "Train: 0.602413168516505 Test: 0.5997077088928088\n",
      "(67017, 1056) (67017, 3)\n",
      "Train: 0.5899487118048926 Test: 0.5933497642964877\n",
      "(67015, 1120) (67015, 3)\n",
      "Train: 0.5750494132679167 Test: 0.5942700741589846\n",
      "(67013, 1184) (67013, 3)\n",
      "Train: 0.5669847026609031 Test: 0.5657005367548984\n",
      "(67011, 1248) (67011, 3)\n",
      "Train: 0.5548098305492012 Test: 0.5573435567283845\n",
      "(67009, 1312) (67009, 3)\n",
      "Train: 0.5342117449216034 Test: 0.5899670124756755\n",
      "(67007, 1376) (67007, 3)\n",
      "Train: 0.5304457163030957 Test: 0.5572144980980194\n",
      "(67005, 1440) (67005, 3)\n",
      "Train: 0.521643902531659 Test: 0.5455748797769285\n",
      "(67003, 1504) (67003, 3)\n",
      "Train: 0.5104883156239465 Test: 0.5484092205438004\n",
      "(67001, 1568) (67001, 3)\n",
      "Train: 0.5018849928140791 Test: 0.5490438337820328\n"
     ]
    }
   ],
   "source": [
    "T=[]\n",
    "for i in range(1,50,2):\n",
    "    T.append(runRidge(t=i))\n",
    "T=np.array(T)\n",
    "np.save(\"/its/home/drs25/GonkRobot/Data/experimentdata/Ridge_T\",T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5000], Loss: 1.2411\n",
      "Epoch [101/5000], Loss: 0.0555\n",
      "Epoch [201/5000], Loss: 0.0449\n",
      "Epoch [301/5000], Loss: 0.0394\n",
      "Epoch [401/5000], Loss: 0.0371\n",
      "Epoch [501/5000], Loss: 0.0359\n",
      "Epoch [601/5000], Loss: 0.0349\n",
      "Epoch [701/5000], Loss: 0.0305\n",
      "Epoch [801/5000], Loss: 0.0290\n",
      "Epoch [901/5000], Loss: 0.0274\n",
      "Epoch [1001/5000], Loss: 0.0275\n",
      "Epoch [1101/5000], Loss: 0.0261\n",
      "Epoch [1201/5000], Loss: 0.0257\n",
      "Epoch [1301/5000], Loss: 0.0257\n",
      "Epoch [1401/5000], Loss: 0.0250\n",
      "Epoch [1501/5000], Loss: 0.0247\n",
      "Epoch [1601/5000], Loss: 0.0206\n",
      "Epoch [1701/5000], Loss: 0.0204\n",
      "Epoch [1801/5000], Loss: 0.0192\n",
      "Epoch [1901/5000], Loss: 0.0187\n",
      "Epoch [2001/5000], Loss: 0.0185\n",
      "Epoch [2101/5000], Loss: 0.0183\n",
      "Epoch [2201/5000], Loss: 0.0179\n",
      "Epoch [2301/5000], Loss: 0.0177\n",
      "Epoch [2401/5000], Loss: 0.0180\n",
      "Epoch [2501/5000], Loss: 0.0171\n",
      "Epoch [2601/5000], Loss: 0.0174\n",
      "Epoch [2701/5000], Loss: 0.0167\n",
      "Epoch [2801/5000], Loss: 0.0164\n",
      "Epoch [2901/5000], Loss: 0.0163\n",
      "Epoch [3001/5000], Loss: 0.0162\n",
      "Epoch [3101/5000], Loss: 0.0157\n",
      "Epoch [3201/5000], Loss: 0.0155\n",
      "Epoch [3301/5000], Loss: 0.0158\n",
      "Epoch [3401/5000], Loss: 0.0152\n",
      "Epoch [3501/5000], Loss: 0.0150\n",
      "Epoch [3601/5000], Loss: 0.0148\n",
      "Epoch [3701/5000], Loss: 0.0145\n",
      "Epoch [3801/5000], Loss: 0.0158\n",
      "Epoch [3901/5000], Loss: 0.0144\n",
      "Epoch [4001/5000], Loss: 0.0141\n",
      "Epoch [4101/5000], Loss: 0.0141\n",
      "Epoch [4201/5000], Loss: 0.0138\n",
      "Epoch [4301/5000], Loss: 0.0145\n",
      "Epoch [4401/5000], Loss: 0.0141\n",
      "Epoch [4501/5000], Loss: 0.0142\n",
      "Epoch [4601/5000], Loss: 0.0134\n",
      "Epoch [4701/5000], Loss: 0.0133\n",
      "Epoch [4801/5000], Loss: 0.0131\n",
      "Epoch [4901/5000], Loss: 0.0131\n",
      "Train: 0.013567321 Test: 0.013467215210254185\n",
      "Epoch [1/5000], Loss: 1.2247\n",
      "Epoch [101/5000], Loss: 0.0555\n",
      "Epoch [201/5000], Loss: 0.0451\n",
      "Epoch [301/5000], Loss: 0.0343\n",
      "Epoch [401/5000], Loss: 0.0287\n",
      "Epoch [501/5000], Loss: 0.0263\n",
      "Epoch [601/5000], Loss: 0.0240\n",
      "Epoch [701/5000], Loss: 0.0221\n",
      "Epoch [801/5000], Loss: 0.0209\n",
      "Epoch [901/5000], Loss: 0.0202\n",
      "Epoch [1001/5000], Loss: 0.0196\n",
      "Epoch [1101/5000], Loss: 0.0191\n",
      "Epoch [1201/5000], Loss: 0.0191\n",
      "Epoch [1301/5000], Loss: 0.0188\n",
      "Epoch [1401/5000], Loss: 0.0187\n",
      "Epoch [1501/5000], Loss: 0.0185\n",
      "Epoch [1601/5000], Loss: 0.0182\n",
      "Epoch [1701/5000], Loss: 0.0176\n",
      "Epoch [1801/5000], Loss: 0.0173\n",
      "Epoch [1901/5000], Loss: 0.0174\n",
      "Epoch [2001/5000], Loss: 0.0177\n",
      "Epoch [2101/5000], Loss: 0.0174\n",
      "Epoch [2201/5000], Loss: 0.0164\n",
      "Epoch [2301/5000], Loss: 0.0166\n",
      "Epoch [2401/5000], Loss: 0.0161\n",
      "Epoch [2501/5000], Loss: 0.0171\n",
      "Epoch [2601/5000], Loss: 0.0158\n",
      "Epoch [2701/5000], Loss: 0.0162\n",
      "Epoch [2801/5000], Loss: 0.0159\n",
      "Epoch [2901/5000], Loss: 0.0153\n",
      "Epoch [3001/5000], Loss: 0.0149\n",
      "Epoch [3101/5000], Loss: 0.0149\n",
      "Epoch [3201/5000], Loss: 0.0152\n",
      "Epoch [3301/5000], Loss: 0.0145\n",
      "Epoch [3401/5000], Loss: 0.0144\n",
      "Epoch [3501/5000], Loss: 0.0173\n",
      "Epoch [3601/5000], Loss: 0.0143\n",
      "Epoch [3701/5000], Loss: 0.0143\n",
      "Epoch [3801/5000], Loss: 0.0141\n",
      "Epoch [3901/5000], Loss: 0.0138\n",
      "Epoch [4001/5000], Loss: 0.0145\n",
      "Epoch [4101/5000], Loss: 0.0138\n",
      "Epoch [4201/5000], Loss: 0.0136\n",
      "Epoch [4301/5000], Loss: 0.0134\n",
      "Epoch [4401/5000], Loss: 0.0141\n",
      "Epoch [4501/5000], Loss: 0.0134\n",
      "Epoch [4601/5000], Loss: 0.0131\n",
      "Epoch [4701/5000], Loss: 0.0132\n",
      "Epoch [4801/5000], Loss: 0.0130\n",
      "Epoch [4901/5000], Loss: 0.0132\n",
      "Train: 0.0128287375 Test: 0.012698947902042977\n",
      "Epoch [1/5000], Loss: 1.2650\n",
      "Epoch [101/5000], Loss: 0.0500\n",
      "Epoch [201/5000], Loss: 0.0402\n",
      "Epoch [301/5000], Loss: 0.0375\n",
      "Epoch [401/5000], Loss: 0.0324\n",
      "Epoch [501/5000], Loss: 0.0290\n",
      "Epoch [601/5000], Loss: 0.0277\n",
      "Epoch [701/5000], Loss: 0.0255\n",
      "Epoch [801/5000], Loss: 0.0246\n",
      "Epoch [901/5000], Loss: 0.0247\n",
      "Epoch [1001/5000], Loss: 0.0232\n",
      "Epoch [1101/5000], Loss: 0.0229\n",
      "Epoch [1201/5000], Loss: 0.0213\n",
      "Epoch [1301/5000], Loss: 0.0208\n",
      "Epoch [1401/5000], Loss: 0.0208\n",
      "Epoch [1501/5000], Loss: 0.0187\n",
      "Epoch [1601/5000], Loss: 0.0181\n",
      "Epoch [1701/5000], Loss: 0.0181\n",
      "Epoch [1801/5000], Loss: 0.0176\n",
      "Epoch [1901/5000], Loss: 0.0174\n",
      "Epoch [2001/5000], Loss: 0.0169\n",
      "Epoch [2101/5000], Loss: 0.0168\n",
      "Epoch [2201/5000], Loss: 0.0164\n",
      "Epoch [2301/5000], Loss: 0.0164\n",
      "Epoch [2401/5000], Loss: 0.0160\n",
      "Epoch [2501/5000], Loss: 0.0156\n",
      "Epoch [2601/5000], Loss: 0.0167\n",
      "Epoch [2701/5000], Loss: 0.0154\n",
      "Epoch [2801/5000], Loss: 0.0152\n",
      "Epoch [2901/5000], Loss: 0.0152\n",
      "Epoch [3001/5000], Loss: 0.0151\n",
      "Epoch [3101/5000], Loss: 0.0148\n",
      "Epoch [3201/5000], Loss: 0.0155\n",
      "Epoch [3301/5000], Loss: 0.0146\n",
      "Epoch [3401/5000], Loss: 0.0152\n",
      "Epoch [3501/5000], Loss: 0.0143\n",
      "Epoch [3601/5000], Loss: 0.0144\n",
      "Epoch [3701/5000], Loss: 0.0150\n",
      "Epoch [3801/5000], Loss: 0.0140\n",
      "Epoch [3901/5000], Loss: 0.0140\n",
      "Epoch [4001/5000], Loss: 0.0139\n",
      "Epoch [4101/5000], Loss: 0.0140\n",
      "Epoch [4201/5000], Loss: 0.0135\n",
      "Epoch [4301/5000], Loss: 0.0133\n",
      "Epoch [4401/5000], Loss: 0.0133\n",
      "Epoch [4501/5000], Loss: 0.0137\n",
      "Epoch [4601/5000], Loss: 0.0136\n",
      "Epoch [4701/5000], Loss: 0.0135\n",
      "Epoch [4801/5000], Loss: 0.0137\n",
      "Epoch [4901/5000], Loss: 0.0129\n",
      "Train: 0.012564373 Test: 0.012618227052919283\n",
      "Epoch [1/5000], Loss: 1.3849\n",
      "Epoch [101/5000], Loss: 0.0702\n",
      "Epoch [201/5000], Loss: 0.0560\n",
      "Epoch [301/5000], Loss: 0.0516\n",
      "Epoch [401/5000], Loss: 0.0463\n",
      "Epoch [501/5000], Loss: 0.0406\n",
      "Epoch [601/5000], Loss: 0.0354\n",
      "Epoch [701/5000], Loss: 0.0352\n",
      "Epoch [801/5000], Loss: 0.0310\n",
      "Epoch [901/5000], Loss: 0.0289\n",
      "Epoch [1001/5000], Loss: 0.0274\n",
      "Epoch [1101/5000], Loss: 0.0264\n",
      "Epoch [1201/5000], Loss: 0.0261\n",
      "Epoch [1301/5000], Loss: 0.0249\n",
      "Epoch [1401/5000], Loss: 0.0231\n",
      "Epoch [1501/5000], Loss: 0.0224\n",
      "Epoch [1601/5000], Loss: 0.0219\n",
      "Epoch [1701/5000], Loss: 0.0212\n",
      "Epoch [1801/5000], Loss: 0.0223\n",
      "Epoch [1901/5000], Loss: 0.0204\n",
      "Epoch [2001/5000], Loss: 0.0212\n",
      "Epoch [2101/5000], Loss: 0.0199\n",
      "Epoch [2201/5000], Loss: 0.0197\n",
      "Epoch [2301/5000], Loss: 0.0189\n",
      "Epoch [2401/5000], Loss: 0.0187\n",
      "Epoch [2501/5000], Loss: 0.0182\n",
      "Epoch [2601/5000], Loss: 0.0179\n",
      "Epoch [2701/5000], Loss: 0.0180\n",
      "Epoch [2801/5000], Loss: 0.0189\n",
      "Epoch [2901/5000], Loss: 0.0174\n",
      "Epoch [3001/5000], Loss: 0.0174\n",
      "Epoch [3101/5000], Loss: 0.0172\n",
      "Epoch [3201/5000], Loss: 0.0170\n",
      "Epoch [3301/5000], Loss: 0.0168\n",
      "Epoch [3401/5000], Loss: 0.0165\n",
      "Epoch [3501/5000], Loss: 0.0194\n",
      "Epoch [3601/5000], Loss: 0.0162\n",
      "Epoch [3701/5000], Loss: 0.0161\n",
      "Epoch [3801/5000], Loss: 0.0165\n",
      "Epoch [3901/5000], Loss: 0.0158\n",
      "Epoch [4001/5000], Loss: 0.0158\n",
      "Epoch [4101/5000], Loss: 0.0156\n",
      "Epoch [4201/5000], Loss: 0.0157\n",
      "Epoch [4301/5000], Loss: 0.0152\n",
      "Epoch [4401/5000], Loss: 0.0150\n",
      "Epoch [4501/5000], Loss: 0.0151\n",
      "Epoch [4601/5000], Loss: 0.0150\n",
      "Epoch [4701/5000], Loss: 0.0159\n",
      "Epoch [4801/5000], Loss: 0.0148\n",
      "Epoch [4901/5000], Loss: 0.0145\n",
      "Train: 0.014349289 Test: 0.013833082686353615\n",
      "Epoch [1/5000], Loss: 1.1644\n",
      "Epoch [101/5000], Loss: 0.0551\n",
      "Epoch [201/5000], Loss: 0.0441\n",
      "Epoch [301/5000], Loss: 0.0399\n",
      "Epoch [401/5000], Loss: 0.0372\n",
      "Epoch [501/5000], Loss: 0.0360\n",
      "Epoch [601/5000], Loss: 0.0353\n",
      "Epoch [701/5000], Loss: 0.0345\n",
      "Epoch [801/5000], Loss: 0.0339\n",
      "Epoch [901/5000], Loss: 0.0335\n",
      "Epoch [1001/5000], Loss: 0.0329\n",
      "Epoch [1101/5000], Loss: 0.0326\n",
      "Epoch [1201/5000], Loss: 0.0269\n",
      "Epoch [1301/5000], Loss: 0.0257\n",
      "Epoch [1401/5000], Loss: 0.0215\n",
      "Epoch [1501/5000], Loss: 0.0211\n",
      "Epoch [1601/5000], Loss: 0.0193\n",
      "Epoch [1701/5000], Loss: 0.0188\n",
      "Epoch [1801/5000], Loss: 0.0185\n",
      "Epoch [1901/5000], Loss: 0.0187\n",
      "Epoch [2001/5000], Loss: 0.0181\n",
      "Epoch [2101/5000], Loss: 0.0178\n",
      "Epoch [2201/5000], Loss: 0.0180\n",
      "Epoch [2301/5000], Loss: 0.0172\n",
      "Epoch [2401/5000], Loss: 0.0190\n",
      "Epoch [2501/5000], Loss: 0.0172\n",
      "Epoch [2601/5000], Loss: 0.0173\n",
      "Epoch [2701/5000], Loss: 0.0166\n",
      "Epoch [2801/5000], Loss: 0.0164\n",
      "Epoch [2901/5000], Loss: 0.0164\n",
      "Epoch [3001/5000], Loss: 0.0166\n",
      "Epoch [3101/5000], Loss: 0.0158\n",
      "Epoch [3201/5000], Loss: 0.0157\n",
      "Epoch [3301/5000], Loss: 0.0157\n",
      "Epoch [3401/5000], Loss: 0.0174\n",
      "Epoch [3501/5000], Loss: 0.0152\n",
      "Epoch [3601/5000], Loss: 0.0150\n",
      "Epoch [3701/5000], Loss: 0.0149\n",
      "Epoch [3801/5000], Loss: 0.0148\n",
      "Epoch [3901/5000], Loss: 0.0146\n",
      "Epoch [4001/5000], Loss: 0.0144\n",
      "Epoch [4101/5000], Loss: 0.0143\n",
      "Epoch [4201/5000], Loss: 0.0146\n",
      "Epoch [4301/5000], Loss: 0.0141\n",
      "Epoch [4401/5000], Loss: 0.0139\n",
      "Epoch [4501/5000], Loss: 0.0138\n",
      "Epoch [4601/5000], Loss: 0.0141\n",
      "Epoch [4701/5000], Loss: 0.0142\n",
      "Epoch [4801/5000], Loss: 0.0135\n",
      "Epoch [4901/5000], Loss: 0.0135\n",
      "Train: 0.01335438 Test: 0.013240342047279181\n",
      "Epoch [1/5000], Loss: 1.1225\n",
      "Epoch [101/5000], Loss: 0.0472\n",
      "Epoch [201/5000], Loss: 0.0377\n",
      "Epoch [301/5000], Loss: 0.0273\n",
      "Epoch [401/5000], Loss: 0.0220\n",
      "Epoch [501/5000], Loss: 0.0198\n",
      "Epoch [601/5000], Loss: 0.0183\n",
      "Epoch [701/5000], Loss: 0.0178\n",
      "Epoch [801/5000], Loss: 0.0165\n",
      "Epoch [901/5000], Loss: 0.0161\n",
      "Epoch [1001/5000], Loss: 0.0156\n",
      "Epoch [1101/5000], Loss: 0.0161\n",
      "Epoch [1201/5000], Loss: 0.0147\n",
      "Epoch [1301/5000], Loss: 0.0144\n",
      "Epoch [1401/5000], Loss: 0.0142\n",
      "Epoch [1501/5000], Loss: 0.0144\n",
      "Epoch [1601/5000], Loss: 0.0137\n",
      "Epoch [1701/5000], Loss: 0.0135\n",
      "Epoch [1801/5000], Loss: 0.0136\n",
      "Epoch [1901/5000], Loss: 0.0131\n",
      "Epoch [2001/5000], Loss: 0.0126\n",
      "Epoch [2101/5000], Loss: 0.0125\n",
      "Epoch [2201/5000], Loss: 0.0133\n",
      "Epoch [2301/5000], Loss: 0.0117\n",
      "Epoch [2401/5000], Loss: 0.0103\n",
      "Epoch [2501/5000], Loss: 0.0105\n",
      "Epoch [2601/5000], Loss: 0.0099\n",
      "Epoch [2701/5000], Loss: 0.0124\n",
      "Epoch [2801/5000], Loss: 0.0096\n",
      "Epoch [2901/5000], Loss: 0.0099\n",
      "Epoch [3001/5000], Loss: 0.0098\n",
      "Epoch [3101/5000], Loss: 0.0095\n",
      "Epoch [3201/5000], Loss: 0.0090\n",
      "Epoch [3301/5000], Loss: 0.0091\n",
      "Epoch [3401/5000], Loss: 0.0088\n",
      "Epoch [3501/5000], Loss: 0.0088\n",
      "Epoch [3601/5000], Loss: 0.0093\n",
      "Epoch [3701/5000], Loss: 0.0094\n",
      "Epoch [3801/5000], Loss: 0.0086\n",
      "Epoch [3901/5000], Loss: 0.0087\n",
      "Epoch [4001/5000], Loss: 0.0083\n",
      "Epoch [4101/5000], Loss: 0.0083\n",
      "Epoch [4201/5000], Loss: 0.0086\n",
      "Epoch [4301/5000], Loss: 0.0086\n",
      "Epoch [4401/5000], Loss: 0.0079\n",
      "Epoch [4501/5000], Loss: 0.0079\n",
      "Epoch [4601/5000], Loss: 0.0082\n",
      "Epoch [4701/5000], Loss: 0.0079\n",
      "Epoch [4801/5000], Loss: 0.0081\n",
      "Epoch [4901/5000], Loss: 0.0075\n",
      "Train: 0.0077190935 Test: 0.009323331160377856\n",
      "Epoch [1/5000], Loss: 1.0098\n",
      "Epoch [101/5000], Loss: 0.0393\n",
      "Epoch [201/5000], Loss: 0.0286\n",
      "Epoch [301/5000], Loss: 0.0235\n",
      "Epoch [401/5000], Loss: 0.0206\n",
      "Epoch [501/5000], Loss: 0.0185\n",
      "Epoch [601/5000], Loss: 0.0171\n",
      "Epoch [701/5000], Loss: 0.0158\n",
      "Epoch [801/5000], Loss: 0.0154\n",
      "Epoch [901/5000], Loss: 0.0150\n",
      "Epoch [1001/5000], Loss: 0.0137\n",
      "Epoch [1101/5000], Loss: 0.0134\n",
      "Epoch [1201/5000], Loss: 0.0129\n",
      "Epoch [1301/5000], Loss: 0.0124\n",
      "Epoch [1401/5000], Loss: 0.0125\n",
      "Epoch [1501/5000], Loss: 0.0125\n",
      "Epoch [1601/5000], Loss: 0.0115\n",
      "Epoch [1701/5000], Loss: 0.0113\n",
      "Epoch [1801/5000], Loss: 0.0114\n",
      "Epoch [1901/5000], Loss: 0.0109\n",
      "Epoch [2001/5000], Loss: 0.0104\n",
      "Epoch [2101/5000], Loss: 0.0107\n",
      "Epoch [2201/5000], Loss: 0.0110\n",
      "Epoch [2301/5000], Loss: 0.0098\n",
      "Epoch [2401/5000], Loss: 0.0107\n",
      "Epoch [2501/5000], Loss: 0.0096\n",
      "Epoch [2601/5000], Loss: 0.0097\n",
      "Epoch [2701/5000], Loss: 0.0098\n",
      "Epoch [2801/5000], Loss: 0.0092\n",
      "Epoch [2901/5000], Loss: 0.0092\n",
      "Epoch [3001/5000], Loss: 0.0092\n",
      "Epoch [3101/5000], Loss: 0.0088\n",
      "Epoch [3201/5000], Loss: 0.0087\n",
      "Epoch [3301/5000], Loss: 0.0089\n",
      "Epoch [3401/5000], Loss: 0.0084\n",
      "Epoch [3501/5000], Loss: 0.0086\n",
      "Epoch [3601/5000], Loss: 0.0083\n",
      "Epoch [3701/5000], Loss: 0.0082\n",
      "Epoch [3801/5000], Loss: 0.0082\n",
      "Epoch [3901/5000], Loss: 0.0081\n",
      "Epoch [4001/5000], Loss: 0.0080\n",
      "Epoch [4101/5000], Loss: 0.0080\n",
      "Epoch [4201/5000], Loss: 0.0078\n",
      "Epoch [4301/5000], Loss: 0.0077\n",
      "Epoch [4401/5000], Loss: 0.0076\n",
      "Epoch [4501/5000], Loss: 0.0079\n",
      "Epoch [4601/5000], Loss: 0.0078\n",
      "Epoch [4701/5000], Loss: 0.0076\n",
      "Epoch [4801/5000], Loss: 0.0082\n",
      "Epoch [4901/5000], Loss: 0.0073\n",
      "Train: 0.008003135 Test: 0.009705329591774454\n",
      "Epoch [1/5000], Loss: 0.7670\n",
      "Epoch [101/5000], Loss: 0.0375\n",
      "Epoch [201/5000], Loss: 0.0257\n",
      "Epoch [301/5000], Loss: 0.0212\n",
      "Epoch [401/5000], Loss: 0.0185\n",
      "Epoch [501/5000], Loss: 0.0171\n",
      "Epoch [601/5000], Loss: 0.0158\n",
      "Epoch [701/5000], Loss: 0.0149\n",
      "Epoch [801/5000], Loss: 0.0146\n",
      "Epoch [901/5000], Loss: 0.0137\n",
      "Epoch [1001/5000], Loss: 0.0138\n",
      "Epoch [1101/5000], Loss: 0.0132\n",
      "Epoch [1201/5000], Loss: 0.0123\n",
      "Epoch [1301/5000], Loss: 0.0119\n",
      "Epoch [1401/5000], Loss: 0.0118\n",
      "Epoch [1501/5000], Loss: 0.0116\n",
      "Epoch [1601/5000], Loss: 0.0110\n",
      "Epoch [1701/5000], Loss: 0.0121\n",
      "Epoch [1801/5000], Loss: 0.0107\n",
      "Epoch [1901/5000], Loss: 0.0103\n",
      "Epoch [2001/5000], Loss: 0.0102\n",
      "Epoch [2101/5000], Loss: 0.0102\n",
      "Epoch [2201/5000], Loss: 0.0098\n",
      "Epoch [2301/5000], Loss: 0.0122\n",
      "Epoch [2401/5000], Loss: 0.0096\n",
      "Epoch [2501/5000], Loss: 0.0093\n",
      "Epoch [2601/5000], Loss: 0.0092\n",
      "Epoch [2701/5000], Loss: 0.0091\n",
      "Epoch [2801/5000], Loss: 0.0090\n",
      "Epoch [2901/5000], Loss: 0.0090\n",
      "Epoch [3001/5000], Loss: 0.0096\n",
      "Epoch [3101/5000], Loss: 0.0087\n",
      "Epoch [3201/5000], Loss: 0.0089\n",
      "Epoch [3301/5000], Loss: 0.0094\n",
      "Epoch [3401/5000], Loss: 0.0083\n",
      "Epoch [3501/5000], Loss: 0.0086\n",
      "Epoch [3601/5000], Loss: 0.0080\n",
      "Epoch [3701/5000], Loss: 0.0088\n",
      "Epoch [3801/5000], Loss: 0.0080\n",
      "Epoch [3901/5000], Loss: 0.0077\n",
      "Epoch [4001/5000], Loss: 0.0077\n",
      "Epoch [4101/5000], Loss: 0.0078\n",
      "Epoch [4201/5000], Loss: 0.0076\n",
      "Epoch [4301/5000], Loss: 0.0083\n",
      "Epoch [4401/5000], Loss: 0.0077\n",
      "Epoch [4501/5000], Loss: 0.0083\n",
      "Epoch [4601/5000], Loss: 0.0078\n",
      "Epoch [4701/5000], Loss: 0.0071\n",
      "Epoch [4801/5000], Loss: 0.0075\n",
      "Epoch [4901/5000], Loss: 0.0071\n",
      "Train: 0.0070568384 Test: 0.008799930846014966\n",
      "Epoch [1/5000], Loss: 1.9371\n",
      "Epoch [101/5000], Loss: 0.0508\n",
      "Epoch [201/5000], Loss: 0.0379\n",
      "Epoch [301/5000], Loss: 0.0337\n",
      "Epoch [401/5000], Loss: 0.0313\n",
      "Epoch [501/5000], Loss: 0.0299\n",
      "Epoch [601/5000], Loss: 0.0228\n",
      "Epoch [701/5000], Loss: 0.0212\n",
      "Epoch [801/5000], Loss: 0.0191\n",
      "Epoch [901/5000], Loss: 0.0183\n",
      "Epoch [1001/5000], Loss: 0.0179\n",
      "Epoch [1101/5000], Loss: 0.0175\n",
      "Epoch [1201/5000], Loss: 0.0195\n",
      "Epoch [1301/5000], Loss: 0.0165\n",
      "Epoch [1401/5000], Loss: 0.0167\n",
      "Epoch [1501/5000], Loss: 0.0153\n",
      "Epoch [1601/5000], Loss: 0.0146\n",
      "Epoch [1701/5000], Loss: 0.0127\n",
      "Epoch [1801/5000], Loss: 0.0125\n",
      "Epoch [1901/5000], Loss: 0.0123\n",
      "Epoch [2001/5000], Loss: 0.0123\n",
      "Epoch [2101/5000], Loss: 0.0114\n",
      "Epoch [2201/5000], Loss: 0.0118\n",
      "Epoch [2301/5000], Loss: 0.0118\n",
      "Epoch [2401/5000], Loss: 0.0115\n",
      "Epoch [2501/5000], Loss: 0.0113\n",
      "Epoch [2601/5000], Loss: 0.0116\n",
      "Epoch [2701/5000], Loss: 0.0109\n",
      "Epoch [2801/5000], Loss: 0.0102\n",
      "Epoch [2901/5000], Loss: 0.0106\n",
      "Epoch [3001/5000], Loss: 0.0100\n",
      "Epoch [3101/5000], Loss: 0.0098\n",
      "Epoch [3201/5000], Loss: 0.0095\n",
      "Epoch [3301/5000], Loss: 0.0094\n",
      "Epoch [3401/5000], Loss: 0.0111\n",
      "Epoch [3501/5000], Loss: 0.0097\n",
      "Epoch [3601/5000], Loss: 0.0101\n",
      "Epoch [3701/5000], Loss: 0.0090\n",
      "Epoch [3801/5000], Loss: 0.0093\n",
      "Epoch [3901/5000], Loss: 0.0086\n",
      "Epoch [4001/5000], Loss: 0.0086\n",
      "Epoch [4101/5000], Loss: 0.0085\n",
      "Epoch [4201/5000], Loss: 0.0088\n",
      "Epoch [4301/5000], Loss: 0.0092\n",
      "Epoch [4401/5000], Loss: 0.0088\n",
      "Epoch [4501/5000], Loss: 0.0081\n",
      "Epoch [4601/5000], Loss: 0.0083\n",
      "Epoch [4701/5000], Loss: 0.0084\n",
      "Epoch [4801/5000], Loss: 0.0090\n",
      "Epoch [4901/5000], Loss: 0.0093\n",
      "Train: 0.009072027 Test: 0.011100859784889205\n",
      "Epoch [1/5000], Loss: 0.7638\n",
      "Epoch [101/5000], Loss: 0.0481\n",
      "Epoch [201/5000], Loss: 0.0307\n",
      "Epoch [301/5000], Loss: 0.0238\n",
      "Epoch [401/5000], Loss: 0.0205\n",
      "Epoch [501/5000], Loss: 0.0189\n",
      "Epoch [601/5000], Loss: 0.0179\n",
      "Epoch [701/5000], Loss: 0.0168\n",
      "Epoch [801/5000], Loss: 0.0151\n",
      "Epoch [901/5000], Loss: 0.0139\n",
      "Epoch [1001/5000], Loss: 0.0133\n",
      "Epoch [1101/5000], Loss: 0.0131\n",
      "Epoch [1201/5000], Loss: 0.0124\n",
      "Epoch [1301/5000], Loss: 0.0130\n",
      "Epoch [1401/5000], Loss: 0.0117\n",
      "Epoch [1501/5000], Loss: 0.0113\n",
      "Epoch [1601/5000], Loss: 0.0110\n",
      "Epoch [1701/5000], Loss: 0.0107\n",
      "Epoch [1801/5000], Loss: 0.0108\n",
      "Epoch [1901/5000], Loss: 0.0111\n",
      "Epoch [2001/5000], Loss: 0.0110\n",
      "Epoch [2101/5000], Loss: 0.0102\n",
      "Epoch [2201/5000], Loss: 0.0099\n",
      "Epoch [2301/5000], Loss: 0.0096\n",
      "Epoch [2401/5000], Loss: 0.0099\n",
      "Epoch [2501/5000], Loss: 0.0092\n",
      "Epoch [2601/5000], Loss: 0.0092\n",
      "Epoch [2701/5000], Loss: 0.0091\n",
      "Epoch [2801/5000], Loss: 0.0087\n",
      "Epoch [2901/5000], Loss: 0.0087\n",
      "Epoch [3001/5000], Loss: 0.0087\n",
      "Epoch [3101/5000], Loss: 0.0087\n",
      "Epoch [3201/5000], Loss: 0.0084\n",
      "Epoch [3301/5000], Loss: 0.0081\n",
      "Epoch [3401/5000], Loss: 0.0082\n",
      "Epoch [3501/5000], Loss: 0.0084\n",
      "Epoch [3601/5000], Loss: 0.0095\n",
      "Epoch [3701/5000], Loss: 0.0088\n",
      "Epoch [3801/5000], Loss: 0.0087\n",
      "Epoch [3901/5000], Loss: 0.0075\n",
      "Epoch [4001/5000], Loss: 0.0076\n",
      "Epoch [4101/5000], Loss: 0.0075\n",
      "Epoch [4201/5000], Loss: 0.0076\n",
      "Epoch [4301/5000], Loss: 0.0080\n",
      "Epoch [4401/5000], Loss: 0.0073\n",
      "Epoch [4501/5000], Loss: 0.0072\n",
      "Epoch [4601/5000], Loss: 0.0073\n",
      "Epoch [4701/5000], Loss: 0.0070\n",
      "Epoch [4801/5000], Loss: 0.0070\n",
      "Epoch [4901/5000], Loss: 0.0074\n",
      "Train: 0.0067575243 Test: 0.008625696385092363\n",
      "Epoch [1/5000], Loss: 1.2673\n",
      "Epoch [101/5000], Loss: 0.0781\n",
      "Epoch [201/5000], Loss: 0.0550\n",
      "Epoch [301/5000], Loss: 0.0463\n",
      "Epoch [401/5000], Loss: 0.0415\n",
      "Epoch [501/5000], Loss: 0.0392\n",
      "Epoch [601/5000], Loss: 0.0341\n",
      "Epoch [701/5000], Loss: 0.0321\n",
      "Epoch [801/5000], Loss: 0.0320\n",
      "Epoch [901/5000], Loss: 0.0312\n",
      "Epoch [1001/5000], Loss: 0.0289\n",
      "Epoch [1101/5000], Loss: 0.0285\n",
      "Epoch [1201/5000], Loss: 0.0287\n",
      "Epoch [1301/5000], Loss: 0.0279\n",
      "Epoch [1401/5000], Loss: 0.0295\n",
      "Epoch [1501/5000], Loss: 0.0226\n",
      "Epoch [1601/5000], Loss: 0.0196\n",
      "Epoch [1701/5000], Loss: 0.0191\n",
      "Epoch [1801/5000], Loss: 0.0165\n",
      "Epoch [1901/5000], Loss: 0.0149\n",
      "Epoch [2001/5000], Loss: 0.0144\n",
      "Epoch [2101/5000], Loss: 0.0142\n",
      "Epoch [2201/5000], Loss: 0.0135\n",
      "Epoch [2301/5000], Loss: 0.0144\n",
      "Epoch [2401/5000], Loss: 0.0127\n",
      "Epoch [2501/5000], Loss: 0.0133\n",
      "Epoch [2601/5000], Loss: 0.0127\n",
      "Epoch [2701/5000], Loss: 0.0123\n",
      "Epoch [2801/5000], Loss: 0.0120\n",
      "Epoch [2901/5000], Loss: 0.0121\n",
      "Epoch [3001/5000], Loss: 0.0129\n",
      "Epoch [3101/5000], Loss: 0.0120\n",
      "Epoch [3201/5000], Loss: 0.0139\n",
      "Epoch [3301/5000], Loss: 0.0116\n",
      "Epoch [3401/5000], Loss: 0.0115\n",
      "Epoch [3501/5000], Loss: 0.0110\n",
      "Epoch [3601/5000], Loss: 0.0110\n",
      "Epoch [3701/5000], Loss: 0.0109\n",
      "Epoch [3801/5000], Loss: 0.0109\n",
      "Epoch [3901/5000], Loss: 0.0111\n",
      "Epoch [4001/5000], Loss: 0.0118\n",
      "Epoch [4101/5000], Loss: 0.0113\n",
      "Epoch [4201/5000], Loss: 0.0106\n",
      "Epoch [4301/5000], Loss: 0.0109\n",
      "Epoch [4401/5000], Loss: 0.0103\n",
      "Epoch [4501/5000], Loss: 0.0102\n",
      "Epoch [4601/5000], Loss: 0.0113\n",
      "Epoch [4701/5000], Loss: 0.0121\n",
      "Epoch [4801/5000], Loss: 0.0101\n",
      "Epoch [4901/5000], Loss: 0.0099\n",
      "Train: 0.009955546 Test: 0.011713057463103027\n",
      "Epoch [1/5000], Loss: 0.7461\n",
      "Epoch [101/5000], Loss: 0.0358\n",
      "Epoch [201/5000], Loss: 0.0247\n",
      "Epoch [301/5000], Loss: 0.0204\n",
      "Epoch [401/5000], Loss: 0.0180\n",
      "Epoch [501/5000], Loss: 0.0160\n",
      "Epoch [601/5000], Loss: 0.0142\n",
      "Epoch [701/5000], Loss: 0.0129\n",
      "Epoch [801/5000], Loss: 0.0121\n",
      "Epoch [901/5000], Loss: 0.0116\n",
      "Epoch [1001/5000], Loss: 0.0112\n",
      "Epoch [1101/5000], Loss: 0.0111\n",
      "Epoch [1201/5000], Loss: 0.0104\n",
      "Epoch [1301/5000], Loss: 0.0106\n",
      "Epoch [1401/5000], Loss: 0.0111\n",
      "Epoch [1501/5000], Loss: 0.0101\n",
      "Epoch [1601/5000], Loss: 0.0094\n",
      "Epoch [1701/5000], Loss: 0.0095\n",
      "Epoch [1801/5000], Loss: 0.0089\n",
      "Epoch [1901/5000], Loss: 0.0088\n",
      "Epoch [2001/5000], Loss: 0.0090\n",
      "Epoch [2101/5000], Loss: 0.0088\n",
      "Epoch [2201/5000], Loss: 0.0088\n",
      "Epoch [2301/5000], Loss: 0.0084\n",
      "Epoch [2401/5000], Loss: 0.0085\n",
      "Epoch [2501/5000], Loss: 0.0078\n",
      "Epoch [2601/5000], Loss: 0.0077\n",
      "Epoch [2701/5000], Loss: 0.0078\n",
      "Epoch [2801/5000], Loss: 0.0076\n",
      "Epoch [2901/5000], Loss: 0.0076\n",
      "Epoch [3001/5000], Loss: 0.0072\n",
      "Epoch [3101/5000], Loss: 0.0071\n",
      "Epoch [3201/5000], Loss: 0.0084\n",
      "Epoch [3301/5000], Loss: 0.0080\n",
      "Epoch [3401/5000], Loss: 0.0071\n",
      "Epoch [3501/5000], Loss: 0.0073\n",
      "Epoch [3601/5000], Loss: 0.0099\n",
      "Epoch [3701/5000], Loss: 0.0074\n",
      "Epoch [3801/5000], Loss: 0.0071\n",
      "Epoch [3901/5000], Loss: 0.0065\n",
      "Epoch [4001/5000], Loss: 0.0069\n",
      "Epoch [4101/5000], Loss: 0.0066\n",
      "Epoch [4201/5000], Loss: 0.0069\n",
      "Epoch [4301/5000], Loss: 0.0066\n",
      "Epoch [4401/5000], Loss: 0.0071\n",
      "Epoch [4501/5000], Loss: 0.0067\n",
      "Epoch [4601/5000], Loss: 0.0067\n",
      "Epoch [4701/5000], Loss: 0.0069\n",
      "Epoch [4801/5000], Loss: 0.0061\n",
      "Epoch [4901/5000], Loss: 0.0059\n",
      "Train: 0.0067025535 Test: 0.008971618904071403\n",
      "Epoch [1/5000], Loss: 1.0151\n",
      "Epoch [101/5000], Loss: 0.0374\n",
      "Epoch [201/5000], Loss: 0.0256\n",
      "Epoch [301/5000], Loss: 0.0202\n",
      "Epoch [401/5000], Loss: 0.0175\n",
      "Epoch [501/5000], Loss: 0.0161\n",
      "Epoch [601/5000], Loss: 0.0180\n",
      "Epoch [701/5000], Loss: 0.0146\n",
      "Epoch [801/5000], Loss: 0.0142\n",
      "Epoch [901/5000], Loss: 0.0137\n",
      "Epoch [1001/5000], Loss: 0.0133\n",
      "Epoch [1101/5000], Loss: 0.0129\n",
      "Epoch [1201/5000], Loss: 0.0125\n",
      "Epoch [1301/5000], Loss: 0.0122\n",
      "Epoch [1401/5000], Loss: 0.0120\n",
      "Epoch [1501/5000], Loss: 0.0118\n",
      "Epoch [1601/5000], Loss: 0.0115\n",
      "Epoch [1701/5000], Loss: 0.0113\n",
      "Epoch [1801/5000], Loss: 0.0112\n",
      "Epoch [1901/5000], Loss: 0.0109\n",
      "Epoch [2001/5000], Loss: 0.0107\n",
      "Epoch [2101/5000], Loss: 0.0116\n",
      "Epoch [2201/5000], Loss: 0.0107\n",
      "Epoch [2301/5000], Loss: 0.0104\n",
      "Epoch [2401/5000], Loss: 0.0106\n",
      "Epoch [2501/5000], Loss: 0.0101\n",
      "Epoch [2601/5000], Loss: 0.0101\n",
      "Epoch [2701/5000], Loss: 0.0101\n",
      "Epoch [2801/5000], Loss: 0.0098\n",
      "Epoch [2901/5000], Loss: 0.0095\n",
      "Epoch [3001/5000], Loss: 0.0094\n",
      "Epoch [3101/5000], Loss: 0.0092\n",
      "Epoch [3201/5000], Loss: 0.0092\n",
      "Epoch [3301/5000], Loss: 0.0123\n",
      "Epoch [3401/5000], Loss: 0.0094\n",
      "Epoch [3501/5000], Loss: 0.0079\n",
      "Epoch [3601/5000], Loss: 0.0072\n",
      "Epoch [3701/5000], Loss: 0.0073\n",
      "Epoch [3801/5000], Loss: 0.0068\n",
      "Epoch [3901/5000], Loss: 0.0070\n",
      "Epoch [4001/5000], Loss: 0.0070\n",
      "Epoch [4101/5000], Loss: 0.0065\n",
      "Epoch [4201/5000], Loss: 0.0066\n",
      "Epoch [4301/5000], Loss: 0.0066\n",
      "Epoch [4401/5000], Loss: 0.0064\n",
      "Epoch [4501/5000], Loss: 0.0063\n",
      "Epoch [4601/5000], Loss: 0.0086\n",
      "Epoch [4701/5000], Loss: 0.0067\n",
      "Epoch [4801/5000], Loss: 0.0074\n",
      "Epoch [4901/5000], Loss: 0.0069\n",
      "Train: 0.0072168447 Test: 0.009321684697322538\n",
      "Epoch [1/5000], Loss: 0.8541\n",
      "Epoch [101/5000], Loss: 0.0345\n",
      "Epoch [201/5000], Loss: 0.0249\n",
      "Epoch [301/5000], Loss: 0.0207\n",
      "Epoch [401/5000], Loss: 0.0184\n",
      "Epoch [501/5000], Loss: 0.0169\n",
      "Epoch [601/5000], Loss: 0.0158\n",
      "Epoch [701/5000], Loss: 0.0153\n",
      "Epoch [801/5000], Loss: 0.0142\n",
      "Epoch [901/5000], Loss: 0.0142\n",
      "Epoch [1001/5000], Loss: 0.0133\n",
      "Epoch [1101/5000], Loss: 0.0129\n",
      "Epoch [1201/5000], Loss: 0.0126\n",
      "Epoch [1301/5000], Loss: 0.0122\n",
      "Epoch [1401/5000], Loss: 0.0120\n",
      "Epoch [1501/5000], Loss: 0.0117\n",
      "Epoch [1601/5000], Loss: 0.0114\n",
      "Epoch [1701/5000], Loss: 0.0113\n",
      "Epoch [1801/5000], Loss: 0.0135\n",
      "Epoch [1901/5000], Loss: 0.0138\n",
      "Epoch [2001/5000], Loss: 0.0108\n",
      "Epoch [2101/5000], Loss: 0.0106\n",
      "Epoch [2201/5000], Loss: 0.0105\n",
      "Epoch [2301/5000], Loss: 0.0107\n",
      "Epoch [2401/5000], Loss: 0.0104\n",
      "Epoch [2501/5000], Loss: 0.0136\n",
      "Epoch [2601/5000], Loss: 0.0101\n",
      "Epoch [2701/5000], Loss: 0.0100\n",
      "Epoch [2801/5000], Loss: 0.0100\n",
      "Epoch [2901/5000], Loss: 0.0099\n",
      "Epoch [3001/5000], Loss: 0.0097\n",
      "Epoch [3101/5000], Loss: 0.0115\n",
      "Epoch [3201/5000], Loss: 0.0089\n",
      "Epoch [3301/5000], Loss: 0.0078\n",
      "Epoch [3401/5000], Loss: 0.0088\n",
      "Epoch [3501/5000], Loss: 0.0075\n",
      "Epoch [3601/5000], Loss: 0.0074\n",
      "Epoch [3701/5000], Loss: 0.0074\n",
      "Epoch [3801/5000], Loss: 0.0091\n",
      "Epoch [3901/5000], Loss: 0.0071\n",
      "Epoch [4001/5000], Loss: 0.0071\n",
      "Epoch [4101/5000], Loss: 0.0076\n",
      "Epoch [4201/5000], Loss: 0.0072\n",
      "Epoch [4301/5000], Loss: 0.0067\n",
      "Epoch [4401/5000], Loss: 0.0069\n",
      "Epoch [4501/5000], Loss: 0.0066\n",
      "Epoch [4601/5000], Loss: 0.0071\n",
      "Epoch [4701/5000], Loss: 0.0066\n",
      "Epoch [4801/5000], Loss: 0.0066\n",
      "Epoch [4901/5000], Loss: 0.0064\n",
      "Train: 0.0064286846 Test: 0.00812562783862046\n",
      "Epoch [1/5000], Loss: 1.2178\n",
      "Epoch [101/5000], Loss: 0.0530\n",
      "Epoch [201/5000], Loss: 0.0352\n",
      "Epoch [301/5000], Loss: 0.0311\n",
      "Epoch [401/5000], Loss: 0.0290\n",
      "Epoch [501/5000], Loss: 0.0284\n",
      "Epoch [601/5000], Loss: 0.0272\n",
      "Epoch [701/5000], Loss: 0.0268\n",
      "Epoch [801/5000], Loss: 0.0262\n",
      "Epoch [901/5000], Loss: 0.0269\n",
      "Epoch [1001/5000], Loss: 0.0258\n",
      "Epoch [1101/5000], Loss: 0.0255\n",
      "Epoch [1201/5000], Loss: 0.0253\n",
      "Epoch [1301/5000], Loss: 0.0196\n",
      "Epoch [1401/5000], Loss: 0.0190\n",
      "Epoch [1501/5000], Loss: 0.0188\n",
      "Epoch [1601/5000], Loss: 0.0187\n",
      "Epoch [1701/5000], Loss: 0.0185\n",
      "Epoch [1801/5000], Loss: 0.0180\n",
      "Epoch [1901/5000], Loss: 0.0140\n",
      "Epoch [2001/5000], Loss: 0.0143\n",
      "Epoch [2101/5000], Loss: 0.0142\n",
      "Epoch [2201/5000], Loss: 0.0135\n",
      "Epoch [2301/5000], Loss: 0.0126\n",
      "Epoch [2401/5000], Loss: 0.0122\n",
      "Epoch [2501/5000], Loss: 0.0127\n",
      "Epoch [2601/5000], Loss: 0.0131\n",
      "Epoch [2701/5000], Loss: 0.0121\n",
      "Epoch [2801/5000], Loss: 0.0097\n",
      "Epoch [2901/5000], Loss: 0.0093\n",
      "Epoch [3001/5000], Loss: 0.0092\n",
      "Epoch [3101/5000], Loss: 0.0089\n",
      "Epoch [3201/5000], Loss: 0.0083\n",
      "Epoch [3301/5000], Loss: 0.0084\n",
      "Epoch [3401/5000], Loss: 0.0093\n",
      "Epoch [3501/5000], Loss: 0.0079\n",
      "Epoch [3601/5000], Loss: 0.0085\n",
      "Epoch [3701/5000], Loss: 0.0081\n",
      "Epoch [3801/5000], Loss: 0.0078\n",
      "Epoch [3901/5000], Loss: 0.0075\n",
      "Epoch [4001/5000], Loss: 0.0083\n",
      "Epoch [4101/5000], Loss: 0.0087\n",
      "Epoch [4201/5000], Loss: 0.0083\n",
      "Epoch [4301/5000], Loss: 0.0073\n",
      "Epoch [4401/5000], Loss: 0.0068\n",
      "Epoch [4501/5000], Loss: 0.0071\n",
      "Epoch [4601/5000], Loss: 0.0072\n",
      "Epoch [4701/5000], Loss: 0.0067\n",
      "Epoch [4801/5000], Loss: 0.0065\n",
      "Epoch [4901/5000], Loss: 0.0064\n",
      "Train: 0.00629586 Test: 0.008032874786672814\n",
      "Epoch [1/5000], Loss: 1.1217\n",
      "Epoch [101/5000], Loss: 0.0445\n",
      "Epoch [201/5000], Loss: 0.0283\n",
      "Epoch [301/5000], Loss: 0.0232\n",
      "Epoch [401/5000], Loss: 0.0194\n",
      "Epoch [501/5000], Loss: 0.0180\n",
      "Epoch [601/5000], Loss: 0.0169\n",
      "Epoch [701/5000], Loss: 0.0177\n",
      "Epoch [801/5000], Loss: 0.0159\n",
      "Epoch [901/5000], Loss: 0.0153\n",
      "Epoch [1001/5000], Loss: 0.0142\n",
      "Epoch [1101/5000], Loss: 0.0134\n",
      "Epoch [1201/5000], Loss: 0.0127\n",
      "Epoch [1301/5000], Loss: 0.0117\n",
      "Epoch [1401/5000], Loss: 0.0115\n",
      "Epoch [1501/5000], Loss: 0.0111\n",
      "Epoch [1601/5000], Loss: 0.0108\n",
      "Epoch [1701/5000], Loss: 0.0107\n",
      "Epoch [1801/5000], Loss: 0.0101\n",
      "Epoch [1901/5000], Loss: 0.0106\n",
      "Epoch [2001/5000], Loss: 0.0097\n",
      "Epoch [2101/5000], Loss: 0.0098\n",
      "Epoch [2201/5000], Loss: 0.0095\n",
      "Epoch [2301/5000], Loss: 0.0097\n",
      "Epoch [2401/5000], Loss: 0.0093\n",
      "Epoch [2501/5000], Loss: 0.0084\n",
      "Epoch [2601/5000], Loss: 0.0086\n",
      "Epoch [2701/5000], Loss: 0.0086\n",
      "Epoch [2801/5000], Loss: 0.0086\n",
      "Epoch [2901/5000], Loss: 0.0080\n",
      "Epoch [3001/5000], Loss: 0.0080\n",
      "Epoch [3101/5000], Loss: 0.0091\n",
      "Epoch [3201/5000], Loss: 0.0076\n",
      "Epoch [3301/5000], Loss: 0.0079\n",
      "Epoch [3401/5000], Loss: 0.0076\n",
      "Epoch [3501/5000], Loss: 0.0079\n",
      "Epoch [3601/5000], Loss: 0.0071\n",
      "Epoch [3701/5000], Loss: 0.0077\n",
      "Epoch [3801/5000], Loss: 0.0070\n",
      "Epoch [3901/5000], Loss: 0.0065\n",
      "Epoch [4001/5000], Loss: 0.0072\n",
      "Epoch [4101/5000], Loss: 0.0076\n",
      "Epoch [4201/5000], Loss: 0.0066\n",
      "Epoch [4301/5000], Loss: 0.0068\n",
      "Epoch [4401/5000], Loss: 0.0064\n",
      "Epoch [4501/5000], Loss: 0.0065\n",
      "Epoch [4601/5000], Loss: 0.0067\n",
      "Epoch [4701/5000], Loss: 0.0066\n",
      "Epoch [4801/5000], Loss: 0.0058\n",
      "Epoch [4901/5000], Loss: 0.0059\n",
      "Train: 0.005693967 Test: 0.007535067849077699\n",
      "Epoch [1/5000], Loss: 0.9371\n",
      "Epoch [101/5000], Loss: 0.0377\n",
      "Epoch [201/5000], Loss: 0.0253\n",
      "Epoch [301/5000], Loss: 0.0201\n",
      "Epoch [401/5000], Loss: 0.0179\n",
      "Epoch [501/5000], Loss: 0.0162\n",
      "Epoch [601/5000], Loss: 0.0151\n",
      "Epoch [701/5000], Loss: 0.0144\n",
      "Epoch [801/5000], Loss: 0.0145\n",
      "Epoch [901/5000], Loss: 0.0135\n",
      "Epoch [1001/5000], Loss: 0.0131\n",
      "Epoch [1101/5000], Loss: 0.0130\n",
      "Epoch [1201/5000], Loss: 0.0139\n",
      "Epoch [1301/5000], Loss: 0.0138\n",
      "Epoch [1401/5000], Loss: 0.0115\n",
      "Epoch [1501/5000], Loss: 0.0108\n",
      "Epoch [1601/5000], Loss: 0.0110\n",
      "Epoch [1701/5000], Loss: 0.0102\n",
      "Epoch [1801/5000], Loss: 0.0106\n",
      "Epoch [1901/5000], Loss: 0.0094\n",
      "Epoch [2001/5000], Loss: 0.0093\n",
      "Epoch [2101/5000], Loss: 0.0105\n",
      "Epoch [2201/5000], Loss: 0.0100\n",
      "Epoch [2301/5000], Loss: 0.0131\n",
      "Epoch [2401/5000], Loss: 0.0086\n",
      "Epoch [2501/5000], Loss: 0.0087\n",
      "Epoch [2601/5000], Loss: 0.0085\n",
      "Epoch [2701/5000], Loss: 0.0083\n",
      "Epoch [2801/5000], Loss: 0.0080\n",
      "Epoch [2901/5000], Loss: 0.0084\n",
      "Epoch [3001/5000], Loss: 0.0089\n",
      "Epoch [3101/5000], Loss: 0.0078\n",
      "Epoch [3201/5000], Loss: 0.0080\n",
      "Epoch [3301/5000], Loss: 0.0077\n",
      "Epoch [3401/5000], Loss: 0.0072\n",
      "Epoch [3501/5000], Loss: 0.0073\n",
      "Epoch [3601/5000], Loss: 0.0074\n",
      "Epoch [3701/5000], Loss: 0.0073\n",
      "Epoch [3801/5000], Loss: 0.0074\n",
      "Epoch [3901/5000], Loss: 0.0069\n",
      "Epoch [4001/5000], Loss: 0.0067\n",
      "Epoch [4101/5000], Loss: 0.0069\n",
      "Epoch [4201/5000], Loss: 0.0064\n",
      "Epoch [4301/5000], Loss: 0.0071\n",
      "Epoch [4401/5000], Loss: 0.0067\n",
      "Epoch [4501/5000], Loss: 0.0072\n",
      "Epoch [4601/5000], Loss: 0.0060\n",
      "Epoch [4701/5000], Loss: 0.0064\n",
      "Epoch [4801/5000], Loss: 0.0071\n",
      "Epoch [4901/5000], Loss: 0.0064\n",
      "Train: 0.006078767 Test: 0.00744018394160359\n",
      "Epoch [1/5000], Loss: 1.1712\n",
      "Epoch [101/5000], Loss: 0.0442\n",
      "Epoch [201/5000], Loss: 0.0325\n",
      "Epoch [301/5000], Loss: 0.0292\n",
      "Epoch [401/5000], Loss: 0.0277\n",
      "Epoch [501/5000], Loss: 0.0266\n",
      "Epoch [601/5000], Loss: 0.0256\n",
      "Epoch [701/5000], Loss: 0.0251\n",
      "Epoch [801/5000], Loss: 0.0173\n",
      "Epoch [901/5000], Loss: 0.0171\n",
      "Epoch [1001/5000], Loss: 0.0144\n",
      "Epoch [1101/5000], Loss: 0.0129\n",
      "Epoch [1201/5000], Loss: 0.0152\n",
      "Epoch [1301/5000], Loss: 0.0116\n",
      "Epoch [1401/5000], Loss: 0.0111\n",
      "Epoch [1501/5000], Loss: 0.0114\n",
      "Epoch [1601/5000], Loss: 0.0107\n",
      "Epoch [1701/5000], Loss: 0.0102\n",
      "Epoch [1801/5000], Loss: 0.0102\n",
      "Epoch [1901/5000], Loss: 0.0098\n",
      "Epoch [2001/5000], Loss: 0.0099\n",
      "Epoch [2101/5000], Loss: 0.0094\n",
      "Epoch [2201/5000], Loss: 0.0102\n",
      "Epoch [2301/5000], Loss: 0.0091\n",
      "Epoch [2401/5000], Loss: 0.0092\n",
      "Epoch [2501/5000], Loss: 0.0085\n",
      "Epoch [2601/5000], Loss: 0.0089\n",
      "Epoch [2701/5000], Loss: 0.0086\n",
      "Epoch [2801/5000], Loss: 0.0105\n",
      "Epoch [2901/5000], Loss: 0.0083\n",
      "Epoch [3001/5000], Loss: 0.0080\n",
      "Epoch [3101/5000], Loss: 0.0077\n",
      "Epoch [3201/5000], Loss: 0.0076\n",
      "Epoch [3301/5000], Loss: 0.0079\n",
      "Epoch [3401/5000], Loss: 0.0079\n",
      "Epoch [3501/5000], Loss: 0.0078\n",
      "Epoch [3601/5000], Loss: 0.0072\n",
      "Epoch [3701/5000], Loss: 0.0070\n",
      "Epoch [3801/5000], Loss: 0.0073\n",
      "Epoch [3901/5000], Loss: 0.0090\n",
      "Epoch [4001/5000], Loss: 0.0067\n",
      "Epoch [4101/5000], Loss: 0.0070\n",
      "Epoch [4201/5000], Loss: 0.0065\n",
      "Epoch [4301/5000], Loss: 0.0066\n",
      "Epoch [4401/5000], Loss: 0.0076\n",
      "Epoch [4501/5000], Loss: 0.0062\n",
      "Epoch [4601/5000], Loss: 0.0064\n",
      "Epoch [4701/5000], Loss: 0.0067\n",
      "Epoch [4801/5000], Loss: 0.0058\n",
      "Epoch [4901/5000], Loss: 0.0060\n",
      "Train: 0.0057565006 Test: 0.0075964825571760404\n",
      "Epoch [1/5000], Loss: 1.1061\n",
      "Epoch [101/5000], Loss: 0.0438\n",
      "Epoch [201/5000], Loss: 0.0295\n",
      "Epoch [301/5000], Loss: 0.0233\n",
      "Epoch [401/5000], Loss: 0.0202\n",
      "Epoch [501/5000], Loss: 0.0184\n",
      "Epoch [601/5000], Loss: 0.0181\n",
      "Epoch [701/5000], Loss: 0.0161\n",
      "Epoch [801/5000], Loss: 0.0153\n",
      "Epoch [901/5000], Loss: 0.0146\n",
      "Epoch [1001/5000], Loss: 0.0144\n",
      "Epoch [1101/5000], Loss: 0.0137\n",
      "Epoch [1201/5000], Loss: 0.0145\n",
      "Epoch [1301/5000], Loss: 0.0127\n",
      "Epoch [1401/5000], Loss: 0.0128\n",
      "Epoch [1501/5000], Loss: 0.0122\n",
      "Epoch [1601/5000], Loss: 0.0122\n",
      "Epoch [1701/5000], Loss: 0.0119\n",
      "Epoch [1801/5000], Loss: 0.0118\n",
      "Epoch [1901/5000], Loss: 0.0114\n",
      "Epoch [2001/5000], Loss: 0.0098\n",
      "Epoch [2101/5000], Loss: 0.0096\n",
      "Epoch [2201/5000], Loss: 0.0089\n",
      "Epoch [2301/5000], Loss: 0.0085\n",
      "Epoch [2401/5000], Loss: 0.0097\n",
      "Epoch [2501/5000], Loss: 0.0086\n",
      "Epoch [2601/5000], Loss: 0.0078\n",
      "Epoch [2701/5000], Loss: 0.0086\n",
      "Epoch [2801/5000], Loss: 0.0075\n",
      "Epoch [2901/5000], Loss: 0.0073\n",
      "Epoch [3001/5000], Loss: 0.0072\n",
      "Epoch [3101/5000], Loss: 0.0075\n",
      "Epoch [3201/5000], Loss: 0.0074\n",
      "Epoch [3301/5000], Loss: 0.0070\n",
      "Epoch [3401/5000], Loss: 0.0067\n",
      "Epoch [3501/5000], Loss: 0.0065\n",
      "Epoch [3601/5000], Loss: 0.0071\n",
      "Epoch [3701/5000], Loss: 0.0064\n",
      "Epoch [3801/5000], Loss: 0.0065\n",
      "Epoch [3901/5000], Loss: 0.0060\n",
      "Epoch [4001/5000], Loss: 0.0064\n",
      "Epoch [4101/5000], Loss: 0.0060\n",
      "Epoch [4201/5000], Loss: 0.0062\n",
      "Epoch [4301/5000], Loss: 0.0060\n",
      "Epoch [4401/5000], Loss: 0.0056\n",
      "Epoch [4501/5000], Loss: 0.0059\n",
      "Epoch [4601/5000], Loss: 0.0058\n",
      "Epoch [4701/5000], Loss: 0.0057\n",
      "Epoch [4801/5000], Loss: 0.0060\n",
      "Epoch [4901/5000], Loss: 0.0053\n",
      "Train: 0.005848784 Test: 0.0076813230763928516\n",
      "Epoch [1/5000], Loss: 0.6950\n",
      "Epoch [101/5000], Loss: 0.0329\n",
      "Epoch [201/5000], Loss: 0.0226\n",
      "Epoch [301/5000], Loss: 0.0183\n",
      "Epoch [401/5000], Loss: 0.0159\n",
      "Epoch [501/5000], Loss: 0.0149\n",
      "Epoch [601/5000], Loss: 0.0141\n",
      "Epoch [701/5000], Loss: 0.0126\n",
      "Epoch [801/5000], Loss: 0.0114\n",
      "Epoch [901/5000], Loss: 0.0122\n",
      "Epoch [1001/5000], Loss: 0.0105\n",
      "Epoch [1101/5000], Loss: 0.0111\n",
      "Epoch [1201/5000], Loss: 0.0101\n",
      "Epoch [1301/5000], Loss: 0.0099\n",
      "Epoch [1401/5000], Loss: 0.0093\n",
      "Epoch [1501/5000], Loss: 0.0096\n",
      "Epoch [1601/5000], Loss: 0.0093\n",
      "Epoch [1701/5000], Loss: 0.0085\n",
      "Epoch [1801/5000], Loss: 0.0082\n",
      "Epoch [1901/5000], Loss: 0.0082\n",
      "Epoch [2001/5000], Loss: 0.0082\n",
      "Epoch [2101/5000], Loss: 0.0077\n",
      "Epoch [2201/5000], Loss: 0.0093\n",
      "Epoch [2301/5000], Loss: 0.0086\n",
      "Epoch [2401/5000], Loss: 0.0073\n",
      "Epoch [2501/5000], Loss: 0.0073\n",
      "Epoch [2601/5000], Loss: 0.0071\n",
      "Epoch [2701/5000], Loss: 0.0067\n",
      "Epoch [2801/5000], Loss: 0.0087\n",
      "Epoch [2901/5000], Loss: 0.0067\n",
      "Epoch [3001/5000], Loss: 0.0074\n",
      "Epoch [3101/5000], Loss: 0.0066\n",
      "Epoch [3201/5000], Loss: 0.0070\n",
      "Epoch [3301/5000], Loss: 0.0061\n",
      "Epoch [3401/5000], Loss: 0.0059\n",
      "Epoch [3501/5000], Loss: 0.0059\n",
      "Epoch [3601/5000], Loss: 0.0058\n",
      "Epoch [3701/5000], Loss: 0.0060\n",
      "Epoch [3801/5000], Loss: 0.0057\n",
      "Epoch [3901/5000], Loss: 0.0057\n",
      "Epoch [4001/5000], Loss: 0.0059\n",
      "Epoch [4101/5000], Loss: 0.0054\n",
      "Epoch [4201/5000], Loss: 0.0053\n",
      "Epoch [4301/5000], Loss: 0.0057\n",
      "Epoch [4401/5000], Loss: 0.0053\n",
      "Epoch [4501/5000], Loss: 0.0053\n",
      "Epoch [4601/5000], Loss: 0.0055\n",
      "Epoch [4701/5000], Loss: 0.0053\n",
      "Epoch [4801/5000], Loss: 0.0049\n",
      "Epoch [4901/5000], Loss: 0.0050\n",
      "Train: 0.005055155 Test: 0.007303476703045345\n",
      "Epoch [1/5000], Loss: 0.8883\n",
      "Epoch [101/5000], Loss: 0.0304\n",
      "Epoch [201/5000], Loss: 0.0206\n",
      "Epoch [301/5000], Loss: 0.0168\n",
      "Epoch [401/5000], Loss: 0.0137\n",
      "Epoch [501/5000], Loss: 0.0124\n",
      "Epoch [601/5000], Loss: 0.0120\n",
      "Epoch [701/5000], Loss: 0.0109\n",
      "Epoch [801/5000], Loss: 0.0110\n",
      "Epoch [901/5000], Loss: 0.0103\n",
      "Epoch [1001/5000], Loss: 0.0099\n",
      "Epoch [1101/5000], Loss: 0.0099\n",
      "Epoch [1201/5000], Loss: 0.0090\n",
      "Epoch [1301/5000], Loss: 0.0095\n",
      "Epoch [1401/5000], Loss: 0.0137\n",
      "Epoch [1501/5000], Loss: 0.0087\n",
      "Epoch [1601/5000], Loss: 0.0097\n",
      "Epoch [1701/5000], Loss: 0.0087\n",
      "Epoch [1801/5000], Loss: 0.0078\n",
      "Epoch [1901/5000], Loss: 0.0079\n",
      "Epoch [2001/5000], Loss: 0.0075\n",
      "Epoch [2101/5000], Loss: 0.0077\n",
      "Epoch [2201/5000], Loss: 0.0075\n",
      "Epoch [2301/5000], Loss: 0.0089\n",
      "Epoch [2401/5000], Loss: 0.0072\n",
      "Epoch [2501/5000], Loss: 0.0081\n",
      "Epoch [2601/5000], Loss: 0.0084\n",
      "Epoch [2701/5000], Loss: 0.0067\n",
      "Epoch [2801/5000], Loss: 0.0073\n",
      "Epoch [2901/5000], Loss: 0.0066\n",
      "Epoch [3001/5000], Loss: 0.0066\n",
      "Epoch [3101/5000], Loss: 0.0064\n",
      "Epoch [3201/5000], Loss: 0.0068\n",
      "Epoch [3301/5000], Loss: 0.0063\n",
      "Epoch [3401/5000], Loss: 0.0065\n",
      "Epoch [3501/5000], Loss: 0.0070\n",
      "Epoch [3601/5000], Loss: 0.0060\n",
      "Epoch [3701/5000], Loss: 0.0057\n",
      "Epoch [3801/5000], Loss: 0.0063\n",
      "Epoch [3901/5000], Loss: 0.0056\n",
      "Epoch [4001/5000], Loss: 0.0060\n",
      "Epoch [4101/5000], Loss: 0.0056\n",
      "Epoch [4201/5000], Loss: 0.0058\n",
      "Epoch [4301/5000], Loss: 0.0058\n",
      "Epoch [4401/5000], Loss: 0.0064\n",
      "Epoch [4501/5000], Loss: 0.0058\n",
      "Epoch [4601/5000], Loss: 0.0054\n",
      "Epoch [4701/5000], Loss: 0.0057\n",
      "Epoch [4801/5000], Loss: 0.0061\n",
      "Epoch [4901/5000], Loss: 0.0057\n",
      "Train: 0.006190656 Test: 0.008213692727260821\n",
      "Epoch [1/5000], Loss: 1.0889\n",
      "Epoch [101/5000], Loss: 0.0389\n",
      "Epoch [201/5000], Loss: 0.0308\n",
      "Epoch [301/5000], Loss: 0.0279\n",
      "Epoch [401/5000], Loss: 0.0258\n",
      "Epoch [501/5000], Loss: 0.0243\n",
      "Epoch [601/5000], Loss: 0.0219\n",
      "Epoch [701/5000], Loss: 0.0186\n",
      "Epoch [801/5000], Loss: 0.0174\n",
      "Epoch [901/5000], Loss: 0.0161\n",
      "Epoch [1001/5000], Loss: 0.0155\n",
      "Epoch [1101/5000], Loss: 0.0155\n",
      "Epoch [1201/5000], Loss: 0.0152\n",
      "Epoch [1301/5000], Loss: 0.0136\n",
      "Epoch [1401/5000], Loss: 0.0123\n",
      "Epoch [1501/5000], Loss: 0.0125\n",
      "Epoch [1601/5000], Loss: 0.0118\n",
      "Epoch [1701/5000], Loss: 0.0119\n",
      "Epoch [1801/5000], Loss: 0.0117\n",
      "Epoch [1901/5000], Loss: 0.0117\n",
      "Epoch [2001/5000], Loss: 0.0121\n",
      "Epoch [2101/5000], Loss: 0.0104\n",
      "Epoch [2201/5000], Loss: 0.0108\n",
      "Epoch [2301/5000], Loss: 0.0111\n",
      "Epoch [2401/5000], Loss: 0.0094\n",
      "Epoch [2501/5000], Loss: 0.0107\n",
      "Epoch [2601/5000], Loss: 0.0129\n",
      "Epoch [2701/5000], Loss: 0.0090\n",
      "Epoch [2801/5000], Loss: 0.0081\n",
      "Epoch [2901/5000], Loss: 0.0082\n",
      "Epoch [3001/5000], Loss: 0.0088\n",
      "Epoch [3101/5000], Loss: 0.0079\n",
      "Epoch [3201/5000], Loss: 0.0076\n",
      "Epoch [3301/5000], Loss: 0.0072\n",
      "Epoch [3401/5000], Loss: 0.0074\n",
      "Epoch [3501/5000], Loss: 0.0074\n",
      "Epoch [3601/5000], Loss: 0.0072\n",
      "Epoch [3701/5000], Loss: 0.0068\n",
      "Epoch [3801/5000], Loss: 0.0073\n",
      "Epoch [3901/5000], Loss: 0.0068\n",
      "Epoch [4001/5000], Loss: 0.0068\n",
      "Epoch [4101/5000], Loss: 0.0067\n",
      "Epoch [4201/5000], Loss: 0.0066\n",
      "Epoch [4301/5000], Loss: 0.0064\n",
      "Epoch [4401/5000], Loss: 0.0062\n",
      "Epoch [4501/5000], Loss: 0.0070\n",
      "Epoch [4601/5000], Loss: 0.0069\n",
      "Epoch [4701/5000], Loss: 0.0064\n",
      "Epoch [4801/5000], Loss: 0.0062\n",
      "Epoch [4901/5000], Loss: 0.0058\n",
      "Train: 0.0057405513 Test: 0.007705313396350648\n",
      "Epoch [1/5000], Loss: 1.1035\n",
      "Epoch [101/5000], Loss: 0.0480\n",
      "Epoch [201/5000], Loss: 0.0328\n",
      "Epoch [301/5000], Loss: 0.0266\n",
      "Epoch [401/5000], Loss: 0.0219\n",
      "Epoch [501/5000], Loss: 0.0194\n",
      "Epoch [601/5000], Loss: 0.0205\n",
      "Epoch [701/5000], Loss: 0.0167\n",
      "Epoch [801/5000], Loss: 0.0163\n",
      "Epoch [901/5000], Loss: 0.0151\n",
      "Epoch [1001/5000], Loss: 0.0146\n",
      "Epoch [1101/5000], Loss: 0.0151\n",
      "Epoch [1201/5000], Loss: 0.0137\n",
      "Epoch [1301/5000], Loss: 0.0131\n",
      "Epoch [1401/5000], Loss: 0.0132\n",
      "Epoch [1501/5000], Loss: 0.0129\n",
      "Epoch [1601/5000], Loss: 0.0134\n",
      "Epoch [1701/5000], Loss: 0.0132\n",
      "Epoch [1801/5000], Loss: 0.0123\n",
      "Epoch [1901/5000], Loss: 0.0124\n",
      "Epoch [2001/5000], Loss: 0.0118\n",
      "Epoch [2101/5000], Loss: 0.0117\n",
      "Epoch [2201/5000], Loss: 0.0125\n",
      "Epoch [2301/5000], Loss: 0.0127\n",
      "Epoch [2401/5000], Loss: 0.0115\n",
      "Epoch [2501/5000], Loss: 0.0124\n",
      "Epoch [2601/5000], Loss: 0.0112\n",
      "Epoch [2701/5000], Loss: 0.0116\n",
      "Epoch [2801/5000], Loss: 0.0112\n",
      "Epoch [2901/5000], Loss: 0.0119\n",
      "Epoch [3001/5000], Loss: 0.0113\n",
      "Epoch [3101/5000], Loss: 0.0114\n",
      "Epoch [3201/5000], Loss: 0.0108\n",
      "Epoch [3301/5000], Loss: 0.0108\n",
      "Epoch [3401/5000], Loss: 0.0103\n",
      "Epoch [3501/5000], Loss: 0.0102\n",
      "Epoch [3601/5000], Loss: 0.0100\n",
      "Epoch [3701/5000], Loss: 0.0102\n",
      "Epoch [3801/5000], Loss: 0.0101\n",
      "Epoch [3901/5000], Loss: 0.0094\n",
      "Epoch [4001/5000], Loss: 0.0090\n",
      "Epoch [4101/5000], Loss: 0.0085\n",
      "Epoch [4201/5000], Loss: 0.0079\n",
      "Epoch [4301/5000], Loss: 0.0073\n",
      "Epoch [4401/5000], Loss: 0.0072\n",
      "Epoch [4501/5000], Loss: 0.0084\n",
      "Epoch [4601/5000], Loss: 0.0067\n",
      "Epoch [4701/5000], Loss: 0.0071\n",
      "Epoch [4801/5000], Loss: 0.0067\n",
      "Epoch [4901/5000], Loss: 0.0077\n",
      "Train: 0.006784141 Test: 0.008885772921128673\n",
      "Epoch [1/5000], Loss: 1.1697\n",
      "Epoch [101/5000], Loss: 0.0405\n",
      "Epoch [201/5000], Loss: 0.0240\n",
      "Epoch [301/5000], Loss: 0.0176\n",
      "Epoch [401/5000], Loss: 0.0143\n",
      "Epoch [501/5000], Loss: 0.0130\n",
      "Epoch [601/5000], Loss: 0.0138\n",
      "Epoch [701/5000], Loss: 0.0117\n",
      "Epoch [801/5000], Loss: 0.0108\n",
      "Epoch [901/5000], Loss: 0.0105\n",
      "Epoch [1001/5000], Loss: 0.0099\n",
      "Epoch [1101/5000], Loss: 0.0096\n",
      "Epoch [1201/5000], Loss: 0.0126\n",
      "Epoch [1301/5000], Loss: 0.0090\n",
      "Epoch [1401/5000], Loss: 0.0090\n",
      "Epoch [1501/5000], Loss: 0.0084\n",
      "Epoch [1601/5000], Loss: 0.0083\n",
      "Epoch [1701/5000], Loss: 0.0090\n",
      "Epoch [1801/5000], Loss: 0.0088\n",
      "Epoch [1901/5000], Loss: 0.0085\n",
      "Epoch [2001/5000], Loss: 0.0077\n",
      "Epoch [2101/5000], Loss: 0.0079\n",
      "Epoch [2201/5000], Loss: 0.0078\n",
      "Epoch [2301/5000], Loss: 0.0075\n",
      "Epoch [2401/5000], Loss: 0.0071\n",
      "Epoch [2501/5000], Loss: 0.0071\n",
      "Epoch [2601/5000], Loss: 0.0069\n",
      "Epoch [2701/5000], Loss: 0.0070\n",
      "Epoch [2801/5000], Loss: 0.0065\n",
      "Epoch [2901/5000], Loss: 0.0068\n",
      "Epoch [3001/5000], Loss: 0.0067\n",
      "Epoch [3101/5000], Loss: 0.0065\n",
      "Epoch [3201/5000], Loss: 0.0069\n",
      "Epoch [3301/5000], Loss: 0.0064\n",
      "Epoch [3401/5000], Loss: 0.0065\n",
      "Epoch [3501/5000], Loss: 0.0063\n",
      "Epoch [3601/5000], Loss: 0.0076\n",
      "Epoch [3701/5000], Loss: 0.0060\n",
      "Epoch [3801/5000], Loss: 0.0059\n",
      "Epoch [3901/5000], Loss: 0.0061\n",
      "Epoch [4001/5000], Loss: 0.0064\n",
      "Epoch [4101/5000], Loss: 0.0062\n",
      "Epoch [4201/5000], Loss: 0.0062\n",
      "Epoch [4301/5000], Loss: 0.0064\n",
      "Epoch [4401/5000], Loss: 0.0062\n",
      "Epoch [4501/5000], Loss: 0.0056\n",
      "Epoch [4601/5000], Loss: 0.0056\n",
      "Epoch [4701/5000], Loss: 0.0061\n",
      "Epoch [4801/5000], Loss: 0.0057\n",
      "Epoch [4901/5000], Loss: 0.0055\n",
      "Train: 0.0058286614 Test: 0.007507126591340785\n",
      "Epoch [1/5000], Loss: 0.9884\n",
      "Epoch [101/5000], Loss: 0.0322\n",
      "Epoch [201/5000], Loss: 0.0217\n",
      "Epoch [301/5000], Loss: 0.0162\n",
      "Epoch [401/5000], Loss: 0.0140\n",
      "Epoch [501/5000], Loss: 0.0123\n",
      "Epoch [601/5000], Loss: 0.0133\n",
      "Epoch [701/5000], Loss: 0.0109\n",
      "Epoch [801/5000], Loss: 0.0104\n",
      "Epoch [901/5000], Loss: 0.0108\n",
      "Epoch [1001/5000], Loss: 0.0105\n",
      "Epoch [1101/5000], Loss: 0.0092\n",
      "Epoch [1201/5000], Loss: 0.0107\n",
      "Epoch [1301/5000], Loss: 0.0085\n",
      "Epoch [1401/5000], Loss: 0.0087\n",
      "Epoch [1501/5000], Loss: 0.0085\n",
      "Epoch [1601/5000], Loss: 0.0079\n",
      "Epoch [1701/5000], Loss: 0.0086\n",
      "Epoch [1801/5000], Loss: 0.0073\n",
      "Epoch [1901/5000], Loss: 0.0072\n",
      "Epoch [2001/5000], Loss: 0.0072\n",
      "Epoch [2101/5000], Loss: 0.0071\n",
      "Epoch [2201/5000], Loss: 0.0072\n",
      "Epoch [2301/5000], Loss: 0.0068\n",
      "Epoch [2401/5000], Loss: 0.0066\n",
      "Epoch [2501/5000], Loss: 0.0067\n",
      "Epoch [2601/5000], Loss: 0.0063\n",
      "Epoch [2701/5000], Loss: 0.0064\n",
      "Epoch [2801/5000], Loss: 0.0061\n",
      "Epoch [2901/5000], Loss: 0.0065\n",
      "Epoch [3001/5000], Loss: 0.0059\n",
      "Epoch [3101/5000], Loss: 0.0061\n",
      "Epoch [3201/5000], Loss: 0.0070\n",
      "Epoch [3301/5000], Loss: 0.0059\n",
      "Epoch [3401/5000], Loss: 0.0059\n",
      "Epoch [3501/5000], Loss: 0.0058\n",
      "Epoch [3601/5000], Loss: 0.0056\n",
      "Epoch [3701/5000], Loss: 0.0057\n",
      "Epoch [3801/5000], Loss: 0.0054\n",
      "Epoch [3901/5000], Loss: 0.0054\n",
      "Epoch [4001/5000], Loss: 0.0054\n",
      "Epoch [4101/5000], Loss: 0.0052\n",
      "Epoch [4201/5000], Loss: 0.0052\n",
      "Epoch [4301/5000], Loss: 0.0049\n",
      "Epoch [4401/5000], Loss: 0.0049\n",
      "Epoch [4501/5000], Loss: 0.0052\n",
      "Epoch [4601/5000], Loss: 0.0050\n",
      "Epoch [4701/5000], Loss: 0.0052\n",
      "Epoch [4801/5000], Loss: 0.0049\n",
      "Epoch [4901/5000], Loss: 0.0046\n",
      "Train: 0.0047510867 Test: 0.007230854354888061\n",
      "Epoch [1/5000], Loss: 1.1382\n",
      "Epoch [101/5000], Loss: 0.0408\n",
      "Epoch [201/5000], Loss: 0.0267\n",
      "Epoch [301/5000], Loss: 0.0224\n",
      "Epoch [401/5000], Loss: 0.0202\n",
      "Epoch [501/5000], Loss: 0.0190\n",
      "Epoch [601/5000], Loss: 0.0185\n",
      "Epoch [701/5000], Loss: 0.0179\n",
      "Epoch [801/5000], Loss: 0.0179\n",
      "Epoch [901/5000], Loss: 0.0166\n",
      "Epoch [1001/5000], Loss: 0.0147\n",
      "Epoch [1101/5000], Loss: 0.0135\n",
      "Epoch [1201/5000], Loss: 0.0128\n",
      "Epoch [1301/5000], Loss: 0.0139\n",
      "Epoch [1401/5000], Loss: 0.0122\n",
      "Epoch [1501/5000], Loss: 0.0121\n",
      "Epoch [1601/5000], Loss: 0.0116\n",
      "Epoch [1701/5000], Loss: 0.0118\n",
      "Epoch [1801/5000], Loss: 0.0114\n",
      "Epoch [1901/5000], Loss: 0.0114\n",
      "Epoch [2001/5000], Loss: 0.0112\n",
      "Epoch [2101/5000], Loss: 0.0110\n",
      "Epoch [2201/5000], Loss: 0.0105\n",
      "Epoch [2301/5000], Loss: 0.0112\n",
      "Epoch [2401/5000], Loss: 0.0108\n",
      "Epoch [2501/5000], Loss: 0.0103\n",
      "Epoch [2601/5000], Loss: 0.0101\n",
      "Epoch [2701/5000], Loss: 0.0102\n",
      "Epoch [2801/5000], Loss: 0.0103\n",
      "Epoch [2901/5000], Loss: 0.0105\n",
      "Epoch [3001/5000], Loss: 0.0105\n",
      "Epoch [3101/5000], Loss: 0.0096\n",
      "Epoch [3201/5000], Loss: 0.0096\n",
      "Epoch [3301/5000], Loss: 0.0101\n",
      "Epoch [3401/5000], Loss: 0.0098\n",
      "Epoch [3501/5000], Loss: 0.0096\n",
      "Epoch [3601/5000], Loss: 0.0093\n",
      "Epoch [3701/5000], Loss: 0.0096\n",
      "Epoch [3801/5000], Loss: 0.0095\n",
      "Epoch [3901/5000], Loss: 0.0092\n",
      "Epoch [4001/5000], Loss: 0.0098\n",
      "Epoch [4101/5000], Loss: 0.0089\n",
      "Epoch [4201/5000], Loss: 0.0103\n",
      "Epoch [4301/5000], Loss: 0.0092\n",
      "Epoch [4401/5000], Loss: 0.0089\n",
      "Epoch [4501/5000], Loss: 0.0093\n",
      "Epoch [4601/5000], Loss: 0.0087\n",
      "Epoch [4701/5000], Loss: 0.0087\n",
      "Epoch [4801/5000], Loss: 0.0086\n",
      "Epoch [4901/5000], Loss: 0.0088\n",
      "Train: 0.008422039 Test: 0.01035209227807835\n",
      "Epoch [1/5000], Loss: 1.2139\n",
      "Epoch [101/5000], Loss: 0.0333\n",
      "Epoch [201/5000], Loss: 0.0236\n",
      "Epoch [301/5000], Loss: 0.0202\n",
      "Epoch [401/5000], Loss: 0.0184\n",
      "Epoch [501/5000], Loss: 0.0149\n",
      "Epoch [601/5000], Loss: 0.0169\n",
      "Epoch [701/5000], Loss: 0.0121\n",
      "Epoch [801/5000], Loss: 0.0120\n",
      "Epoch [901/5000], Loss: 0.0114\n",
      "Epoch [1001/5000], Loss: 0.0105\n",
      "Epoch [1101/5000], Loss: 0.0104\n",
      "Epoch [1201/5000], Loss: 0.0099\n",
      "Epoch [1301/5000], Loss: 0.0109\n",
      "Epoch [1401/5000], Loss: 0.0090\n",
      "Epoch [1501/5000], Loss: 0.0088\n",
      "Epoch [1601/5000], Loss: 0.0086\n",
      "Epoch [1701/5000], Loss: 0.0084\n",
      "Epoch [1801/5000], Loss: 0.0092\n",
      "Epoch [1901/5000], Loss: 0.0083\n",
      "Epoch [2001/5000], Loss: 0.0081\n",
      "Epoch [2101/5000], Loss: 0.0081\n",
      "Epoch [2201/5000], Loss: 0.0074\n",
      "Epoch [2301/5000], Loss: 0.0071\n",
      "Epoch [2401/5000], Loss: 0.0081\n",
      "Epoch [2501/5000], Loss: 0.0075\n",
      "Epoch [2601/5000], Loss: 0.0071\n",
      "Epoch [2701/5000], Loss: 0.0068\n",
      "Epoch [2801/5000], Loss: 0.0064\n",
      "Epoch [2901/5000], Loss: 0.0066\n",
      "Epoch [3001/5000], Loss: 0.0063\n",
      "Epoch [3101/5000], Loss: 0.0063\n",
      "Epoch [3201/5000], Loss: 0.0059\n",
      "Epoch [3301/5000], Loss: 0.0058\n",
      "Epoch [3401/5000], Loss: 0.0057\n",
      "Epoch [3501/5000], Loss: 0.0071\n",
      "Epoch [3601/5000], Loss: 0.0059\n",
      "Epoch [3701/5000], Loss: 0.0054\n",
      "Epoch [3801/5000], Loss: 0.0056\n",
      "Epoch [3901/5000], Loss: 0.0057\n",
      "Epoch [4001/5000], Loss: 0.0051\n",
      "Epoch [4101/5000], Loss: 0.0060\n",
      "Epoch [4201/5000], Loss: 0.0051\n",
      "Epoch [4301/5000], Loss: 0.0054\n",
      "Epoch [4401/5000], Loss: 0.0052\n",
      "Epoch [4501/5000], Loss: 0.0057\n",
      "Epoch [4601/5000], Loss: 0.0050\n",
      "Epoch [4701/5000], Loss: 0.0052\n",
      "Epoch [4801/5000], Loss: 0.0050\n",
      "Epoch [4901/5000], Loss: 0.0051\n",
      "Train: 0.0047525633 Test: 0.007216306032615058\n",
      "Epoch [1/5000], Loss: 0.7868\n",
      "Epoch [101/5000], Loss: 0.0319\n",
      "Epoch [201/5000], Loss: 0.0233\n",
      "Epoch [301/5000], Loss: 0.0178\n",
      "Epoch [401/5000], Loss: 0.0151\n",
      "Epoch [501/5000], Loss: 0.0135\n",
      "Epoch [601/5000], Loss: 0.0136\n",
      "Epoch [701/5000], Loss: 0.0146\n",
      "Epoch [801/5000], Loss: 0.0108\n",
      "Epoch [901/5000], Loss: 0.0104\n",
      "Epoch [1001/5000], Loss: 0.0099\n",
      "Epoch [1101/5000], Loss: 0.0099\n",
      "Epoch [1201/5000], Loss: 0.0091\n",
      "Epoch [1301/5000], Loss: 0.0092\n",
      "Epoch [1401/5000], Loss: 0.0085\n",
      "Epoch [1501/5000], Loss: 0.0099\n",
      "Epoch [1601/5000], Loss: 0.0084\n",
      "Epoch [1701/5000], Loss: 0.0085\n",
      "Epoch [1801/5000], Loss: 0.0079\n",
      "Epoch [1901/5000], Loss: 0.0074\n",
      "Epoch [2001/5000], Loss: 0.0073\n",
      "Epoch [2101/5000], Loss: 0.0070\n",
      "Epoch [2201/5000], Loss: 0.0074\n",
      "Epoch [2301/5000], Loss: 0.0069\n",
      "Epoch [2401/5000], Loss: 0.0071\n",
      "Epoch [2501/5000], Loss: 0.0067\n",
      "Epoch [2601/5000], Loss: 0.0064\n",
      "Epoch [2701/5000], Loss: 0.0070\n",
      "Epoch [2801/5000], Loss: 0.0073\n",
      "Epoch [2901/5000], Loss: 0.0061\n",
      "Epoch [3001/5000], Loss: 0.0062\n",
      "Epoch [3101/5000], Loss: 0.0060\n",
      "Epoch [3201/5000], Loss: 0.0061\n",
      "Epoch [3301/5000], Loss: 0.0061\n",
      "Epoch [3401/5000], Loss: 0.0067\n",
      "Epoch [3501/5000], Loss: 0.0061\n",
      "Epoch [3601/5000], Loss: 0.0060\n",
      "Epoch [3701/5000], Loss: 0.0060\n",
      "Epoch [3801/5000], Loss: 0.0055\n",
      "Epoch [3901/5000], Loss: 0.0055\n",
      "Epoch [4001/5000], Loss: 0.0055\n",
      "Epoch [4101/5000], Loss: 0.0056\n",
      "Epoch [4201/5000], Loss: 0.0060\n",
      "Epoch [4301/5000], Loss: 0.0059\n",
      "Epoch [4401/5000], Loss: 0.0054\n",
      "Epoch [4501/5000], Loss: 0.0057\n",
      "Epoch [4601/5000], Loss: 0.0052\n",
      "Epoch [4701/5000], Loss: 0.0051\n",
      "Epoch [4801/5000], Loss: 0.0049\n",
      "Epoch [4901/5000], Loss: 0.0052\n",
      "Train: 0.004706228 Test: 0.006965203629642959\n",
      "Epoch [1/5000], Loss: 0.7739\n",
      "Epoch [101/5000], Loss: 0.0290\n",
      "Epoch [201/5000], Loss: 0.0201\n",
      "Epoch [301/5000], Loss: 0.0164\n",
      "Epoch [401/5000], Loss: 0.0142\n",
      "Epoch [501/5000], Loss: 0.0123\n",
      "Epoch [601/5000], Loss: 0.0113\n",
      "Epoch [701/5000], Loss: 0.0111\n",
      "Epoch [801/5000], Loss: 0.0152\n",
      "Epoch [901/5000], Loss: 0.0095\n",
      "Epoch [1001/5000], Loss: 0.0090\n",
      "Epoch [1101/5000], Loss: 0.0088\n",
      "Epoch [1201/5000], Loss: 0.0084\n",
      "Epoch [1301/5000], Loss: 0.0092\n",
      "Epoch [1401/5000], Loss: 0.0109\n",
      "Epoch [1501/5000], Loss: 0.0081\n",
      "Epoch [1601/5000], Loss: 0.0073\n",
      "Epoch [1701/5000], Loss: 0.0083\n",
      "Epoch [1801/5000], Loss: 0.0077\n",
      "Epoch [1901/5000], Loss: 0.0078\n",
      "Epoch [2001/5000], Loss: 0.0070\n",
      "Epoch [2101/5000], Loss: 0.0065\n",
      "Epoch [2201/5000], Loss: 0.0070\n",
      "Epoch [2301/5000], Loss: 0.0078\n",
      "Epoch [2401/5000], Loss: 0.0075\n",
      "Epoch [2501/5000], Loss: 0.0063\n",
      "Epoch [2601/5000], Loss: 0.0082\n",
      "Epoch [2701/5000], Loss: 0.0060\n",
      "Epoch [2801/5000], Loss: 0.0063\n",
      "Epoch [2901/5000], Loss: 0.0059\n",
      "Epoch [3001/5000], Loss: 0.0056\n",
      "Epoch [3101/5000], Loss: 0.0057\n",
      "Epoch [3201/5000], Loss: 0.0055\n",
      "Epoch [3301/5000], Loss: 0.0069\n",
      "Epoch [3401/5000], Loss: 0.0055\n",
      "Epoch [3501/5000], Loss: 0.0063\n",
      "Epoch [3601/5000], Loss: 0.0053\n",
      "Epoch [3701/5000], Loss: 0.0052\n",
      "Epoch [3801/5000], Loss: 0.0057\n",
      "Epoch [3901/5000], Loss: 0.0051\n",
      "Epoch [4001/5000], Loss: 0.0050\n",
      "Epoch [4101/5000], Loss: 0.0054\n",
      "Epoch [4201/5000], Loss: 0.0049\n",
      "Epoch [4301/5000], Loss: 0.0050\n",
      "Epoch [4401/5000], Loss: 0.0058\n",
      "Epoch [4501/5000], Loss: 0.0051\n",
      "Epoch [4601/5000], Loss: 0.0046\n",
      "Epoch [4701/5000], Loss: 0.0047\n",
      "Epoch [4801/5000], Loss: 0.0046\n",
      "Epoch [4901/5000], Loss: 0.0046\n",
      "Train: 0.0045174197 Test: 0.006802180542178095\n",
      "Epoch [1/5000], Loss: 0.5046\n",
      "Epoch [101/5000], Loss: 0.0284\n",
      "Epoch [201/5000], Loss: 0.0209\n",
      "Epoch [301/5000], Loss: 0.0171\n",
      "Epoch [401/5000], Loss: 0.0152\n",
      "Epoch [501/5000], Loss: 0.0140\n",
      "Epoch [601/5000], Loss: 0.0133\n",
      "Epoch [701/5000], Loss: 0.0112\n",
      "Epoch [801/5000], Loss: 0.0156\n",
      "Epoch [901/5000], Loss: 0.0099\n",
      "Epoch [1001/5000], Loss: 0.0090\n",
      "Epoch [1101/5000], Loss: 0.0089\n",
      "Epoch [1201/5000], Loss: 0.0089\n",
      "Epoch [1301/5000], Loss: 0.0084\n",
      "Epoch [1401/5000], Loss: 0.0080\n",
      "Epoch [1501/5000], Loss: 0.0076\n",
      "Epoch [1601/5000], Loss: 0.0075\n",
      "Epoch [1701/5000], Loss: 0.0074\n",
      "Epoch [1801/5000], Loss: 0.0067\n",
      "Epoch [1901/5000], Loss: 0.0067\n",
      "Epoch [2001/5000], Loss: 0.0064\n",
      "Epoch [2101/5000], Loss: 0.0061\n",
      "Epoch [2201/5000], Loss: 0.0064\n",
      "Epoch [2301/5000], Loss: 0.0063\n",
      "Epoch [2401/5000], Loss: 0.0061\n",
      "Epoch [2501/5000], Loss: 0.0064\n",
      "Epoch [2601/5000], Loss: 0.0059\n",
      "Epoch [2701/5000], Loss: 0.0058\n",
      "Epoch [2801/5000], Loss: 0.0056\n",
      "Epoch [2901/5000], Loss: 0.0054\n",
      "Epoch [3001/5000], Loss: 0.0057\n",
      "Epoch [3101/5000], Loss: 0.0054\n",
      "Epoch [3201/5000], Loss: 0.0061\n",
      "Epoch [3301/5000], Loss: 0.0052\n",
      "Epoch [3401/5000], Loss: 0.0060\n",
      "Epoch [3501/5000], Loss: 0.0048\n",
      "Epoch [3601/5000], Loss: 0.0052\n",
      "Epoch [3701/5000], Loss: 0.0062\n",
      "Epoch [3801/5000], Loss: 0.0049\n",
      "Epoch [3901/5000], Loss: 0.0046\n",
      "Epoch [4001/5000], Loss: 0.0046\n",
      "Epoch [4101/5000], Loss: 0.0061\n",
      "Epoch [4201/5000], Loss: 0.0047\n",
      "Epoch [4301/5000], Loss: 0.0045\n",
      "Epoch [4401/5000], Loss: 0.0045\n",
      "Epoch [4501/5000], Loss: 0.0049\n",
      "Epoch [4601/5000], Loss: 0.0042\n",
      "Epoch [4701/5000], Loss: 0.0060\n",
      "Epoch [4801/5000], Loss: 0.0041\n",
      "Epoch [4901/5000], Loss: 0.0041\n",
      "Train: 0.004093085 Test: 0.006655228401932355\n",
      "Epoch [1/5000], Loss: 1.3924\n",
      "Epoch [101/5000], Loss: 0.0493\n",
      "Epoch [201/5000], Loss: 0.0278\n",
      "Epoch [301/5000], Loss: 0.0211\n",
      "Epoch [401/5000], Loss: 0.0201\n",
      "Epoch [501/5000], Loss: 0.0190\n",
      "Epoch [601/5000], Loss: 0.0182\n",
      "Epoch [701/5000], Loss: 0.0165\n",
      "Epoch [801/5000], Loss: 0.0168\n",
      "Epoch [901/5000], Loss: 0.0142\n",
      "Epoch [1001/5000], Loss: 0.0154\n",
      "Epoch [1101/5000], Loss: 0.0129\n",
      "Epoch [1201/5000], Loss: 0.0129\n",
      "Epoch [1301/5000], Loss: 0.0118\n",
      "Epoch [1401/5000], Loss: 0.0121\n",
      "Epoch [1501/5000], Loss: 0.0123\n",
      "Epoch [1601/5000], Loss: 0.0109\n",
      "Epoch [1701/5000], Loss: 0.0107\n",
      "Epoch [1801/5000], Loss: 0.0101\n",
      "Epoch [1901/5000], Loss: 0.0095\n",
      "Epoch [2001/5000], Loss: 0.0093\n",
      "Epoch [2101/5000], Loss: 0.0086\n",
      "Epoch [2201/5000], Loss: 0.0086\n",
      "Epoch [2301/5000], Loss: 0.0089\n",
      "Epoch [2401/5000], Loss: 0.0091\n",
      "Epoch [2501/5000], Loss: 0.0078\n",
      "Epoch [2601/5000], Loss: 0.0080\n",
      "Epoch [2701/5000], Loss: 0.0079\n",
      "Epoch [2801/5000], Loss: 0.0078\n",
      "Epoch [2901/5000], Loss: 0.0072\n",
      "Epoch [3001/5000], Loss: 0.0070\n",
      "Epoch [3101/5000], Loss: 0.0084\n",
      "Epoch [3201/5000], Loss: 0.0081\n",
      "Epoch [3301/5000], Loss: 0.0081\n",
      "Epoch [3401/5000], Loss: 0.0085\n",
      "Epoch [3501/5000], Loss: 0.0079\n",
      "Epoch [3601/5000], Loss: 0.0066\n",
      "Epoch [3701/5000], Loss: 0.0073\n",
      "Epoch [3801/5000], Loss: 0.0064\n",
      "Epoch [3901/5000], Loss: 0.0071\n",
      "Epoch [4001/5000], Loss: 0.0063\n",
      "Epoch [4101/5000], Loss: 0.0070\n",
      "Epoch [4201/5000], Loss: 0.0064\n",
      "Epoch [4301/5000], Loss: 0.0082\n",
      "Epoch [4401/5000], Loss: 0.0066\n",
      "Epoch [4501/5000], Loss: 0.0077\n",
      "Epoch [4601/5000], Loss: 0.0073\n",
      "Epoch [4701/5000], Loss: 0.0064\n",
      "Epoch [4801/5000], Loss: 0.0078\n",
      "Epoch [4901/5000], Loss: 0.0069\n",
      "Train: 0.007323012 Test: 0.009223710227811299\n",
      "Epoch [1/5000], Loss: 0.8981\n",
      "Epoch [101/5000], Loss: 0.0381\n",
      "Epoch [201/5000], Loss: 0.0301\n",
      "Epoch [301/5000], Loss: 0.0220\n",
      "Epoch [401/5000], Loss: 0.0174\n",
      "Epoch [501/5000], Loss: 0.0156\n",
      "Epoch [601/5000], Loss: 0.0142\n",
      "Epoch [701/5000], Loss: 0.0136\n",
      "Epoch [801/5000], Loss: 0.0132\n",
      "Epoch [901/5000], Loss: 0.0147\n",
      "Epoch [1001/5000], Loss: 0.0124\n",
      "Epoch [1101/5000], Loss: 0.0122\n",
      "Epoch [1201/5000], Loss: 0.0118\n",
      "Epoch [1301/5000], Loss: 0.0117\n",
      "Epoch [1401/5000], Loss: 0.0119\n",
      "Epoch [1501/5000], Loss: 0.0117\n",
      "Epoch [1601/5000], Loss: 0.0119\n",
      "Epoch [1701/5000], Loss: 0.0107\n",
      "Epoch [1801/5000], Loss: 0.0113\n",
      "Epoch [1901/5000], Loss: 0.0103\n",
      "Epoch [2001/5000], Loss: 0.0103\n",
      "Epoch [2101/5000], Loss: 0.0102\n",
      "Epoch [2201/5000], Loss: 0.0097\n",
      "Epoch [2301/5000], Loss: 0.0096\n",
      "Epoch [2401/5000], Loss: 0.0094\n",
      "Epoch [2501/5000], Loss: 0.0100\n",
      "Epoch [2601/5000], Loss: 0.0097\n",
      "Epoch [2701/5000], Loss: 0.0093\n",
      "Epoch [2801/5000], Loss: 0.0091\n",
      "Epoch [2901/5000], Loss: 0.0090\n",
      "Epoch [3001/5000], Loss: 0.0110\n",
      "Epoch [3101/5000], Loss: 0.0090\n",
      "Epoch [3201/5000], Loss: 0.0092\n",
      "Epoch [3301/5000], Loss: 0.0094\n",
      "Epoch [3401/5000], Loss: 0.0089\n",
      "Epoch [3501/5000], Loss: 0.0089\n",
      "Epoch [3601/5000], Loss: 0.0087\n",
      "Epoch [3701/5000], Loss: 0.0088\n",
      "Epoch [3801/5000], Loss: 0.0090\n",
      "Epoch [3901/5000], Loss: 0.0085\n",
      "Epoch [4001/5000], Loss: 0.0085\n",
      "Epoch [4101/5000], Loss: 0.0086\n",
      "Epoch [4201/5000], Loss: 0.0087\n",
      "Epoch [4301/5000], Loss: 0.0083\n",
      "Epoch [4401/5000], Loss: 0.0084\n",
      "Epoch [4501/5000], Loss: 0.0088\n",
      "Epoch [4601/5000], Loss: 0.0087\n",
      "Epoch [4701/5000], Loss: 0.0082\n",
      "Epoch [4801/5000], Loss: 0.0086\n",
      "Epoch [4901/5000], Loss: 0.0081\n",
      "Train: 0.00856327 Test: 0.01053412036115675\n",
      "Epoch [1/5000], Loss: 0.9649\n",
      "Epoch [101/5000], Loss: 0.0395\n",
      "Epoch [201/5000], Loss: 0.0244\n",
      "Epoch [301/5000], Loss: 0.0200\n",
      "Epoch [401/5000], Loss: 0.0163\n",
      "Epoch [501/5000], Loss: 0.0152\n",
      "Epoch [601/5000], Loss: 0.0141\n",
      "Epoch [701/5000], Loss: 0.0133\n",
      "Epoch [801/5000], Loss: 0.0132\n",
      "Epoch [901/5000], Loss: 0.0129\n",
      "Epoch [1001/5000], Loss: 0.0123\n",
      "Epoch [1101/5000], Loss: 0.0120\n",
      "Epoch [1201/5000], Loss: 0.0119\n",
      "Epoch [1301/5000], Loss: 0.0130\n",
      "Epoch [1401/5000], Loss: 0.0113\n",
      "Epoch [1501/5000], Loss: 0.0111\n",
      "Epoch [1601/5000], Loss: 0.0121\n",
      "Epoch [1701/5000], Loss: 0.0110\n",
      "Epoch [1801/5000], Loss: 0.0105\n",
      "Epoch [1901/5000], Loss: 0.0103\n",
      "Epoch [2001/5000], Loss: 0.0095\n",
      "Epoch [2101/5000], Loss: 0.0088\n",
      "Epoch [2201/5000], Loss: 0.0107\n",
      "Epoch [2301/5000], Loss: 0.0084\n",
      "Epoch [2401/5000], Loss: 0.0079\n",
      "Epoch [2501/5000], Loss: 0.0080\n",
      "Epoch [2601/5000], Loss: 0.0072\n",
      "Epoch [2701/5000], Loss: 0.0069\n",
      "Epoch [2801/5000], Loss: 0.0070\n",
      "Epoch [2901/5000], Loss: 0.0069\n",
      "Epoch [3001/5000], Loss: 0.0066\n",
      "Epoch [3101/5000], Loss: 0.0063\n",
      "Epoch [3201/5000], Loss: 0.0064\n",
      "Epoch [3301/5000], Loss: 0.0067\n",
      "Epoch [3401/5000], Loss: 0.0068\n",
      "Epoch [3501/5000], Loss: 0.0061\n",
      "Epoch [3601/5000], Loss: 0.0060\n",
      "Epoch [3701/5000], Loss: 0.0057\n",
      "Epoch [3801/5000], Loss: 0.0057\n",
      "Epoch [3901/5000], Loss: 0.0056\n",
      "Epoch [4001/5000], Loss: 0.0057\n",
      "Epoch [4101/5000], Loss: 0.0058\n",
      "Epoch [4201/5000], Loss: 0.0052\n",
      "Epoch [4301/5000], Loss: 0.0056\n",
      "Epoch [4401/5000], Loss: 0.0052\n",
      "Epoch [4501/5000], Loss: 0.0053\n",
      "Epoch [4601/5000], Loss: 0.0051\n",
      "Epoch [4701/5000], Loss: 0.0057\n",
      "Epoch [4801/5000], Loss: 0.0049\n",
      "Epoch [4901/5000], Loss: 0.0053\n",
      "Train: 0.0047702095 Test: 0.00661192866896327\n",
      "Epoch [1/5000], Loss: 0.7698\n",
      "Epoch [101/5000], Loss: 0.0281\n",
      "Epoch [201/5000], Loss: 0.0190\n",
      "Epoch [301/5000], Loss: 0.0149\n",
      "Epoch [401/5000], Loss: 0.0131\n",
      "Epoch [501/5000], Loss: 0.0117\n",
      "Epoch [601/5000], Loss: 0.0114\n",
      "Epoch [701/5000], Loss: 0.0099\n",
      "Epoch [801/5000], Loss: 0.0099\n",
      "Epoch [901/5000], Loss: 0.0090\n",
      "Epoch [1001/5000], Loss: 0.0104\n",
      "Epoch [1101/5000], Loss: 0.0086\n",
      "Epoch [1201/5000], Loss: 0.0149\n",
      "Epoch [1301/5000], Loss: 0.0080\n",
      "Epoch [1401/5000], Loss: 0.0084\n",
      "Epoch [1501/5000], Loss: 0.0083\n",
      "Epoch [1601/5000], Loss: 0.0071\n",
      "Epoch [1701/5000], Loss: 0.0073\n",
      "Epoch [1801/5000], Loss: 0.0072\n",
      "Epoch [1901/5000], Loss: 0.0065\n",
      "Epoch [2001/5000], Loss: 0.0063\n",
      "Epoch [2101/5000], Loss: 0.0063\n",
      "Epoch [2201/5000], Loss: 0.0063\n",
      "Epoch [2301/5000], Loss: 0.0062\n",
      "Epoch [2401/5000], Loss: 0.0067\n",
      "Epoch [2501/5000], Loss: 0.0058\n",
      "Epoch [2601/5000], Loss: 0.0065\n",
      "Epoch [2701/5000], Loss: 0.0057\n",
      "Epoch [2801/5000], Loss: 0.0055\n",
      "Epoch [2901/5000], Loss: 0.0057\n",
      "Epoch [3001/5000], Loss: 0.0055\n",
      "Epoch [3101/5000], Loss: 0.0058\n",
      "Epoch [3201/5000], Loss: 0.0052\n",
      "Epoch [3301/5000], Loss: 0.0050\n",
      "Epoch [3401/5000], Loss: 0.0050\n",
      "Epoch [3501/5000], Loss: 0.0049\n",
      "Epoch [3601/5000], Loss: 0.0049\n",
      "Epoch [3701/5000], Loss: 0.0048\n",
      "Epoch [3801/5000], Loss: 0.0054\n",
      "Epoch [3901/5000], Loss: 0.0056\n",
      "Epoch [4001/5000], Loss: 0.0054\n",
      "Epoch [4101/5000], Loss: 0.0046\n",
      "Epoch [4201/5000], Loss: 0.0065\n",
      "Epoch [4301/5000], Loss: 0.0071\n",
      "Epoch [4401/5000], Loss: 0.0050\n",
      "Epoch [4501/5000], Loss: 0.0046\n",
      "Epoch [4601/5000], Loss: 0.0048\n",
      "Epoch [4701/5000], Loss: 0.0043\n",
      "Epoch [4801/5000], Loss: 0.0060\n",
      "Epoch [4901/5000], Loss: 0.0042\n",
      "Train: 0.0048055216 Test: 0.006974989692792449\n",
      "Epoch [1/5000], Loss: 1.3414\n",
      "Epoch [101/5000], Loss: 0.0473\n",
      "Epoch [201/5000], Loss: 0.0269\n",
      "Epoch [301/5000], Loss: 0.0219\n",
      "Epoch [401/5000], Loss: 0.0197\n",
      "Epoch [501/5000], Loss: 0.0191\n",
      "Epoch [601/5000], Loss: 0.0180\n",
      "Epoch [701/5000], Loss: 0.0173\n",
      "Epoch [801/5000], Loss: 0.0171\n",
      "Epoch [901/5000], Loss: 0.0169\n",
      "Epoch [1001/5000], Loss: 0.0169\n",
      "Epoch [1101/5000], Loss: 0.0161\n",
      "Epoch [1201/5000], Loss: 0.0158\n",
      "Epoch [1301/5000], Loss: 0.0163\n",
      "Epoch [1401/5000], Loss: 0.0147\n",
      "Epoch [1501/5000], Loss: 0.0160\n",
      "Epoch [1601/5000], Loss: 0.0148\n",
      "Epoch [1701/5000], Loss: 0.0149\n",
      "Epoch [1801/5000], Loss: 0.0133\n",
      "Epoch [1901/5000], Loss: 0.0127\n",
      "Epoch [2001/5000], Loss: 0.0136\n",
      "Epoch [2101/5000], Loss: 0.0119\n",
      "Epoch [2201/5000], Loss: 0.0129\n",
      "Epoch [2301/5000], Loss: 0.0141\n",
      "Epoch [2401/5000], Loss: 0.0147\n",
      "Epoch [2501/5000], Loss: 0.0122\n",
      "Epoch [2601/5000], Loss: 0.0116\n",
      "Epoch [2701/5000], Loss: 0.0116\n",
      "Epoch [2801/5000], Loss: 0.0114\n",
      "Epoch [2901/5000], Loss: 0.0112\n",
      "Epoch [3001/5000], Loss: 0.0108\n",
      "Epoch [3101/5000], Loss: 0.0108\n",
      "Epoch [3201/5000], Loss: 0.0121\n",
      "Epoch [3301/5000], Loss: 0.0103\n",
      "Epoch [3401/5000], Loss: 0.0105\n",
      "Epoch [3501/5000], Loss: 0.0118\n",
      "Epoch [3601/5000], Loss: 0.0097\n",
      "Epoch [3701/5000], Loss: 0.0105\n",
      "Epoch [3801/5000], Loss: 0.0121\n",
      "Epoch [3901/5000], Loss: 0.0101\n",
      "Epoch [4001/5000], Loss: 0.0110\n",
      "Epoch [4101/5000], Loss: 0.0100\n",
      "Epoch [4201/5000], Loss: 0.0101\n",
      "Epoch [4301/5000], Loss: 0.0096\n",
      "Epoch [4401/5000], Loss: 0.0114\n",
      "Epoch [4501/5000], Loss: 0.0100\n",
      "Epoch [4601/5000], Loss: 0.0082\n",
      "Epoch [4701/5000], Loss: 0.0078\n",
      "Epoch [4801/5000], Loss: 0.0082\n",
      "Epoch [4901/5000], Loss: 0.0083\n",
      "Train: 0.008353601 Test: 0.009841294886709708\n",
      "Epoch [1/5000], Loss: 0.7820\n",
      "Epoch [101/5000], Loss: 0.0301\n",
      "Epoch [201/5000], Loss: 0.0209\n",
      "Epoch [301/5000], Loss: 0.0182\n",
      "Epoch [401/5000], Loss: 0.0151\n",
      "Epoch [501/5000], Loss: 0.0152\n",
      "Epoch [601/5000], Loss: 0.0134\n",
      "Epoch [701/5000], Loss: 0.0125\n",
      "Epoch [801/5000], Loss: 0.0125\n",
      "Epoch [901/5000], Loss: 0.0120\n",
      "Epoch [1001/5000], Loss: 0.0122\n",
      "Epoch [1101/5000], Loss: 0.0128\n",
      "Epoch [1201/5000], Loss: 0.0117\n",
      "Epoch [1301/5000], Loss: 0.0108\n",
      "Epoch [1401/5000], Loss: 0.0111\n",
      "Epoch [1501/5000], Loss: 0.0120\n",
      "Epoch [1601/5000], Loss: 0.0102\n",
      "Epoch [1701/5000], Loss: 0.0101\n",
      "Epoch [1801/5000], Loss: 0.0099\n",
      "Epoch [1901/5000], Loss: 0.0099\n",
      "Epoch [2001/5000], Loss: 0.0096\n",
      "Epoch [2101/5000], Loss: 0.0095\n",
      "Epoch [2201/5000], Loss: 0.0101\n",
      "Epoch [2301/5000], Loss: 0.0092\n",
      "Epoch [2401/5000], Loss: 0.0097\n",
      "Epoch [2501/5000], Loss: 0.0090\n",
      "Epoch [2601/5000], Loss: 0.0088\n",
      "Epoch [2701/5000], Loss: 0.0075\n",
      "Epoch [2801/5000], Loss: 0.0084\n",
      "Epoch [2901/5000], Loss: 0.0068\n",
      "Epoch [3001/5000], Loss: 0.0065\n",
      "Epoch [3101/5000], Loss: 0.0062\n",
      "Epoch [3201/5000], Loss: 0.0062\n",
      "Epoch [3301/5000], Loss: 0.0060\n",
      "Epoch [3401/5000], Loss: 0.0055\n",
      "Epoch [3501/5000], Loss: 0.0057\n",
      "Epoch [3601/5000], Loss: 0.0063\n",
      "Epoch [3701/5000], Loss: 0.0057\n",
      "Epoch [3801/5000], Loss: 0.0058\n",
      "Epoch [3901/5000], Loss: 0.0054\n",
      "Epoch [4001/5000], Loss: 0.0057\n",
      "Epoch [4101/5000], Loss: 0.0063\n",
      "Epoch [4201/5000], Loss: 0.0055\n",
      "Epoch [4301/5000], Loss: 0.0051\n",
      "Epoch [4401/5000], Loss: 0.0053\n",
      "Epoch [4501/5000], Loss: 0.0053\n",
      "Epoch [4601/5000], Loss: 0.0067\n",
      "Epoch [4701/5000], Loss: 0.0048\n",
      "Epoch [4801/5000], Loss: 0.0054\n",
      "Epoch [4901/5000], Loss: 0.0047\n",
      "Train: 0.00507934 Test: 0.007033017663365723\n",
      "Epoch [1/5000], Loss: 0.9000\n",
      "Epoch [101/5000], Loss: 0.0398\n",
      "Epoch [201/5000], Loss: 0.0255\n",
      "Epoch [301/5000], Loss: 0.0201\n",
      "Epoch [401/5000], Loss: 0.0174\n",
      "Epoch [501/5000], Loss: 0.0161\n",
      "Epoch [601/5000], Loss: 0.0151\n",
      "Epoch [701/5000], Loss: 0.0158\n",
      "Epoch [801/5000], Loss: 0.0147\n",
      "Epoch [901/5000], Loss: 0.0138\n",
      "Epoch [1001/5000], Loss: 0.0133\n",
      "Epoch [1101/5000], Loss: 0.0126\n",
      "Epoch [1201/5000], Loss: 0.0129\n",
      "Epoch [1301/5000], Loss: 0.0126\n",
      "Epoch [1401/5000], Loss: 0.0121\n",
      "Epoch [1501/5000], Loss: 0.0120\n",
      "Epoch [1601/5000], Loss: 0.0119\n",
      "Epoch [1701/5000], Loss: 0.0117\n",
      "Epoch [1801/5000], Loss: 0.0132\n",
      "Epoch [1901/5000], Loss: 0.0119\n",
      "Epoch [2001/5000], Loss: 0.0117\n",
      "Epoch [2101/5000], Loss: 0.0110\n",
      "Epoch [2201/5000], Loss: 0.0108\n",
      "Epoch [2301/5000], Loss: 0.0109\n",
      "Epoch [2401/5000], Loss: 0.0108\n",
      "Epoch [2501/5000], Loss: 0.0110\n",
      "Epoch [2601/5000], Loss: 0.0110\n",
      "Epoch [2701/5000], Loss: 0.0107\n",
      "Epoch [2801/5000], Loss: 0.0108\n",
      "Epoch [2901/5000], Loss: 0.0112\n",
      "Epoch [3001/5000], Loss: 0.0103\n",
      "Epoch [3101/5000], Loss: 0.0101\n",
      "Epoch [3201/5000], Loss: 0.0116\n",
      "Epoch [3301/5000], Loss: 0.0113\n",
      "Epoch [3401/5000], Loss: 0.0099\n",
      "Epoch [3501/5000], Loss: 0.0098\n",
      "Epoch [3601/5000], Loss: 0.0099\n",
      "Epoch [3701/5000], Loss: 0.0098\n",
      "Epoch [3801/5000], Loss: 0.0096\n",
      "Epoch [3901/5000], Loss: 0.0099\n",
      "Epoch [4001/5000], Loss: 0.0099\n",
      "Epoch [4101/5000], Loss: 0.0103\n",
      "Epoch [4201/5000], Loss: 0.0102\n",
      "Epoch [4301/5000], Loss: 0.0091\n",
      "Epoch [4401/5000], Loss: 0.0100\n",
      "Epoch [4501/5000], Loss: 0.0094\n",
      "Epoch [4601/5000], Loss: 0.0089\n",
      "Epoch [4701/5000], Loss: 0.0095\n",
      "Epoch [4801/5000], Loss: 0.0089\n",
      "Epoch [4901/5000], Loss: 0.0088\n",
      "Train: 0.009122506 Test: 0.011190259415659829\n",
      "Epoch [1/5000], Loss: 1.2306\n",
      "Epoch [101/5000], Loss: 0.0404\n",
      "Epoch [201/5000], Loss: 0.0255\n",
      "Epoch [301/5000], Loss: 0.0217\n",
      "Epoch [401/5000], Loss: 0.0201\n",
      "Epoch [501/5000], Loss: 0.0193\n",
      "Epoch [601/5000], Loss: 0.0200\n",
      "Epoch [701/5000], Loss: 0.0180\n",
      "Epoch [801/5000], Loss: 0.0177\n",
      "Epoch [901/5000], Loss: 0.0178\n",
      "Epoch [1001/5000], Loss: 0.0193\n",
      "Epoch [1101/5000], Loss: 0.0167\n",
      "Epoch [1201/5000], Loss: 0.0181\n",
      "Epoch [1301/5000], Loss: 0.0163\n",
      "Epoch [1401/5000], Loss: 0.0164\n",
      "Epoch [1501/5000], Loss: 0.0149\n",
      "Epoch [1601/5000], Loss: 0.0144\n",
      "Epoch [1701/5000], Loss: 0.0135\n",
      "Epoch [1801/5000], Loss: 0.0138\n",
      "Epoch [1901/5000], Loss: 0.0139\n",
      "Epoch [2001/5000], Loss: 0.0126\n",
      "Epoch [2101/5000], Loss: 0.0131\n",
      "Epoch [2201/5000], Loss: 0.0126\n",
      "Epoch [2301/5000], Loss: 0.0135\n",
      "Epoch [2401/5000], Loss: 0.0120\n",
      "Epoch [2501/5000], Loss: 0.0119\n",
      "Epoch [2601/5000], Loss: 0.0120\n",
      "Epoch [2701/5000], Loss: 0.0128\n",
      "Epoch [2801/5000], Loss: 0.0125\n",
      "Epoch [2901/5000], Loss: 0.0117\n",
      "Epoch [3001/5000], Loss: 0.0123\n",
      "Epoch [3101/5000], Loss: 0.0126\n",
      "Epoch [3201/5000], Loss: 0.0130\n",
      "Epoch [3301/5000], Loss: 0.0108\n",
      "Epoch [3401/5000], Loss: 0.0109\n",
      "Epoch [3501/5000], Loss: 0.0109\n",
      "Epoch [3601/5000], Loss: 0.0106\n",
      "Epoch [3701/5000], Loss: 0.0103\n",
      "Epoch [3801/5000], Loss: 0.0112\n",
      "Epoch [3901/5000], Loss: 0.0098\n",
      "Epoch [4001/5000], Loss: 0.0090\n",
      "Epoch [4101/5000], Loss: 0.0097\n",
      "Epoch [4201/5000], Loss: 0.0078\n",
      "Epoch [4301/5000], Loss: 0.0083\n",
      "Epoch [4401/5000], Loss: 0.0083\n",
      "Epoch [4501/5000], Loss: 0.0078\n",
      "Epoch [4601/5000], Loss: 0.0072\n",
      "Epoch [4701/5000], Loss: 0.0084\n",
      "Epoch [4801/5000], Loss: 0.0071\n",
      "Epoch [4901/5000], Loss: 0.0070\n",
      "Train: 0.007320218 Test: 0.009084348630569266\n",
      "Epoch [1/5000], Loss: 0.9393\n",
      "Epoch [101/5000], Loss: 0.0348\n",
      "Epoch [201/5000], Loss: 0.0229\n",
      "Epoch [301/5000], Loss: 0.0181\n",
      "Epoch [401/5000], Loss: 0.0163\n",
      "Epoch [501/5000], Loss: 0.0158\n",
      "Epoch [601/5000], Loss: 0.0133\n",
      "Epoch [701/5000], Loss: 0.0125\n",
      "Epoch [801/5000], Loss: 0.0116\n",
      "Epoch [901/5000], Loss: 0.0114\n",
      "Epoch [1001/5000], Loss: 0.0105\n",
      "Epoch [1101/5000], Loss: 0.0105\n",
      "Epoch [1201/5000], Loss: 0.0101\n",
      "Epoch [1301/5000], Loss: 0.0097\n",
      "Epoch [1401/5000], Loss: 0.0089\n",
      "Epoch [1501/5000], Loss: 0.0094\n",
      "Epoch [1601/5000], Loss: 0.0087\n",
      "Epoch [1701/5000], Loss: 0.0088\n",
      "Epoch [1801/5000], Loss: 0.0096\n",
      "Epoch [1901/5000], Loss: 0.0080\n",
      "Epoch [2001/5000], Loss: 0.0081\n",
      "Epoch [2101/5000], Loss: 0.0089\n",
      "Epoch [2201/5000], Loss: 0.0075\n",
      "Epoch [2301/5000], Loss: 0.0084\n",
      "Epoch [2401/5000], Loss: 0.0080\n",
      "Epoch [2501/5000], Loss: 0.0075\n",
      "Epoch [2601/5000], Loss: 0.0074\n",
      "Epoch [2701/5000], Loss: 0.0074\n",
      "Epoch [2801/5000], Loss: 0.0068\n",
      "Epoch [2901/5000], Loss: 0.0073\n",
      "Epoch [3001/5000], Loss: 0.0067\n",
      "Epoch [3101/5000], Loss: 0.0074\n",
      "Epoch [3201/5000], Loss: 0.0064\n",
      "Epoch [3301/5000], Loss: 0.0079\n",
      "Epoch [3401/5000], Loss: 0.0064\n",
      "Epoch [3501/5000], Loss: 0.0069\n",
      "Epoch [3601/5000], Loss: 0.0065\n",
      "Epoch [3701/5000], Loss: 0.0068\n",
      "Epoch [3801/5000], Loss: 0.0069\n",
      "Epoch [3901/5000], Loss: 0.0060\n",
      "Epoch [4001/5000], Loss: 0.0066\n",
      "Epoch [4101/5000], Loss: 0.0062\n",
      "Epoch [4201/5000], Loss: 0.0058\n",
      "Epoch [4301/5000], Loss: 0.0058\n",
      "Epoch [4401/5000], Loss: 0.0057\n",
      "Epoch [4501/5000], Loss: 0.0056\n",
      "Epoch [4601/5000], Loss: 0.0052\n",
      "Epoch [4701/5000], Loss: 0.0057\n",
      "Epoch [4801/5000], Loss: 0.0057\n",
      "Epoch [4901/5000], Loss: 0.0053\n",
      "Train: 0.0049804584 Test: 0.0069910523297772725\n",
      "Epoch [1/5000], Loss: 1.0256\n",
      "Epoch [101/5000], Loss: 0.0340\n",
      "Epoch [201/5000], Loss: 0.0251\n",
      "Epoch [301/5000], Loss: 0.0212\n",
      "Epoch [401/5000], Loss: 0.0199\n",
      "Epoch [501/5000], Loss: 0.0163\n",
      "Epoch [601/5000], Loss: 0.0160\n",
      "Epoch [701/5000], Loss: 0.0146\n",
      "Epoch [801/5000], Loss: 0.0140\n",
      "Epoch [901/5000], Loss: 0.0136\n",
      "Epoch [1001/5000], Loss: 0.0137\n",
      "Epoch [1101/5000], Loss: 0.0125\n",
      "Epoch [1201/5000], Loss: 0.0122\n",
      "Epoch [1301/5000], Loss: 0.0125\n",
      "Epoch [1401/5000], Loss: 0.0119\n",
      "Epoch [1501/5000], Loss: 0.0116\n",
      "Epoch [1601/5000], Loss: 0.0122\n",
      "Epoch [1701/5000], Loss: 0.0117\n",
      "Epoch [1801/5000], Loss: 0.0114\n",
      "Epoch [1901/5000], Loss: 0.0116\n",
      "Epoch [2001/5000], Loss: 0.0110\n",
      "Epoch [2101/5000], Loss: 0.0119\n",
      "Epoch [2201/5000], Loss: 0.0106\n",
      "Epoch [2301/5000], Loss: 0.0109\n",
      "Epoch [2401/5000], Loss: 0.0112\n",
      "Epoch [2501/5000], Loss: 0.0109\n",
      "Epoch [2601/5000], Loss: 0.0132\n",
      "Epoch [2701/5000], Loss: 0.0112\n",
      "Epoch [2801/5000], Loss: 0.0097\n",
      "Epoch [2901/5000], Loss: 0.0097\n",
      "Epoch [3001/5000], Loss: 0.0100\n",
      "Epoch [3101/5000], Loss: 0.0095\n",
      "Epoch [3201/5000], Loss: 0.0097\n",
      "Epoch [3301/5000], Loss: 0.0094\n",
      "Epoch [3401/5000], Loss: 0.0094\n",
      "Epoch [3501/5000], Loss: 0.0094\n",
      "Epoch [3601/5000], Loss: 0.0091\n",
      "Epoch [3701/5000], Loss: 0.0091\n",
      "Epoch [3801/5000], Loss: 0.0099\n",
      "Epoch [3901/5000], Loss: 0.0091\n",
      "Epoch [4001/5000], Loss: 0.0091\n",
      "Epoch [4101/5000], Loss: 0.0092\n",
      "Epoch [4201/5000], Loss: 0.0088\n",
      "Epoch [4301/5000], Loss: 0.0088\n",
      "Epoch [4401/5000], Loss: 0.0089\n",
      "Epoch [4501/5000], Loss: 0.0096\n",
      "Epoch [4601/5000], Loss: 0.0089\n",
      "Epoch [4701/5000], Loss: 0.0090\n",
      "Epoch [4801/5000], Loss: 0.0089\n",
      "Epoch [4901/5000], Loss: 0.0087\n",
      "Train: 0.008778092 Test: 0.011033284490670982\n",
      "Epoch [1/5000], Loss: 1.6460\n",
      "Epoch [101/5000], Loss: 0.0419\n",
      "Epoch [201/5000], Loss: 0.0246\n",
      "Epoch [301/5000], Loss: 0.0199\n",
      "Epoch [401/5000], Loss: 0.0161\n",
      "Epoch [501/5000], Loss: 0.0138\n",
      "Epoch [601/5000], Loss: 0.0127\n",
      "Epoch [701/5000], Loss: 0.0117\n",
      "Epoch [801/5000], Loss: 0.0113\n",
      "Epoch [901/5000], Loss: 0.0114\n",
      "Epoch [1001/5000], Loss: 0.0122\n",
      "Epoch [1101/5000], Loss: 0.0114\n",
      "Epoch [1201/5000], Loss: 0.0108\n",
      "Epoch [1301/5000], Loss: 0.0099\n",
      "Epoch [1401/5000], Loss: 0.0107\n",
      "Epoch [1501/5000], Loss: 0.0097\n",
      "Epoch [1601/5000], Loss: 0.0090\n",
      "Epoch [1701/5000], Loss: 0.0090\n",
      "Epoch [1801/5000], Loss: 0.0088\n",
      "Epoch [1901/5000], Loss: 0.0088\n",
      "Epoch [2001/5000], Loss: 0.0088\n",
      "Epoch [2101/5000], Loss: 0.0099\n",
      "Epoch [2201/5000], Loss: 0.0083\n",
      "Epoch [2301/5000], Loss: 0.0090\n",
      "Epoch [2401/5000], Loss: 0.0083\n",
      "Epoch [2501/5000], Loss: 0.0082\n",
      "Epoch [2601/5000], Loss: 0.0076\n",
      "Epoch [2701/5000], Loss: 0.0089\n",
      "Epoch [2801/5000], Loss: 0.0080\n",
      "Epoch [2901/5000], Loss: 0.0076\n",
      "Epoch [3001/5000], Loss: 0.0074\n",
      "Epoch [3101/5000], Loss: 0.0075\n",
      "Epoch [3201/5000], Loss: 0.0071\n",
      "Epoch [3301/5000], Loss: 0.0080\n",
      "Epoch [3401/5000], Loss: 0.0068\n",
      "Epoch [3501/5000], Loss: 0.0066\n",
      "Epoch [3601/5000], Loss: 0.0066\n",
      "Epoch [3701/5000], Loss: 0.0065\n",
      "Epoch [3801/5000], Loss: 0.0064\n",
      "Epoch [3901/5000], Loss: 0.0091\n",
      "Epoch [4001/5000], Loss: 0.0069\n",
      "Epoch [4101/5000], Loss: 0.0067\n",
      "Epoch [4201/5000], Loss: 0.0061\n",
      "Epoch [4301/5000], Loss: 0.0062\n",
      "Epoch [4401/5000], Loss: 0.0061\n",
      "Epoch [4501/5000], Loss: 0.0061\n",
      "Epoch [4601/5000], Loss: 0.0065\n",
      "Epoch [4701/5000], Loss: 0.0064\n",
      "Epoch [4801/5000], Loss: 0.0082\n",
      "Epoch [4901/5000], Loss: 0.0070\n",
      "Train: 0.0061702835 Test: 0.008193699475900073\n",
      "Epoch [1/5000], Loss: 0.8973\n",
      "Epoch [101/5000], Loss: 0.0370\n",
      "Epoch [201/5000], Loss: 0.0289\n",
      "Epoch [301/5000], Loss: 0.0260\n",
      "Epoch [401/5000], Loss: 0.0192\n",
      "Epoch [501/5000], Loss: 0.0170\n",
      "Epoch [601/5000], Loss: 0.0161\n",
      "Epoch [701/5000], Loss: 0.0154\n",
      "Epoch [801/5000], Loss: 0.0146\n",
      "Epoch [901/5000], Loss: 0.0146\n",
      "Epoch [1001/5000], Loss: 0.0136\n",
      "Epoch [1101/5000], Loss: 0.0130\n",
      "Epoch [1201/5000], Loss: 0.0125\n",
      "Epoch [1301/5000], Loss: 0.0122\n",
      "Epoch [1401/5000], Loss: 0.0119\n",
      "Epoch [1501/5000], Loss: 0.0117\n",
      "Epoch [1601/5000], Loss: 0.0110\n",
      "Epoch [1701/5000], Loss: 0.0112\n",
      "Epoch [1801/5000], Loss: 0.0104\n",
      "Epoch [1901/5000], Loss: 0.0111\n",
      "Epoch [2001/5000], Loss: 0.0114\n",
      "Epoch [2101/5000], Loss: 0.0112\n",
      "Epoch [2201/5000], Loss: 0.0097\n",
      "Epoch [2301/5000], Loss: 0.0105\n",
      "Epoch [2401/5000], Loss: 0.0097\n",
      "Epoch [2501/5000], Loss: 0.0096\n",
      "Epoch [2601/5000], Loss: 0.0094\n",
      "Epoch [2701/5000], Loss: 0.0090\n",
      "Epoch [2801/5000], Loss: 0.0094\n",
      "Epoch [2901/5000], Loss: 0.0089\n",
      "Epoch [3001/5000], Loss: 0.0091\n",
      "Epoch [3101/5000], Loss: 0.0090\n",
      "Epoch [3201/5000], Loss: 0.0089\n",
      "Epoch [3301/5000], Loss: 0.0088\n",
      "Epoch [3401/5000], Loss: 0.0086\n",
      "Epoch [3501/5000], Loss: 0.0084\n",
      "Epoch [3601/5000], Loss: 0.0096\n",
      "Epoch [3701/5000], Loss: 0.0094\n",
      "Epoch [3801/5000], Loss: 0.0082\n",
      "Epoch [3901/5000], Loss: 0.0082\n",
      "Epoch [4001/5000], Loss: 0.0083\n",
      "Epoch [4101/5000], Loss: 0.0084\n",
      "Epoch [4201/5000], Loss: 0.0083\n",
      "Epoch [4301/5000], Loss: 0.0082\n",
      "Epoch [4401/5000], Loss: 0.0083\n",
      "Epoch [4501/5000], Loss: 0.0088\n",
      "Epoch [4601/5000], Loss: 0.0080\n",
      "Epoch [4701/5000], Loss: 0.0084\n",
      "Epoch [4801/5000], Loss: 0.0080\n",
      "Epoch [4901/5000], Loss: 0.0080\n",
      "Train: 0.009027376 Test: 0.011529463557560063\n",
      "Epoch [1/5000], Loss: 1.2881\n",
      "Epoch [101/5000], Loss: 0.0444\n",
      "Epoch [201/5000], Loss: 0.0269\n",
      "Epoch [301/5000], Loss: 0.0216\n",
      "Epoch [401/5000], Loss: 0.0197\n",
      "Epoch [501/5000], Loss: 0.0188\n",
      "Epoch [601/5000], Loss: 0.0183\n",
      "Epoch [701/5000], Loss: 0.0179\n",
      "Epoch [801/5000], Loss: 0.0180\n",
      "Epoch [901/5000], Loss: 0.0172\n",
      "Epoch [1001/5000], Loss: 0.0167\n",
      "Epoch [1101/5000], Loss: 0.0170\n",
      "Epoch [1201/5000], Loss: 0.0166\n",
      "Epoch [1301/5000], Loss: 0.0160\n",
      "Epoch [1401/5000], Loss: 0.0195\n",
      "Epoch [1501/5000], Loss: 0.0146\n",
      "Epoch [1601/5000], Loss: 0.0147\n",
      "Epoch [1701/5000], Loss: 0.0153\n",
      "Epoch [1801/5000], Loss: 0.0132\n",
      "Epoch [1901/5000], Loss: 0.0136\n",
      "Epoch [2001/5000], Loss: 0.0131\n",
      "Epoch [2101/5000], Loss: 0.0125\n",
      "Epoch [2201/5000], Loss: 0.0127\n",
      "Epoch [2301/5000], Loss: 0.0129\n",
      "Epoch [2401/5000], Loss: 0.0130\n",
      "Epoch [2501/5000], Loss: 0.0128\n",
      "Epoch [2601/5000], Loss: 0.0120\n",
      "Epoch [2701/5000], Loss: 0.0134\n",
      "Epoch [2801/5000], Loss: 0.0115\n",
      "Epoch [2901/5000], Loss: 0.0114\n",
      "Epoch [3001/5000], Loss: 0.0115\n",
      "Epoch [3101/5000], Loss: 0.0109\n",
      "Epoch [3201/5000], Loss: 0.0108\n",
      "Epoch [3301/5000], Loss: 0.0111\n",
      "Epoch [3401/5000], Loss: 0.0107\n",
      "Epoch [3501/5000], Loss: 0.0110\n",
      "Epoch [3601/5000], Loss: 0.0104\n",
      "Epoch [3701/5000], Loss: 0.0182\n",
      "Epoch [3801/5000], Loss: 0.0109\n",
      "Epoch [3901/5000], Loss: 0.0105\n",
      "Epoch [4001/5000], Loss: 0.0110\n",
      "Epoch [4101/5000], Loss: 0.0105\n",
      "Epoch [4201/5000], Loss: 0.0106\n",
      "Epoch [4301/5000], Loss: 0.0116\n",
      "Epoch [4401/5000], Loss: 0.0131\n",
      "Epoch [4501/5000], Loss: 0.0111\n",
      "Epoch [4601/5000], Loss: 0.0104\n",
      "Epoch [4701/5000], Loss: 0.0114\n",
      "Epoch [4801/5000], Loss: 0.0100\n",
      "Epoch [4901/5000], Loss: 0.0099\n",
      "Train: 0.009980868 Test: 0.012322905067924926\n",
      "Epoch [1/5000], Loss: 0.7733\n",
      "Epoch [101/5000], Loss: 0.0275\n",
      "Epoch [201/5000], Loss: 0.0204\n",
      "Epoch [301/5000], Loss: 0.0155\n",
      "Epoch [401/5000], Loss: 0.0134\n",
      "Epoch [501/5000], Loss: 0.0122\n",
      "Epoch [601/5000], Loss: 0.0114\n",
      "Epoch [701/5000], Loss: 0.0106\n",
      "Epoch [801/5000], Loss: 0.0103\n",
      "Epoch [901/5000], Loss: 0.0104\n",
      "Epoch [1001/5000], Loss: 0.0094\n",
      "Epoch [1101/5000], Loss: 0.0093\n",
      "Epoch [1201/5000], Loss: 0.0106\n",
      "Epoch [1301/5000], Loss: 0.0082\n",
      "Epoch [1401/5000], Loss: 0.0078\n",
      "Epoch [1501/5000], Loss: 0.0077\n",
      "Epoch [1601/5000], Loss: 0.0075\n",
      "Epoch [1701/5000], Loss: 0.0072\n",
      "Epoch [1801/5000], Loss: 0.0079\n",
      "Epoch [1901/5000], Loss: 0.0068\n",
      "Epoch [2001/5000], Loss: 0.0067\n",
      "Epoch [2101/5000], Loss: 0.0065\n",
      "Epoch [2201/5000], Loss: 0.0067\n",
      "Epoch [2301/5000], Loss: 0.0087\n",
      "Epoch [2401/5000], Loss: 0.0065\n",
      "Epoch [2501/5000], Loss: 0.0062\n",
      "Epoch [2601/5000], Loss: 0.0061\n",
      "Epoch [2701/5000], Loss: 0.0063\n",
      "Epoch [2801/5000], Loss: 0.0060\n",
      "Epoch [2901/5000], Loss: 0.0059\n",
      "Epoch [3001/5000], Loss: 0.0059\n",
      "Epoch [3101/5000], Loss: 0.0055\n",
      "Epoch [3201/5000], Loss: 0.0058\n",
      "Epoch [3301/5000], Loss: 0.0054\n",
      "Epoch [3401/5000], Loss: 0.0056\n",
      "Epoch [3501/5000], Loss: 0.0053\n",
      "Epoch [3601/5000], Loss: 0.0053\n",
      "Epoch [3701/5000], Loss: 0.0057\n",
      "Epoch [3801/5000], Loss: 0.0050\n",
      "Epoch [3901/5000], Loss: 0.0055\n",
      "Epoch [4001/5000], Loss: 0.0054\n",
      "Epoch [4101/5000], Loss: 0.0063\n",
      "Epoch [4201/5000], Loss: 0.0049\n",
      "Epoch [4301/5000], Loss: 0.0055\n",
      "Epoch [4401/5000], Loss: 0.0048\n",
      "Epoch [4501/5000], Loss: 0.0052\n",
      "Epoch [4601/5000], Loss: 0.0048\n",
      "Epoch [4701/5000], Loss: 0.0046\n",
      "Epoch [4801/5000], Loss: 0.0049\n",
      "Epoch [4901/5000], Loss: 0.0050\n",
      "Train: 0.0045781345 Test: 0.006790028923811614\n",
      "Epoch [1/5000], Loss: 1.3951\n",
      "Epoch [101/5000], Loss: 0.0375\n",
      "Epoch [201/5000], Loss: 0.0291\n",
      "Epoch [301/5000], Loss: 0.0223\n",
      "Epoch [401/5000], Loss: 0.0198\n",
      "Epoch [501/5000], Loss: 0.0194\n",
      "Epoch [601/5000], Loss: 0.0184\n",
      "Epoch [701/5000], Loss: 0.0174\n",
      "Epoch [801/5000], Loss: 0.0151\n",
      "Epoch [901/5000], Loss: 0.0146\n",
      "Epoch [1001/5000], Loss: 0.0133\n",
      "Epoch [1101/5000], Loss: 0.0132\n",
      "Epoch [1201/5000], Loss: 0.0131\n",
      "Epoch [1301/5000], Loss: 0.0123\n",
      "Epoch [1401/5000], Loss: 0.0126\n",
      "Epoch [1501/5000], Loss: 0.0123\n",
      "Epoch [1601/5000], Loss: 0.0120\n",
      "Epoch [1701/5000], Loss: 0.0119\n",
      "Epoch [1801/5000], Loss: 0.0114\n",
      "Epoch [1901/5000], Loss: 0.0112\n",
      "Epoch [2001/5000], Loss: 0.0114\n",
      "Epoch [2101/5000], Loss: 0.0101\n",
      "Epoch [2201/5000], Loss: 0.0092\n",
      "Epoch [2301/5000], Loss: 0.0089\n",
      "Epoch [2401/5000], Loss: 0.0088\n",
      "Epoch [2501/5000], Loss: 0.0090\n",
      "Epoch [2601/5000], Loss: 0.0084\n",
      "Epoch [2701/5000], Loss: 0.0088\n",
      "Epoch [2801/5000], Loss: 0.0082\n",
      "Epoch [2901/5000], Loss: 0.0081\n",
      "Epoch [3001/5000], Loss: 0.0079\n",
      "Epoch [3101/5000], Loss: 0.0074\n",
      "Epoch [3201/5000], Loss: 0.0083\n",
      "Epoch [3301/5000], Loss: 0.0070\n",
      "Epoch [3401/5000], Loss: 0.0076\n",
      "Epoch [3501/5000], Loss: 0.0077\n",
      "Epoch [3601/5000], Loss: 0.0081\n",
      "Epoch [3701/5000], Loss: 0.0068\n",
      "Epoch [3801/5000], Loss: 0.0067\n",
      "Epoch [3901/5000], Loss: 0.0074\n",
      "Epoch [4001/5000], Loss: 0.0073\n",
      "Epoch [4101/5000], Loss: 0.0079\n",
      "Epoch [4201/5000], Loss: 0.0070\n",
      "Epoch [4301/5000], Loss: 0.0061\n",
      "Epoch [4401/5000], Loss: 0.0065\n",
      "Epoch [4501/5000], Loss: 0.0070\n",
      "Epoch [4601/5000], Loss: 0.0073\n",
      "Epoch [4701/5000], Loss: 0.0063\n",
      "Epoch [4801/5000], Loss: 0.0066\n",
      "Epoch [4901/5000], Loss: 0.0066\n",
      "Train: 0.0060040704 Test: 0.007994767801310804\n",
      "Epoch [1/5000], Loss: 1.7845\n",
      "Epoch [101/5000], Loss: 0.0382\n",
      "Epoch [201/5000], Loss: 0.0228\n",
      "Epoch [301/5000], Loss: 0.0184\n",
      "Epoch [401/5000], Loss: 0.0171\n",
      "Epoch [501/5000], Loss: 0.0157\n",
      "Epoch [601/5000], Loss: 0.0143\n",
      "Epoch [701/5000], Loss: 0.0124\n",
      "Epoch [801/5000], Loss: 0.0117\n",
      "Epoch [901/5000], Loss: 0.0107\n",
      "Epoch [1001/5000], Loss: 0.0105\n",
      "Epoch [1101/5000], Loss: 0.0117\n",
      "Epoch [1201/5000], Loss: 0.0106\n",
      "Epoch [1301/5000], Loss: 0.0107\n",
      "Epoch [1401/5000], Loss: 0.0090\n",
      "Epoch [1501/5000], Loss: 0.0097\n",
      "Epoch [1601/5000], Loss: 0.0086\n",
      "Epoch [1701/5000], Loss: 0.0098\n",
      "Epoch [1801/5000], Loss: 0.0087\n",
      "Epoch [1901/5000], Loss: 0.0093\n",
      "Epoch [2001/5000], Loss: 0.0093\n",
      "Epoch [2101/5000], Loss: 0.0081\n",
      "Epoch [2201/5000], Loss: 0.0083\n",
      "Epoch [2301/5000], Loss: 0.0076\n",
      "Epoch [2401/5000], Loss: 0.0074\n",
      "Epoch [2501/5000], Loss: 0.0079\n",
      "Epoch [2601/5000], Loss: 0.0107\n",
      "Epoch [2701/5000], Loss: 0.0073\n",
      "Epoch [2801/5000], Loss: 0.0105\n",
      "Epoch [2901/5000], Loss: 0.0071\n",
      "Epoch [3001/5000], Loss: 0.0069\n",
      "Epoch [3101/5000], Loss: 0.0086\n",
      "Epoch [3201/5000], Loss: 0.0065\n",
      "Epoch [3301/5000], Loss: 0.0066\n",
      "Epoch [3401/5000], Loss: 0.0064\n",
      "Epoch [3501/5000], Loss: 0.0072\n",
      "Epoch [3601/5000], Loss: 0.0067\n",
      "Epoch [3701/5000], Loss: 0.0071\n",
      "Epoch [3801/5000], Loss: 0.0065\n",
      "Epoch [3901/5000], Loss: 0.0064\n",
      "Epoch [4001/5000], Loss: 0.0062\n",
      "Epoch [4101/5000], Loss: 0.0064\n",
      "Epoch [4201/5000], Loss: 0.0060\n",
      "Epoch [4301/5000], Loss: 0.0063\n",
      "Epoch [4401/5000], Loss: 0.0068\n",
      "Epoch [4501/5000], Loss: 0.0059\n",
      "Epoch [4601/5000], Loss: 0.0058\n",
      "Epoch [4701/5000], Loss: 0.0057\n",
      "Epoch [4801/5000], Loss: 0.0066\n",
      "Epoch [4901/5000], Loss: 0.0061\n",
      "Train: 0.005867244 Test: 0.00720726140572779\n",
      "Epoch [1/5000], Loss: 0.7678\n",
      "Epoch [101/5000], Loss: 0.0275\n",
      "Epoch [201/5000], Loss: 0.0206\n",
      "Epoch [301/5000], Loss: 0.0171\n",
      "Epoch [401/5000], Loss: 0.0154\n",
      "Epoch [501/5000], Loss: 0.0139\n",
      "Epoch [601/5000], Loss: 0.0123\n",
      "Epoch [701/5000], Loss: 0.0111\n",
      "Epoch [801/5000], Loss: 0.0102\n",
      "Epoch [901/5000], Loss: 0.0110\n",
      "Epoch [1001/5000], Loss: 0.0096\n",
      "Epoch [1101/5000], Loss: 0.0094\n",
      "Epoch [1201/5000], Loss: 0.0095\n",
      "Epoch [1301/5000], Loss: 0.0085\n",
      "Epoch [1401/5000], Loss: 0.0083\n",
      "Epoch [1501/5000], Loss: 0.0080\n",
      "Epoch [1601/5000], Loss: 0.0080\n",
      "Epoch [1701/5000], Loss: 0.0077\n",
      "Epoch [1801/5000], Loss: 0.0077\n",
      "Epoch [1901/5000], Loss: 0.0077\n",
      "Epoch [2001/5000], Loss: 0.0076\n",
      "Epoch [2101/5000], Loss: 0.0070\n",
      "Epoch [2201/5000], Loss: 0.0070\n",
      "Epoch [2301/5000], Loss: 0.0066\n",
      "Epoch [2401/5000], Loss: 0.0070\n",
      "Epoch [2501/5000], Loss: 0.0064\n",
      "Epoch [2601/5000], Loss: 0.0073\n",
      "Epoch [2701/5000], Loss: 0.0061\n",
      "Epoch [2801/5000], Loss: 0.0066\n",
      "Epoch [2901/5000], Loss: 0.0057\n",
      "Epoch [3001/5000], Loss: 0.0055\n",
      "Epoch [3101/5000], Loss: 0.0066\n",
      "Epoch [3201/5000], Loss: 0.0054\n",
      "Epoch [3301/5000], Loss: 0.0056\n",
      "Epoch [3401/5000], Loss: 0.0053\n",
      "Epoch [3501/5000], Loss: 0.0053\n",
      "Epoch [3601/5000], Loss: 0.0054\n",
      "Epoch [3701/5000], Loss: 0.0051\n",
      "Epoch [3801/5000], Loss: 0.0050\n",
      "Epoch [3901/5000], Loss: 0.0050\n",
      "Epoch [4001/5000], Loss: 0.0054\n",
      "Epoch [4101/5000], Loss: 0.0053\n",
      "Epoch [4201/5000], Loss: 0.0051\n",
      "Epoch [4301/5000], Loss: 0.0052\n",
      "Epoch [4401/5000], Loss: 0.0056\n",
      "Epoch [4501/5000], Loss: 0.0045\n",
      "Epoch [4601/5000], Loss: 0.0044\n",
      "Epoch [4701/5000], Loss: 0.0046\n",
      "Epoch [4801/5000], Loss: 0.0044\n",
      "Epoch [4901/5000], Loss: 0.0049\n",
      "Train: 0.004146501 Test: 0.006204500841866928\n",
      "Epoch [1/5000], Loss: 1.4635\n",
      "Epoch [101/5000], Loss: 0.0625\n",
      "Epoch [201/5000], Loss: 0.0331\n",
      "Epoch [301/5000], Loss: 0.0263\n",
      "Epoch [401/5000], Loss: 0.0225\n",
      "Epoch [501/5000], Loss: 0.0214\n",
      "Epoch [601/5000], Loss: 0.0200\n",
      "Epoch [701/5000], Loss: 0.0191\n",
      "Epoch [801/5000], Loss: 0.0186\n",
      "Epoch [901/5000], Loss: 0.0187\n",
      "Epoch [1001/5000], Loss: 0.0182\n",
      "Epoch [1101/5000], Loss: 0.0190\n",
      "Epoch [1201/5000], Loss: 0.0193\n",
      "Epoch [1301/5000], Loss: 0.0178\n",
      "Epoch [1401/5000], Loss: 0.0177\n",
      "Epoch [1501/5000], Loss: 0.0197\n",
      "Epoch [1601/5000], Loss: 0.0177\n",
      "Epoch [1701/5000], Loss: 0.0173\n",
      "Epoch [1801/5000], Loss: 0.0182\n",
      "Epoch [1901/5000], Loss: 0.0182\n",
      "Epoch [2001/5000], Loss: 0.0182\n",
      "Epoch [2101/5000], Loss: 0.0172\n",
      "Epoch [2201/5000], Loss: 0.0166\n",
      "Epoch [2301/5000], Loss: 0.0177\n",
      "Epoch [2401/5000], Loss: 0.0171\n",
      "Epoch [2501/5000], Loss: 0.0173\n",
      "Epoch [2601/5000], Loss: 0.0163\n",
      "Epoch [2701/5000], Loss: 0.0166\n",
      "Epoch [2801/5000], Loss: 0.0174\n",
      "Epoch [2901/5000], Loss: 0.0160\n",
      "Epoch [3001/5000], Loss: 0.0169\n",
      "Epoch [3101/5000], Loss: 0.0158\n",
      "Epoch [3201/5000], Loss: 0.0159\n",
      "Epoch [3301/5000], Loss: 0.0168\n",
      "Epoch [3401/5000], Loss: 0.0171\n",
      "Epoch [3501/5000], Loss: 0.0170\n",
      "Epoch [3601/5000], Loss: 0.0154\n",
      "Epoch [3701/5000], Loss: 0.0163\n",
      "Epoch [3801/5000], Loss: 0.0164\n",
      "Epoch [3901/5000], Loss: 0.0159\n",
      "Epoch [4001/5000], Loss: 0.0153\n",
      "Epoch [4101/5000], Loss: 0.0193\n",
      "Epoch [4201/5000], Loss: 0.0156\n",
      "Epoch [4301/5000], Loss: 0.0151\n",
      "Epoch [4401/5000], Loss: 0.0152\n",
      "Epoch [4501/5000], Loss: 0.0147\n",
      "Epoch [4601/5000], Loss: 0.0146\n",
      "Epoch [4701/5000], Loss: 0.0151\n",
      "Epoch [4801/5000], Loss: 0.0147\n",
      "Epoch [4901/5000], Loss: 0.0152\n",
      "Train: 0.014936755 Test: 0.01623367557773896\n",
      "Epoch [1/5000], Loss: 0.9673\n",
      "Epoch [101/5000], Loss: 0.0318\n",
      "Epoch [201/5000], Loss: 0.0209\n",
      "Epoch [301/5000], Loss: 0.0168\n",
      "Epoch [401/5000], Loss: 0.0150\n",
      "Epoch [501/5000], Loss: 0.0135\n",
      "Epoch [601/5000], Loss: 0.0125\n",
      "Epoch [701/5000], Loss: 0.0119\n",
      "Epoch [801/5000], Loss: 0.0117\n",
      "Epoch [901/5000], Loss: 0.0100\n",
      "Epoch [1001/5000], Loss: 0.0097\n",
      "Epoch [1101/5000], Loss: 0.0095\n",
      "Epoch [1201/5000], Loss: 0.0091\n",
      "Epoch [1301/5000], Loss: 0.0086\n",
      "Epoch [1401/5000], Loss: 0.0085\n",
      "Epoch [1501/5000], Loss: 0.0082\n",
      "Epoch [1601/5000], Loss: 0.0083\n",
      "Epoch [1701/5000], Loss: 0.0081\n",
      "Epoch [1801/5000], Loss: 0.0081\n",
      "Epoch [1901/5000], Loss: 0.0076\n",
      "Epoch [2001/5000], Loss: 0.0075\n",
      "Epoch [2101/5000], Loss: 0.0070\n",
      "Epoch [2201/5000], Loss: 0.0071\n",
      "Epoch [2301/5000], Loss: 0.0069\n",
      "Epoch [2401/5000], Loss: 0.0066\n",
      "Epoch [2501/5000], Loss: 0.0063\n",
      "Epoch [2601/5000], Loss: 0.0065\n",
      "Epoch [2701/5000], Loss: 0.0066\n",
      "Epoch [2801/5000], Loss: 0.0066\n",
      "Epoch [2901/5000], Loss: 0.0072\n",
      "Epoch [3001/5000], Loss: 0.0063\n",
      "Epoch [3101/5000], Loss: 0.0072\n",
      "Epoch [3201/5000], Loss: 0.0056\n",
      "Epoch [3301/5000], Loss: 0.0059\n",
      "Epoch [3401/5000], Loss: 0.0060\n",
      "Epoch [3501/5000], Loss: 0.0054\n",
      "Epoch [3601/5000], Loss: 0.0057\n",
      "Epoch [3701/5000], Loss: 0.0054\n",
      "Epoch [3801/5000], Loss: 0.0058\n",
      "Epoch [3901/5000], Loss: 0.0053\n",
      "Epoch [4001/5000], Loss: 0.0056\n",
      "Epoch [4101/5000], Loss: 0.0053\n",
      "Epoch [4201/5000], Loss: 0.0055\n",
      "Epoch [4301/5000], Loss: 0.0053\n",
      "Epoch [4401/5000], Loss: 0.0049\n",
      "Epoch [4501/5000], Loss: 0.0070\n",
      "Epoch [4601/5000], Loss: 0.0051\n",
      "Epoch [4701/5000], Loss: 0.0048\n",
      "Epoch [4801/5000], Loss: 0.0057\n",
      "Epoch [4901/5000], Loss: 0.0052\n",
      "Train: 0.0052285586 Test: 0.0071651359902220844\n",
      "Epoch [1/5000], Loss: 1.0353\n",
      "Epoch [101/5000], Loss: 0.0318\n",
      "Epoch [201/5000], Loss: 0.0217\n",
      "Epoch [301/5000], Loss: 0.0188\n",
      "Epoch [401/5000], Loss: 0.0151\n",
      "Epoch [501/5000], Loss: 0.0130\n",
      "Epoch [601/5000], Loss: 0.0120\n",
      "Epoch [701/5000], Loss: 0.0118\n",
      "Epoch [801/5000], Loss: 0.0113\n",
      "Epoch [901/5000], Loss: 0.0105\n",
      "Epoch [1001/5000], Loss: 0.0099\n",
      "Epoch [1101/5000], Loss: 0.0094\n",
      "Epoch [1201/5000], Loss: 0.0093\n",
      "Epoch [1301/5000], Loss: 0.0090\n",
      "Epoch [1401/5000], Loss: 0.0086\n",
      "Epoch [1501/5000], Loss: 0.0084\n",
      "Epoch [1601/5000], Loss: 0.0086\n",
      "Epoch [1701/5000], Loss: 0.0078\n",
      "Epoch [1801/5000], Loss: 0.0079\n",
      "Epoch [1901/5000], Loss: 0.0077\n",
      "Epoch [2001/5000], Loss: 0.0091\n",
      "Epoch [2101/5000], Loss: 0.0071\n",
      "Epoch [2201/5000], Loss: 0.0080\n",
      "Epoch [2301/5000], Loss: 0.0074\n",
      "Epoch [2401/5000], Loss: 0.0068\n",
      "Epoch [2501/5000], Loss: 0.0072\n",
      "Epoch [2601/5000], Loss: 0.0065\n",
      "Epoch [2701/5000], Loss: 0.0065\n",
      "Epoch [2801/5000], Loss: 0.0064\n",
      "Epoch [2901/5000], Loss: 0.0074\n",
      "Epoch [3001/5000], Loss: 0.0066\n",
      "Epoch [3101/5000], Loss: 0.0060\n",
      "Epoch [3201/5000], Loss: 0.0062\n",
      "Epoch [3301/5000], Loss: 0.0062\n",
      "Epoch [3401/5000], Loss: 0.0057\n",
      "Epoch [3501/5000], Loss: 0.0056\n",
      "Epoch [3601/5000], Loss: 0.0057\n",
      "Epoch [3701/5000], Loss: 0.0055\n",
      "Epoch [3801/5000], Loss: 0.0065\n",
      "Epoch [3901/5000], Loss: 0.0060\n",
      "Epoch [4001/5000], Loss: 0.0057\n",
      "Epoch [4101/5000], Loss: 0.0071\n",
      "Epoch [4201/5000], Loss: 0.0051\n",
      "Epoch [4301/5000], Loss: 0.0052\n",
      "Epoch [4401/5000], Loss: 0.0051\n",
      "Epoch [4501/5000], Loss: 0.0055\n",
      "Epoch [4601/5000], Loss: 0.0049\n",
      "Epoch [4701/5000], Loss: 0.0049\n",
      "Epoch [4801/5000], Loss: 0.0057\n",
      "Epoch [4901/5000], Loss: 0.0047\n",
      "Train: 0.0051207603 Test: 0.0070732686575243784\n",
      "Epoch [1/5000], Loss: 1.5068\n",
      "Epoch [101/5000], Loss: 0.0464\n",
      "Epoch [201/5000], Loss: 0.0268\n",
      "Epoch [301/5000], Loss: 0.0225\n",
      "Epoch [401/5000], Loss: 0.0204\n",
      "Epoch [501/5000], Loss: 0.0187\n",
      "Epoch [601/5000], Loss: 0.0182\n",
      "Epoch [701/5000], Loss: 0.0180\n",
      "Epoch [801/5000], Loss: 0.0172\n",
      "Epoch [901/5000], Loss: 0.0171\n",
      "Epoch [1001/5000], Loss: 0.0144\n",
      "Epoch [1101/5000], Loss: 0.0132\n",
      "Epoch [1201/5000], Loss: 0.0138\n",
      "Epoch [1301/5000], Loss: 0.0132\n",
      "Epoch [1401/5000], Loss: 0.0131\n",
      "Epoch [1501/5000], Loss: 0.0131\n",
      "Epoch [1601/5000], Loss: 0.0152\n",
      "Epoch [1701/5000], Loss: 0.0122\n",
      "Epoch [1801/5000], Loss: 0.0125\n",
      "Epoch [1901/5000], Loss: 0.0120\n",
      "Epoch [2001/5000], Loss: 0.0119\n",
      "Epoch [2101/5000], Loss: 0.0119\n",
      "Epoch [2201/5000], Loss: 0.0119\n",
      "Epoch [2301/5000], Loss: 0.0129\n",
      "Epoch [2401/5000], Loss: 0.0117\n",
      "Epoch [2501/5000], Loss: 0.0124\n",
      "Epoch [2601/5000], Loss: 0.0117\n",
      "Epoch [2701/5000], Loss: 0.0116\n",
      "Epoch [2801/5000], Loss: 0.0112\n",
      "Epoch [2901/5000], Loss: 0.0121\n",
      "Epoch [3001/5000], Loss: 0.0108\n",
      "Epoch [3101/5000], Loss: 0.0112\n",
      "Epoch [3201/5000], Loss: 0.0105\n",
      "Epoch [3301/5000], Loss: 0.0097\n",
      "Epoch [3401/5000], Loss: 0.0102\n",
      "Epoch [3501/5000], Loss: 0.0122\n",
      "Epoch [3601/5000], Loss: 0.0106\n",
      "Epoch [3701/5000], Loss: 0.0103\n",
      "Epoch [3801/5000], Loss: 0.0096\n",
      "Epoch [3901/5000], Loss: 0.0087\n",
      "Epoch [4001/5000], Loss: 0.0086\n",
      "Epoch [4101/5000], Loss: 0.0082\n",
      "Epoch [4201/5000], Loss: 0.0079\n",
      "Epoch [4301/5000], Loss: 0.0079\n",
      "Epoch [4401/5000], Loss: 0.0084\n",
      "Epoch [4501/5000], Loss: 0.0090\n",
      "Epoch [4601/5000], Loss: 0.0076\n",
      "Epoch [4701/5000], Loss: 0.0074\n",
      "Epoch [4801/5000], Loss: 0.0079\n",
      "Epoch [4901/5000], Loss: 0.0073\n",
      "Train: 0.007422631 Test: 0.009356113695074726\n",
      "Epoch [1/5000], Loss: 0.7866\n",
      "Epoch [101/5000], Loss: 0.0299\n",
      "Epoch [201/5000], Loss: 0.0196\n",
      "Epoch [301/5000], Loss: 0.0168\n",
      "Epoch [401/5000], Loss: 0.0145\n",
      "Epoch [501/5000], Loss: 0.0143\n",
      "Epoch [601/5000], Loss: 0.0129\n",
      "Epoch [701/5000], Loss: 0.0125\n",
      "Epoch [801/5000], Loss: 0.0120\n",
      "Epoch [901/5000], Loss: 0.0112\n",
      "Epoch [1001/5000], Loss: 0.0106\n",
      "Epoch [1101/5000], Loss: 0.0099\n",
      "Epoch [1201/5000], Loss: 0.0096\n",
      "Epoch [1301/5000], Loss: 0.0087\n",
      "Epoch [1401/5000], Loss: 0.0089\n",
      "Epoch [1501/5000], Loss: 0.0087\n",
      "Epoch [1601/5000], Loss: 0.0080\n",
      "Epoch [1701/5000], Loss: 0.0081\n",
      "Epoch [1801/5000], Loss: 0.0079\n",
      "Epoch [1901/5000], Loss: 0.0076\n",
      "Epoch [2001/5000], Loss: 0.0078\n",
      "Epoch [2101/5000], Loss: 0.0069\n",
      "Epoch [2201/5000], Loss: 0.0073\n",
      "Epoch [2301/5000], Loss: 0.0067\n",
      "Epoch [2401/5000], Loss: 0.0064\n",
      "Epoch [2501/5000], Loss: 0.0065\n",
      "Epoch [2601/5000], Loss: 0.0064\n",
      "Epoch [2701/5000], Loss: 0.0065\n",
      "Epoch [2801/5000], Loss: 0.0060\n",
      "Epoch [2901/5000], Loss: 0.0066\n",
      "Epoch [3001/5000], Loss: 0.0058\n",
      "Epoch [3101/5000], Loss: 0.0067\n",
      "Epoch [3201/5000], Loss: 0.0068\n",
      "Epoch [3301/5000], Loss: 0.0065\n",
      "Epoch [3401/5000], Loss: 0.0062\n",
      "Epoch [3501/5000], Loss: 0.0062\n",
      "Epoch [3601/5000], Loss: 0.0059\n",
      "Epoch [3701/5000], Loss: 0.0055\n",
      "Epoch [3801/5000], Loss: 0.0055\n",
      "Epoch [3901/5000], Loss: 0.0052\n",
      "Epoch [4001/5000], Loss: 0.0053\n",
      "Epoch [4101/5000], Loss: 0.0050\n",
      "Epoch [4201/5000], Loss: 0.0051\n",
      "Epoch [4301/5000], Loss: 0.0050\n",
      "Epoch [4401/5000], Loss: 0.0050\n",
      "Epoch [4501/5000], Loss: 0.0048\n",
      "Epoch [4601/5000], Loss: 0.0053\n",
      "Epoch [4701/5000], Loss: 0.0048\n",
      "Epoch [4801/5000], Loss: 0.0054\n",
      "Epoch [4901/5000], Loss: 0.0049\n",
      "Train: 0.0051583215 Test: 0.0075949260266106695\n",
      "Epoch [1/5000], Loss: 1.0983\n",
      "Epoch [101/5000], Loss: 0.0331\n",
      "Epoch [201/5000], Loss: 0.0228\n",
      "Epoch [301/5000], Loss: 0.0193\n",
      "Epoch [401/5000], Loss: 0.0165\n",
      "Epoch [501/5000], Loss: 0.0150\n",
      "Epoch [601/5000], Loss: 0.0138\n",
      "Epoch [701/5000], Loss: 0.0117\n",
      "Epoch [801/5000], Loss: 0.0109\n",
      "Epoch [901/5000], Loss: 0.0106\n",
      "Epoch [1001/5000], Loss: 0.0097\n",
      "Epoch [1101/5000], Loss: 0.0109\n",
      "Epoch [1201/5000], Loss: 0.0091\n",
      "Epoch [1301/5000], Loss: 0.0090\n",
      "Epoch [1401/5000], Loss: 0.0091\n",
      "Epoch [1501/5000], Loss: 0.0086\n",
      "Epoch [1601/5000], Loss: 0.0083\n",
      "Epoch [1701/5000], Loss: 0.0083\n",
      "Epoch [1801/5000], Loss: 0.0079\n",
      "Epoch [1901/5000], Loss: 0.0076\n",
      "Epoch [2001/5000], Loss: 0.0075\n",
      "Epoch [2101/5000], Loss: 0.0081\n",
      "Epoch [2201/5000], Loss: 0.0073\n",
      "Epoch [2301/5000], Loss: 0.0076\n",
      "Epoch [2401/5000], Loss: 0.0081\n",
      "Epoch [2501/5000], Loss: 0.0072\n",
      "Epoch [2601/5000], Loss: 0.0072\n",
      "Epoch [2701/5000], Loss: 0.0070\n",
      "Epoch [2801/5000], Loss: 0.0094\n",
      "Epoch [2901/5000], Loss: 0.0071\n",
      "Epoch [3001/5000], Loss: 0.0063\n",
      "Epoch [3101/5000], Loss: 0.0063\n",
      "Epoch [3201/5000], Loss: 0.0061\n",
      "Epoch [3301/5000], Loss: 0.0062\n",
      "Epoch [3401/5000], Loss: 0.0060\n",
      "Epoch [3501/5000], Loss: 0.0070\n",
      "Epoch [3601/5000], Loss: 0.0066\n",
      "Epoch [3701/5000], Loss: 0.0055\n",
      "Epoch [3801/5000], Loss: 0.0057\n",
      "Epoch [3901/5000], Loss: 0.0063\n",
      "Epoch [4001/5000], Loss: 0.0060\n",
      "Epoch [4101/5000], Loss: 0.0054\n",
      "Epoch [4201/5000], Loss: 0.0052\n",
      "Epoch [4301/5000], Loss: 0.0050\n",
      "Epoch [4401/5000], Loss: 0.0054\n",
      "Epoch [4501/5000], Loss: 0.0064\n",
      "Epoch [4601/5000], Loss: 0.0049\n",
      "Epoch [4701/5000], Loss: 0.0051\n",
      "Epoch [4801/5000], Loss: 0.0050\n",
      "Epoch [4901/5000], Loss: 0.0050\n",
      "Train: 0.0054569044 Test: 0.007865300789834223\n",
      "Epoch [1/5000], Loss: 1.0708\n",
      "Epoch [101/5000], Loss: 0.0369\n",
      "Epoch [201/5000], Loss: 0.0242\n",
      "Epoch [301/5000], Loss: 0.0181\n",
      "Epoch [401/5000], Loss: 0.0152\n",
      "Epoch [501/5000], Loss: 0.0146\n",
      "Epoch [601/5000], Loss: 0.0136\n",
      "Epoch [701/5000], Loss: 0.0126\n",
      "Epoch [801/5000], Loss: 0.0152\n",
      "Epoch [901/5000], Loss: 0.0106\n",
      "Epoch [1001/5000], Loss: 0.0100\n",
      "Epoch [1101/5000], Loss: 0.0099\n",
      "Epoch [1201/5000], Loss: 0.0130\n",
      "Epoch [1301/5000], Loss: 0.0094\n",
      "Epoch [1401/5000], Loss: 0.0089\n",
      "Epoch [1501/5000], Loss: 0.0090\n",
      "Epoch [1601/5000], Loss: 0.0093\n",
      "Epoch [1701/5000], Loss: 0.0080\n",
      "Epoch [1801/5000], Loss: 0.0083\n",
      "Epoch [1901/5000], Loss: 0.0081\n",
      "Epoch [2001/5000], Loss: 0.0083\n",
      "Epoch [2101/5000], Loss: 0.0074\n",
      "Epoch [2201/5000], Loss: 0.0077\n",
      "Epoch [2301/5000], Loss: 0.0072\n",
      "Epoch [2401/5000], Loss: 0.0084\n",
      "Epoch [2501/5000], Loss: 0.0075\n",
      "Epoch [2601/5000], Loss: 0.0072\n",
      "Epoch [2701/5000], Loss: 0.0067\n",
      "Epoch [2801/5000], Loss: 0.0066\n",
      "Epoch [2901/5000], Loss: 0.0070\n",
      "Epoch [3001/5000], Loss: 0.0070\n",
      "Epoch [3101/5000], Loss: 0.0066\n",
      "Epoch [3201/5000], Loss: 0.0067\n",
      "Epoch [3301/5000], Loss: 0.0069\n",
      "Epoch [3401/5000], Loss: 0.0066\n",
      "Epoch [3501/5000], Loss: 0.0068\n",
      "Epoch [3601/5000], Loss: 0.0072\n",
      "Epoch [3701/5000], Loss: 0.0061\n",
      "Epoch [3801/5000], Loss: 0.0063\n",
      "Epoch [3901/5000], Loss: 0.0067\n",
      "Epoch [4001/5000], Loss: 0.0056\n",
      "Epoch [4101/5000], Loss: 0.0053\n",
      "Epoch [4201/5000], Loss: 0.0070\n",
      "Epoch [4301/5000], Loss: 0.0052\n",
      "Epoch [4401/5000], Loss: 0.0053\n",
      "Epoch [4501/5000], Loss: 0.0053\n",
      "Epoch [4601/5000], Loss: 0.0050\n",
      "Epoch [4701/5000], Loss: 0.0053\n",
      "Epoch [4801/5000], Loss: 0.0057\n",
      "Epoch [4901/5000], Loss: 0.0054\n",
      "Train: 0.005116299 Test: 0.007429013345379337\n",
      "Epoch [1/5000], Loss: 0.9633\n",
      "Epoch [101/5000], Loss: 0.0342\n",
      "Epoch [201/5000], Loss: 0.0211\n",
      "Epoch [301/5000], Loss: 0.0180\n",
      "Epoch [401/5000], Loss: 0.0154\n",
      "Epoch [501/5000], Loss: 0.0137\n",
      "Epoch [601/5000], Loss: 0.0126\n",
      "Epoch [701/5000], Loss: 0.0118\n",
      "Epoch [801/5000], Loss: 0.0108\n",
      "Epoch [901/5000], Loss: 0.0109\n",
      "Epoch [1001/5000], Loss: 0.0094\n",
      "Epoch [1101/5000], Loss: 0.0098\n",
      "Epoch [1201/5000], Loss: 0.0092\n",
      "Epoch [1301/5000], Loss: 0.0093\n",
      "Epoch [1401/5000], Loss: 0.0083\n",
      "Epoch [1501/5000], Loss: 0.0085\n",
      "Epoch [1601/5000], Loss: 0.0084\n",
      "Epoch [1701/5000], Loss: 0.0084\n",
      "Epoch [1801/5000], Loss: 0.0078\n",
      "Epoch [1901/5000], Loss: 0.0080\n",
      "Epoch [2001/5000], Loss: 0.0075\n",
      "Epoch [2101/5000], Loss: 0.0074\n",
      "Epoch [2201/5000], Loss: 0.0076\n",
      "Epoch [2301/5000], Loss: 0.0070\n",
      "Epoch [2401/5000], Loss: 0.0076\n",
      "Epoch [2501/5000], Loss: 0.0078\n",
      "Epoch [2601/5000], Loss: 0.0070\n",
      "Epoch [2701/5000], Loss: 0.0064\n",
      "Epoch [2801/5000], Loss: 0.0064\n",
      "Epoch [2901/5000], Loss: 0.0073\n",
      "Epoch [3001/5000], Loss: 0.0059\n",
      "Epoch [3101/5000], Loss: 0.0063\n",
      "Epoch [3201/5000], Loss: 0.0061\n",
      "Epoch [3301/5000], Loss: 0.0062\n",
      "Epoch [3401/5000], Loss: 0.0056\n",
      "Epoch [3501/5000], Loss: 0.0060\n",
      "Epoch [3601/5000], Loss: 0.0053\n",
      "Epoch [3701/5000], Loss: 0.0054\n",
      "Epoch [3801/5000], Loss: 0.0056\n",
      "Epoch [3901/5000], Loss: 0.0055\n",
      "Epoch [4001/5000], Loss: 0.0054\n",
      "Epoch [4101/5000], Loss: 0.0055\n",
      "Epoch [4201/5000], Loss: 0.0055\n",
      "Epoch [4301/5000], Loss: 0.0050\n",
      "Epoch [4401/5000], Loss: 0.0051\n",
      "Epoch [4501/5000], Loss: 0.0049\n",
      "Epoch [4601/5000], Loss: 0.0051\n",
      "Epoch [4701/5000], Loss: 0.0051\n",
      "Epoch [4801/5000], Loss: 0.0051\n",
      "Epoch [4901/5000], Loss: 0.0057\n",
      "Train: 0.0048932945 Test: 0.0073662613732048896\n",
      "Epoch [1/5000], Loss: 1.2423\n",
      "Epoch [101/5000], Loss: 0.0511\n",
      "Epoch [201/5000], Loss: 0.0260\n",
      "Epoch [301/5000], Loss: 0.0209\n",
      "Epoch [401/5000], Loss: 0.0191\n",
      "Epoch [501/5000], Loss: 0.0185\n",
      "Epoch [601/5000], Loss: 0.0185\n",
      "Epoch [701/5000], Loss: 0.0170\n",
      "Epoch [801/5000], Loss: 0.0152\n",
      "Epoch [901/5000], Loss: 0.0148\n",
      "Epoch [1001/5000], Loss: 0.0146\n",
      "Epoch [1101/5000], Loss: 0.0141\n",
      "Epoch [1201/5000], Loss: 0.0130\n",
      "Epoch [1301/5000], Loss: 0.0127\n",
      "Epoch [1401/5000], Loss: 0.0134\n",
      "Epoch [1501/5000], Loss: 0.0141\n",
      "Epoch [1601/5000], Loss: 0.0128\n",
      "Epoch [1701/5000], Loss: 0.0128\n",
      "Epoch [1801/5000], Loss: 0.0123\n",
      "Epoch [1901/5000], Loss: 0.0117\n",
      "Epoch [2001/5000], Loss: 0.0119\n",
      "Epoch [2101/5000], Loss: 0.0115\n",
      "Epoch [2201/5000], Loss: 0.0112\n",
      "Epoch [2301/5000], Loss: 0.0121\n",
      "Epoch [2401/5000], Loss: 0.0113\n",
      "Epoch [2501/5000], Loss: 0.0111\n",
      "Epoch [2601/5000], Loss: 0.0109\n",
      "Epoch [2701/5000], Loss: 0.0115\n",
      "Epoch [2801/5000], Loss: 0.0110\n",
      "Epoch [2901/5000], Loss: 0.0120\n",
      "Epoch [3001/5000], Loss: 0.0123\n",
      "Epoch [3101/5000], Loss: 0.0125\n",
      "Epoch [3201/5000], Loss: 0.0116\n",
      "Epoch [3301/5000], Loss: 0.0117\n",
      "Epoch [3401/5000], Loss: 0.0129\n",
      "Epoch [3501/5000], Loss: 0.0104\n",
      "Epoch [3601/5000], Loss: 0.0098\n",
      "Epoch [3701/5000], Loss: 0.0102\n",
      "Epoch [3801/5000], Loss: 0.0106\n",
      "Epoch [3901/5000], Loss: 0.0098\n",
      "Epoch [4001/5000], Loss: 0.0100\n",
      "Epoch [4101/5000], Loss: 0.0122\n",
      "Epoch [4201/5000], Loss: 0.0109\n",
      "Epoch [4301/5000], Loss: 0.0096\n",
      "Epoch [4401/5000], Loss: 0.0097\n",
      "Epoch [4501/5000], Loss: 0.0108\n",
      "Epoch [4601/5000], Loss: 0.0107\n",
      "Epoch [4701/5000], Loss: 0.0118\n",
      "Epoch [4801/5000], Loss: 0.0099\n",
      "Epoch [4901/5000], Loss: 0.0100\n",
      "Train: 0.010346674 Test: 0.012395226572406923\n",
      "Epoch [1/5000], Loss: 1.2068\n",
      "Epoch [101/5000], Loss: 0.0381\n",
      "Epoch [201/5000], Loss: 0.0292\n",
      "Epoch [301/5000], Loss: 0.0271\n",
      "Epoch [401/5000], Loss: 0.0261\n",
      "Epoch [501/5000], Loss: 0.0263\n",
      "Epoch [601/5000], Loss: 0.0237\n",
      "Epoch [701/5000], Loss: 0.0228\n",
      "Epoch [801/5000], Loss: 0.0219\n",
      "Epoch [901/5000], Loss: 0.0212\n",
      "Epoch [1001/5000], Loss: 0.0199\n",
      "Epoch [1101/5000], Loss: 0.0173\n",
      "Epoch [1201/5000], Loss: 0.0164\n",
      "Epoch [1301/5000], Loss: 0.0143\n",
      "Epoch [1401/5000], Loss: 0.0130\n",
      "Epoch [1501/5000], Loss: 0.0123\n",
      "Epoch [1601/5000], Loss: 0.0132\n",
      "Epoch [1701/5000], Loss: 0.0135\n",
      "Epoch [1801/5000], Loss: 0.0116\n",
      "Epoch [1901/5000], Loss: 0.0114\n",
      "Epoch [2001/5000], Loss: 0.0118\n",
      "Epoch [2101/5000], Loss: 0.0118\n",
      "Epoch [2201/5000], Loss: 0.0117\n",
      "Epoch [2301/5000], Loss: 0.0111\n",
      "Epoch [2401/5000], Loss: 0.0114\n",
      "Epoch [2501/5000], Loss: 0.0110\n",
      "Epoch [2601/5000], Loss: 0.0106\n",
      "Epoch [2701/5000], Loss: 0.0107\n",
      "Epoch [2801/5000], Loss: 0.0113\n",
      "Epoch [2901/5000], Loss: 0.0103\n",
      "Epoch [3001/5000], Loss: 0.0105\n",
      "Epoch [3101/5000], Loss: 0.0105\n",
      "Epoch [3201/5000], Loss: 0.0101\n",
      "Epoch [3301/5000], Loss: 0.0099\n",
      "Epoch [3401/5000], Loss: 0.0096\n",
      "Epoch [3501/5000], Loss: 0.0094\n",
      "Epoch [3601/5000], Loss: 0.0105\n",
      "Epoch [3701/5000], Loss: 0.0093\n",
      "Epoch [3801/5000], Loss: 0.0094\n",
      "Epoch [3901/5000], Loss: 0.0095\n",
      "Epoch [4001/5000], Loss: 0.0093\n",
      "Epoch [4101/5000], Loss: 0.0095\n",
      "Epoch [4201/5000], Loss: 0.0092\n",
      "Epoch [4301/5000], Loss: 0.0093\n",
      "Epoch [4401/5000], Loss: 0.0090\n",
      "Epoch [4501/5000], Loss: 0.0096\n",
      "Epoch [4601/5000], Loss: 0.0091\n",
      "Epoch [4701/5000], Loss: 0.0089\n",
      "Epoch [4801/5000], Loss: 0.0091\n",
      "Epoch [4901/5000], Loss: 0.0092\n",
      "Train: 0.009428826 Test: 0.011434487567250804\n",
      "Epoch [1/5000], Loss: 1.0273\n",
      "Epoch [101/5000], Loss: 0.0307\n",
      "Epoch [201/5000], Loss: 0.0225\n",
      "Epoch [301/5000], Loss: 0.0196\n",
      "Epoch [401/5000], Loss: 0.0166\n",
      "Epoch [501/5000], Loss: 0.0156\n",
      "Epoch [601/5000], Loss: 0.0143\n",
      "Epoch [701/5000], Loss: 0.0136\n",
      "Epoch [801/5000], Loss: 0.0127\n",
      "Epoch [901/5000], Loss: 0.0131\n",
      "Epoch [1001/5000], Loss: 0.0124\n",
      "Epoch [1101/5000], Loss: 0.0160\n",
      "Epoch [1201/5000], Loss: 0.0143\n",
      "Epoch [1301/5000], Loss: 0.0119\n",
      "Epoch [1401/5000], Loss: 0.0132\n",
      "Epoch [1501/5000], Loss: 0.0105\n",
      "Epoch [1601/5000], Loss: 0.0117\n",
      "Epoch [1701/5000], Loss: 0.0095\n",
      "Epoch [1801/5000], Loss: 0.0089\n",
      "Epoch [1901/5000], Loss: 0.0095\n",
      "Epoch [2001/5000], Loss: 0.0083\n",
      "Epoch [2101/5000], Loss: 0.0103\n",
      "Epoch [2201/5000], Loss: 0.0081\n",
      "Epoch [2301/5000], Loss: 0.0079\n",
      "Epoch [2401/5000], Loss: 0.0078\n",
      "Epoch [2501/5000], Loss: 0.0087\n",
      "Epoch [2601/5000], Loss: 0.0088\n",
      "Epoch [2701/5000], Loss: 0.0073\n",
      "Epoch [2801/5000], Loss: 0.0073\n",
      "Epoch [2901/5000], Loss: 0.0073\n",
      "Epoch [3001/5000], Loss: 0.0068\n",
      "Epoch [3101/5000], Loss: 0.0092\n",
      "Epoch [3201/5000], Loss: 0.0072\n",
      "Epoch [3301/5000], Loss: 0.0066\n",
      "Epoch [3401/5000], Loss: 0.0077\n",
      "Epoch [3501/5000], Loss: 0.0071\n",
      "Epoch [3601/5000], Loss: 0.0064\n",
      "Epoch [3701/5000], Loss: 0.0063\n",
      "Epoch [3801/5000], Loss: 0.0062\n",
      "Epoch [3901/5000], Loss: 0.0063\n",
      "Epoch [4001/5000], Loss: 0.0061\n",
      "Epoch [4101/5000], Loss: 0.0077\n",
      "Epoch [4201/5000], Loss: 0.0056\n",
      "Epoch [4301/5000], Loss: 0.0059\n",
      "Epoch [4401/5000], Loss: 0.0079\n",
      "Epoch [4501/5000], Loss: 0.0058\n",
      "Epoch [4601/5000], Loss: 0.0056\n",
      "Epoch [4701/5000], Loss: 0.0058\n",
      "Epoch [4801/5000], Loss: 0.0053\n",
      "Epoch [4901/5000], Loss: 0.0076\n",
      "Train: 0.0067918045 Test: 0.008750862585752637\n",
      "Epoch [1/5000], Loss: 1.0928\n",
      "Epoch [101/5000], Loss: 0.0373\n",
      "Epoch [201/5000], Loss: 0.0234\n",
      "Epoch [301/5000], Loss: 0.0187\n",
      "Epoch [401/5000], Loss: 0.0169\n",
      "Epoch [501/5000], Loss: 0.0155\n",
      "Epoch [601/5000], Loss: 0.0155\n",
      "Epoch [701/5000], Loss: 0.0147\n",
      "Epoch [801/5000], Loss: 0.0151\n",
      "Epoch [901/5000], Loss: 0.0136\n",
      "Epoch [1001/5000], Loss: 0.0140\n",
      "Epoch [1101/5000], Loss: 0.0129\n",
      "Epoch [1201/5000], Loss: 0.0127\n",
      "Epoch [1301/5000], Loss: 0.0121\n",
      "Epoch [1401/5000], Loss: 0.0119\n",
      "Epoch [1501/5000], Loss: 0.0116\n",
      "Epoch [1601/5000], Loss: 0.0110\n",
      "Epoch [1701/5000], Loss: 0.0118\n",
      "Epoch [1801/5000], Loss: 0.0103\n",
      "Epoch [1901/5000], Loss: 0.0100\n",
      "Epoch [2001/5000], Loss: 0.0115\n",
      "Epoch [2101/5000], Loss: 0.0085\n",
      "Epoch [2201/5000], Loss: 0.0098\n",
      "Epoch [2301/5000], Loss: 0.0085\n",
      "Epoch [2401/5000], Loss: 0.0084\n",
      "Epoch [2501/5000], Loss: 0.0078\n",
      "Epoch [2601/5000], Loss: 0.0089\n",
      "Epoch [2701/5000], Loss: 0.0077\n",
      "Epoch [2801/5000], Loss: 0.0090\n",
      "Epoch [2901/5000], Loss: 0.0073\n",
      "Epoch [3001/5000], Loss: 0.0072\n",
      "Epoch [3101/5000], Loss: 0.0073\n",
      "Epoch [3201/5000], Loss: 0.0077\n",
      "Epoch [3301/5000], Loss: 0.0071\n",
      "Epoch [3401/5000], Loss: 0.0074\n",
      "Epoch [3501/5000], Loss: 0.0067\n",
      "Epoch [3601/5000], Loss: 0.0068\n",
      "Epoch [3701/5000], Loss: 0.0068\n",
      "Epoch [3801/5000], Loss: 0.0074\n",
      "Epoch [3901/5000], Loss: 0.0065\n",
      "Epoch [4001/5000], Loss: 0.0072\n",
      "Epoch [4101/5000], Loss: 0.0064\n",
      "Epoch [4201/5000], Loss: 0.0062\n",
      "Epoch [4301/5000], Loss: 0.0063\n",
      "Epoch [4401/5000], Loss: 0.0056\n",
      "Epoch [4501/5000], Loss: 0.0069\n",
      "Epoch [4601/5000], Loss: 0.0059\n",
      "Epoch [4701/5000], Loss: 0.0055\n",
      "Epoch [4801/5000], Loss: 0.0054\n",
      "Epoch [4901/5000], Loss: 0.0054\n",
      "Train: 0.0052927868 Test: 0.007434779115873538\n",
      "Epoch [1/5000], Loss: 0.9411\n",
      "Epoch [101/5000], Loss: 0.0312\n",
      "Epoch [201/5000], Loss: 0.0225\n",
      "Epoch [301/5000], Loss: 0.0193\n",
      "Epoch [401/5000], Loss: 0.0166\n",
      "Epoch [501/5000], Loss: 0.0162\n",
      "Epoch [601/5000], Loss: 0.0159\n",
      "Epoch [701/5000], Loss: 0.0147\n",
      "Epoch [801/5000], Loss: 0.0150\n",
      "Epoch [901/5000], Loss: 0.0146\n",
      "Epoch [1001/5000], Loss: 0.0138\n",
      "Epoch [1101/5000], Loss: 0.0138\n",
      "Epoch [1201/5000], Loss: 0.0135\n",
      "Epoch [1301/5000], Loss: 0.0137\n",
      "Epoch [1401/5000], Loss: 0.0130\n",
      "Epoch [1501/5000], Loss: 0.0134\n",
      "Epoch [1601/5000], Loss: 0.0135\n",
      "Epoch [1701/5000], Loss: 0.0124\n",
      "Epoch [1801/5000], Loss: 0.0131\n",
      "Epoch [1901/5000], Loss: 0.0121\n",
      "Epoch [2001/5000], Loss: 0.0126\n",
      "Epoch [2101/5000], Loss: 0.0129\n",
      "Epoch [2201/5000], Loss: 0.0128\n",
      "Epoch [2301/5000], Loss: 0.0126\n",
      "Epoch [2401/5000], Loss: 0.0132\n",
      "Epoch [2501/5000], Loss: 0.0114\n",
      "Epoch [2601/5000], Loss: 0.0133\n",
      "Epoch [2701/5000], Loss: 0.0107\n",
      "Epoch [2801/5000], Loss: 0.0102\n",
      "Epoch [2901/5000], Loss: 0.0122\n",
      "Epoch [3001/5000], Loss: 0.0109\n",
      "Epoch [3101/5000], Loss: 0.0098\n",
      "Epoch [3201/5000], Loss: 0.0098\n",
      "Epoch [3301/5000], Loss: 0.0096\n",
      "Epoch [3401/5000], Loss: 0.0112\n",
      "Epoch [3501/5000], Loss: 0.0107\n",
      "Epoch [3601/5000], Loss: 0.0097\n",
      "Epoch [3701/5000], Loss: 0.0112\n",
      "Epoch [3801/5000], Loss: 0.0094\n",
      "Epoch [3901/5000], Loss: 0.0112\n",
      "Epoch [4001/5000], Loss: 0.0096\n",
      "Epoch [4101/5000], Loss: 0.0091\n",
      "Epoch [4201/5000], Loss: 0.0094\n",
      "Epoch [4301/5000], Loss: 0.0099\n",
      "Epoch [4401/5000], Loss: 0.0093\n",
      "Epoch [4501/5000], Loss: 0.0091\n",
      "Epoch [4601/5000], Loss: 0.0091\n",
      "Epoch [4701/5000], Loss: 0.0110\n",
      "Epoch [4801/5000], Loss: 0.0099\n",
      "Epoch [4901/5000], Loss: 0.0088\n",
      "Train: 0.010825788 Test: 0.01295073534285298\n",
      "Epoch [1/5000], Loss: 1.5231\n",
      "Epoch [101/5000], Loss: 0.0477\n",
      "Epoch [201/5000], Loss: 0.0312\n",
      "Epoch [301/5000], Loss: 0.0225\n",
      "Epoch [401/5000], Loss: 0.0188\n",
      "Epoch [501/5000], Loss: 0.0171\n",
      "Epoch [601/5000], Loss: 0.0162\n",
      "Epoch [701/5000], Loss: 0.0153\n",
      "Epoch [801/5000], Loss: 0.0152\n",
      "Epoch [901/5000], Loss: 0.0144\n",
      "Epoch [1001/5000], Loss: 0.0152\n",
      "Epoch [1101/5000], Loss: 0.0148\n",
      "Epoch [1201/5000], Loss: 0.0147\n",
      "Epoch [1301/5000], Loss: 0.0137\n",
      "Epoch [1401/5000], Loss: 0.0141\n",
      "Epoch [1501/5000], Loss: 0.0145\n",
      "Epoch [1601/5000], Loss: 0.0132\n",
      "Epoch [1701/5000], Loss: 0.0138\n",
      "Epoch [1801/5000], Loss: 0.0136\n",
      "Epoch [1901/5000], Loss: 0.0134\n",
      "Epoch [2001/5000], Loss: 0.0130\n",
      "Epoch [2101/5000], Loss: 0.0135\n",
      "Epoch [2201/5000], Loss: 0.0127\n",
      "Epoch [2301/5000], Loss: 0.0126\n",
      "Epoch [2401/5000], Loss: 0.0126\n",
      "Epoch [2501/5000], Loss: 0.0129\n",
      "Epoch [2601/5000], Loss: 0.0126\n",
      "Epoch [2701/5000], Loss: 0.0123\n",
      "Epoch [2801/5000], Loss: 0.0122\n",
      "Epoch [2901/5000], Loss: 0.0123\n",
      "Epoch [3001/5000], Loss: 0.0124\n",
      "Epoch [3101/5000], Loss: 0.0119\n",
      "Epoch [3201/5000], Loss: 0.0124\n",
      "Epoch [3301/5000], Loss: 0.0123\n",
      "Epoch [3401/5000], Loss: 0.0118\n",
      "Epoch [3501/5000], Loss: 0.0111\n",
      "Epoch [3601/5000], Loss: 0.0114\n",
      "Epoch [3701/5000], Loss: 0.0109\n",
      "Epoch [3801/5000], Loss: 0.0121\n",
      "Epoch [3901/5000], Loss: 0.0109\n",
      "Epoch [4001/5000], Loss: 0.0113\n",
      "Epoch [4101/5000], Loss: 0.0107\n",
      "Epoch [4201/5000], Loss: 0.0121\n",
      "Epoch [4301/5000], Loss: 0.0107\n",
      "Epoch [4401/5000], Loss: 0.0105\n",
      "Epoch [4501/5000], Loss: 0.0122\n",
      "Epoch [4601/5000], Loss: 0.0107\n",
      "Epoch [4701/5000], Loss: 0.0117\n",
      "Epoch [4801/5000], Loss: 0.0106\n",
      "Epoch [4901/5000], Loss: 0.0107\n",
      "Train: 0.010677122 Test: 0.011778839156419058\n",
      "Epoch [1/5000], Loss: 0.6339\n",
      "Epoch [101/5000], Loss: 0.0290\n",
      "Epoch [201/5000], Loss: 0.0236\n",
      "Epoch [301/5000], Loss: 0.0160\n",
      "Epoch [401/5000], Loss: 0.0136\n",
      "Epoch [501/5000], Loss: 0.0127\n",
      "Epoch [601/5000], Loss: 0.0118\n",
      "Epoch [701/5000], Loss: 0.0107\n",
      "Epoch [801/5000], Loss: 0.0105\n",
      "Epoch [901/5000], Loss: 0.0112\n",
      "Epoch [1001/5000], Loss: 0.0098\n",
      "Epoch [1101/5000], Loss: 0.0092\n",
      "Epoch [1201/5000], Loss: 0.0092\n",
      "Epoch [1301/5000], Loss: 0.0085\n",
      "Epoch [1401/5000], Loss: 0.0084\n",
      "Epoch [1501/5000], Loss: 0.0082\n",
      "Epoch [1601/5000], Loss: 0.0074\n",
      "Epoch [1701/5000], Loss: 0.0081\n",
      "Epoch [1801/5000], Loss: 0.0071\n",
      "Epoch [1901/5000], Loss: 0.0070\n",
      "Epoch [2001/5000], Loss: 0.0068\n",
      "Epoch [2101/5000], Loss: 0.0066\n",
      "Epoch [2201/5000], Loss: 0.0064\n",
      "Epoch [2301/5000], Loss: 0.0070\n",
      "Epoch [2401/5000], Loss: 0.0063\n",
      "Epoch [2501/5000], Loss: 0.0061\n",
      "Epoch [2601/5000], Loss: 0.0068\n",
      "Epoch [2701/5000], Loss: 0.0059\n",
      "Epoch [2801/5000], Loss: 0.0060\n",
      "Epoch [2901/5000], Loss: 0.0062\n",
      "Epoch [3001/5000], Loss: 0.0058\n",
      "Epoch [3101/5000], Loss: 0.0058\n",
      "Epoch [3201/5000], Loss: 0.0055\n",
      "Epoch [3301/5000], Loss: 0.0056\n",
      "Epoch [3401/5000], Loss: 0.0060\n",
      "Epoch [3501/5000], Loss: 0.0064\n",
      "Epoch [3601/5000], Loss: 0.0052\n",
      "Epoch [3701/5000], Loss: 0.0050\n",
      "Epoch [3801/5000], Loss: 0.0052\n",
      "Epoch [3901/5000], Loss: 0.0051\n",
      "Epoch [4001/5000], Loss: 0.0053\n",
      "Epoch [4101/5000], Loss: 0.0051\n",
      "Epoch [4201/5000], Loss: 0.0053\n",
      "Epoch [4301/5000], Loss: 0.0062\n",
      "Epoch [4401/5000], Loss: 0.0061\n",
      "Epoch [4501/5000], Loss: 0.0058\n",
      "Epoch [4601/5000], Loss: 0.0049\n",
      "Epoch [4701/5000], Loss: 0.0050\n",
      "Epoch [4801/5000], Loss: 0.0046\n",
      "Epoch [4901/5000], Loss: 0.0051\n",
      "Train: 0.0048233583 Test: 0.006498707405913007\n",
      "Epoch [1/5000], Loss: 0.9139\n",
      "Epoch [101/5000], Loss: 0.0299\n",
      "Epoch [201/5000], Loss: 0.0207\n",
      "Epoch [301/5000], Loss: 0.0170\n",
      "Epoch [401/5000], Loss: 0.0137\n",
      "Epoch [501/5000], Loss: 0.0123\n",
      "Epoch [601/5000], Loss: 0.0110\n",
      "Epoch [701/5000], Loss: 0.0110\n",
      "Epoch [801/5000], Loss: 0.0102\n",
      "Epoch [901/5000], Loss: 0.0094\n",
      "Epoch [1001/5000], Loss: 0.0089\n",
      "Epoch [1101/5000], Loss: 0.0105\n",
      "Epoch [1201/5000], Loss: 0.0080\n",
      "Epoch [1301/5000], Loss: 0.0077\n",
      "Epoch [1401/5000], Loss: 0.0079\n",
      "Epoch [1501/5000], Loss: 0.0083\n",
      "Epoch [1601/5000], Loss: 0.0070\n",
      "Epoch [1701/5000], Loss: 0.0071\n",
      "Epoch [1801/5000], Loss: 0.0073\n",
      "Epoch [1901/5000], Loss: 0.0069\n",
      "Epoch [2001/5000], Loss: 0.0069\n",
      "Epoch [2101/5000], Loss: 0.0061\n",
      "Epoch [2201/5000], Loss: 0.0063\n",
      "Epoch [2301/5000], Loss: 0.0061\n",
      "Epoch [2401/5000], Loss: 0.0086\n",
      "Epoch [2501/5000], Loss: 0.0057\n",
      "Epoch [2601/5000], Loss: 0.0062\n",
      "Epoch [2701/5000], Loss: 0.0059\n",
      "Epoch [2801/5000], Loss: 0.0059\n",
      "Epoch [2901/5000], Loss: 0.0064\n",
      "Epoch [3001/5000], Loss: 0.0054\n",
      "Epoch [3101/5000], Loss: 0.0056\n",
      "Epoch [3201/5000], Loss: 0.0059\n",
      "Epoch [3301/5000], Loss: 0.0055\n",
      "Epoch [3401/5000], Loss: 0.0055\n",
      "Epoch [3501/5000], Loss: 0.0052\n",
      "Epoch [3601/5000], Loss: 0.0051\n",
      "Epoch [3701/5000], Loss: 0.0055\n",
      "Epoch [3801/5000], Loss: 0.0052\n",
      "Epoch [3901/5000], Loss: 0.0053\n",
      "Epoch [4001/5000], Loss: 0.0052\n",
      "Epoch [4101/5000], Loss: 0.0057\n",
      "Epoch [4201/5000], Loss: 0.0052\n",
      "Epoch [4301/5000], Loss: 0.0055\n",
      "Epoch [4401/5000], Loss: 0.0049\n",
      "Epoch [4501/5000], Loss: 0.0050\n",
      "Epoch [4601/5000], Loss: 0.0049\n",
      "Epoch [4701/5000], Loss: 0.0047\n",
      "Epoch [4801/5000], Loss: 0.0053\n",
      "Epoch [4901/5000], Loss: 0.0053\n",
      "Train: 0.0056985435 Test: 0.007457206857272237\n",
      "Epoch [1/5000], Loss: 1.0402\n",
      "Epoch [101/5000], Loss: 0.0296\n",
      "Epoch [201/5000], Loss: 0.0208\n",
      "Epoch [301/5000], Loss: 0.0176\n",
      "Epoch [401/5000], Loss: 0.0152\n",
      "Epoch [501/5000], Loss: 0.0128\n",
      "Epoch [601/5000], Loss: 0.0119\n",
      "Epoch [701/5000], Loss: 0.0111\n",
      "Epoch [801/5000], Loss: 0.0100\n",
      "Epoch [901/5000], Loss: 0.0131\n",
      "Epoch [1001/5000], Loss: 0.0096\n",
      "Epoch [1101/5000], Loss: 0.0093\n",
      "Epoch [1201/5000], Loss: 0.0090\n",
      "Epoch [1301/5000], Loss: 0.0089\n",
      "Epoch [1401/5000], Loss: 0.0081\n",
      "Epoch [1501/5000], Loss: 0.0081\n",
      "Epoch [1601/5000], Loss: 0.0082\n",
      "Epoch [1701/5000], Loss: 0.0085\n",
      "Epoch [1801/5000], Loss: 0.0072\n",
      "Epoch [1901/5000], Loss: 0.0071\n",
      "Epoch [2001/5000], Loss: 0.0072\n",
      "Epoch [2101/5000], Loss: 0.0071\n",
      "Epoch [2201/5000], Loss: 0.0068\n",
      "Epoch [2301/5000], Loss: 0.0072\n",
      "Epoch [2401/5000], Loss: 0.0066\n",
      "Epoch [2501/5000], Loss: 0.0066\n",
      "Epoch [2601/5000], Loss: 0.0070\n",
      "Epoch [2701/5000], Loss: 0.0060\n",
      "Epoch [2801/5000], Loss: 0.0062\n",
      "Epoch [2901/5000], Loss: 0.0057\n",
      "Epoch [3001/5000], Loss: 0.0057\n",
      "Epoch [3101/5000], Loss: 0.0055\n",
      "Epoch [3201/5000], Loss: 0.0061\n",
      "Epoch [3301/5000], Loss: 0.0055\n",
      "Epoch [3401/5000], Loss: 0.0053\n",
      "Epoch [3501/5000], Loss: 0.0055\n",
      "Epoch [3601/5000], Loss: 0.0059\n",
      "Epoch [3701/5000], Loss: 0.0056\n",
      "Epoch [3801/5000], Loss: 0.0052\n",
      "Epoch [3901/5000], Loss: 0.0052\n",
      "Epoch [4001/5000], Loss: 0.0056\n",
      "Epoch [4101/5000], Loss: 0.0053\n",
      "Epoch [4201/5000], Loss: 0.0048\n",
      "Epoch [4301/5000], Loss: 0.0049\n",
      "Epoch [4401/5000], Loss: 0.0050\n",
      "Epoch [4501/5000], Loss: 0.0049\n",
      "Epoch [4601/5000], Loss: 0.0054\n",
      "Epoch [4701/5000], Loss: 0.0054\n",
      "Epoch [4801/5000], Loss: 0.0046\n",
      "Epoch [4901/5000], Loss: 0.0047\n",
      "Train: 0.0047080046 Test: 0.006440383718037609\n",
      "Epoch [1/5000], Loss: 0.8582\n",
      "Epoch [101/5000], Loss: 0.0375\n",
      "Epoch [201/5000], Loss: 0.0248\n",
      "Epoch [301/5000], Loss: 0.0202\n",
      "Epoch [401/5000], Loss: 0.0170\n",
      "Epoch [501/5000], Loss: 0.0151\n",
      "Epoch [601/5000], Loss: 0.0131\n",
      "Epoch [701/5000], Loss: 0.0117\n",
      "Epoch [801/5000], Loss: 0.0110\n",
      "Epoch [901/5000], Loss: 0.0104\n",
      "Epoch [1001/5000], Loss: 0.0104\n",
      "Epoch [1101/5000], Loss: 0.0099\n",
      "Epoch [1201/5000], Loss: 0.0093\n",
      "Epoch [1301/5000], Loss: 0.0089\n",
      "Epoch [1401/5000], Loss: 0.0090\n",
      "Epoch [1501/5000], Loss: 0.0086\n",
      "Epoch [1601/5000], Loss: 0.0087\n",
      "Epoch [1701/5000], Loss: 0.0084\n",
      "Epoch [1801/5000], Loss: 0.0081\n",
      "Epoch [1901/5000], Loss: 0.0081\n",
      "Epoch [2001/5000], Loss: 0.0082\n",
      "Epoch [2101/5000], Loss: 0.0075\n",
      "Epoch [2201/5000], Loss: 0.0091\n",
      "Epoch [2301/5000], Loss: 0.0077\n",
      "Epoch [2401/5000], Loss: 0.0072\n",
      "Epoch [2501/5000], Loss: 0.0071\n",
      "Epoch [2601/5000], Loss: 0.0072\n",
      "Epoch [2701/5000], Loss: 0.0081\n",
      "Epoch [2801/5000], Loss: 0.0070\n",
      "Epoch [2901/5000], Loss: 0.0071\n",
      "Epoch [3001/5000], Loss: 0.0079\n",
      "Epoch [3101/5000], Loss: 0.0072\n",
      "Epoch [3201/5000], Loss: 0.0068\n",
      "Epoch [3301/5000], Loss: 0.0065\n",
      "Epoch [3401/5000], Loss: 0.0074\n",
      "Epoch [3501/5000], Loss: 0.0064\n",
      "Epoch [3601/5000], Loss: 0.0066\n",
      "Epoch [3701/5000], Loss: 0.0065\n",
      "Epoch [3801/5000], Loss: 0.0060\n",
      "Epoch [3901/5000], Loss: 0.0063\n",
      "Epoch [4001/5000], Loss: 0.0065\n",
      "Epoch [4101/5000], Loss: 0.0061\n",
      "Epoch [4201/5000], Loss: 0.0062\n",
      "Epoch [4301/5000], Loss: 0.0056\n",
      "Epoch [4401/5000], Loss: 0.0056\n",
      "Epoch [4501/5000], Loss: 0.0062\n",
      "Epoch [4601/5000], Loss: 0.0060\n",
      "Epoch [4701/5000], Loss: 0.0060\n",
      "Epoch [4801/5000], Loss: 0.0058\n",
      "Epoch [4901/5000], Loss: 0.0054\n",
      "Train: 0.0052960436 Test: 0.006745350290413584\n",
      "Epoch [1/5000], Loss: 1.1524\n",
      "Epoch [101/5000], Loss: 0.0472\n",
      "Epoch [201/5000], Loss: 0.0273\n",
      "Epoch [301/5000], Loss: 0.0206\n",
      "Epoch [401/5000], Loss: 0.0185\n",
      "Epoch [501/5000], Loss: 0.0182\n",
      "Epoch [601/5000], Loss: 0.0171\n",
      "Epoch [701/5000], Loss: 0.0173\n",
      "Epoch [801/5000], Loss: 0.0168\n",
      "Epoch [901/5000], Loss: 0.0163\n",
      "Epoch [1001/5000], Loss: 0.0160\n",
      "Epoch [1101/5000], Loss: 0.0158\n",
      "Epoch [1201/5000], Loss: 0.0154\n",
      "Epoch [1301/5000], Loss: 0.0150\n",
      "Epoch [1401/5000], Loss: 0.0150\n",
      "Epoch [1501/5000], Loss: 0.0156\n",
      "Epoch [1601/5000], Loss: 0.0158\n",
      "Epoch [1701/5000], Loss: 0.0133\n",
      "Epoch [1801/5000], Loss: 0.0135\n",
      "Epoch [1901/5000], Loss: 0.0132\n",
      "Epoch [2001/5000], Loss: 0.0138\n",
      "Epoch [2101/5000], Loss: 0.0128\n",
      "Epoch [2201/5000], Loss: 0.0131\n",
      "Epoch [2301/5000], Loss: 0.0125\n",
      "Epoch [2401/5000], Loss: 0.0116\n",
      "Epoch [2501/5000], Loss: 0.0136\n",
      "Epoch [2601/5000], Loss: 0.0116\n",
      "Epoch [2701/5000], Loss: 0.0126\n",
      "Epoch [2801/5000], Loss: 0.0119\n",
      "Epoch [2901/5000], Loss: 0.0129\n",
      "Epoch [3001/5000], Loss: 0.0137\n",
      "Epoch [3101/5000], Loss: 0.0108\n",
      "Epoch [3201/5000], Loss: 0.0112\n",
      "Epoch [3301/5000], Loss: 0.0106\n",
      "Epoch [3401/5000], Loss: 0.0124\n",
      "Epoch [3501/5000], Loss: 0.0107\n",
      "Epoch [3601/5000], Loss: 0.0122\n",
      "Epoch [3701/5000], Loss: 0.0122\n",
      "Epoch [3801/5000], Loss: 0.0110\n",
      "Epoch [3901/5000], Loss: 0.0118\n",
      "Epoch [4001/5000], Loss: 0.0103\n",
      "Epoch [4101/5000], Loss: 0.0104\n",
      "Epoch [4201/5000], Loss: 0.0100\n",
      "Epoch [4301/5000], Loss: 0.0101\n",
      "Epoch [4401/5000], Loss: 0.0106\n",
      "Epoch [4501/5000], Loss: 0.0100\n",
      "Epoch [4601/5000], Loss: 0.0111\n",
      "Epoch [4701/5000], Loss: 0.0098\n",
      "Epoch [4801/5000], Loss: 0.0119\n",
      "Epoch [4901/5000], Loss: 0.0104\n",
      "Train: 0.00978637 Test: 0.012238801774302357\n",
      "Epoch [1/5000], Loss: 0.6645\n",
      "Epoch [101/5000], Loss: 0.0329\n",
      "Epoch [201/5000], Loss: 0.0216\n",
      "Epoch [301/5000], Loss: 0.0175\n",
      "Epoch [401/5000], Loss: 0.0158\n",
      "Epoch [501/5000], Loss: 0.0136\n",
      "Epoch [601/5000], Loss: 0.0128\n",
      "Epoch [701/5000], Loss: 0.0123\n",
      "Epoch [801/5000], Loss: 0.0129\n",
      "Epoch [901/5000], Loss: 0.0128\n",
      "Epoch [1001/5000], Loss: 0.0098\n",
      "Epoch [1101/5000], Loss: 0.0097\n",
      "Epoch [1201/5000], Loss: 0.0090\n",
      "Epoch [1301/5000], Loss: 0.0088\n",
      "Epoch [1401/5000], Loss: 0.0085\n",
      "Epoch [1501/5000], Loss: 0.0081\n",
      "Epoch [1601/5000], Loss: 0.0080\n",
      "Epoch [1701/5000], Loss: 0.0075\n",
      "Epoch [1801/5000], Loss: 0.0079\n",
      "Epoch [1901/5000], Loss: 0.0073\n",
      "Epoch [2001/5000], Loss: 0.0068\n",
      "Epoch [2101/5000], Loss: 0.0069\n",
      "Epoch [2201/5000], Loss: 0.0064\n",
      "Epoch [2301/5000], Loss: 0.0064\n",
      "Epoch [2401/5000], Loss: 0.0067\n",
      "Epoch [2501/5000], Loss: 0.0061\n",
      "Epoch [2601/5000], Loss: 0.0066\n",
      "Epoch [2701/5000], Loss: 0.0060\n",
      "Epoch [2801/5000], Loss: 0.0057\n",
      "Epoch [2901/5000], Loss: 0.0055\n",
      "Epoch [3001/5000], Loss: 0.0054\n",
      "Epoch [3101/5000], Loss: 0.0061\n",
      "Epoch [3201/5000], Loss: 0.0054\n",
      "Epoch [3301/5000], Loss: 0.0054\n",
      "Epoch [3401/5000], Loss: 0.0056\n",
      "Epoch [3501/5000], Loss: 0.0055\n",
      "Epoch [3601/5000], Loss: 0.0050\n",
      "Epoch [3701/5000], Loss: 0.0053\n",
      "Epoch [3801/5000], Loss: 0.0049\n",
      "Epoch [3901/5000], Loss: 0.0049\n",
      "Epoch [4001/5000], Loss: 0.0048\n",
      "Epoch [4101/5000], Loss: 0.0049\n",
      "Epoch [4201/5000], Loss: 0.0049\n",
      "Epoch [4301/5000], Loss: 0.0049\n",
      "Epoch [4401/5000], Loss: 0.0045\n",
      "Epoch [4501/5000], Loss: 0.0044\n",
      "Epoch [4601/5000], Loss: 0.0045\n",
      "Epoch [4701/5000], Loss: 0.0044\n",
      "Epoch [4801/5000], Loss: 0.0043\n",
      "Epoch [4901/5000], Loss: 0.0045\n",
      "Train: 0.004987558 Test: 0.0075384804003051685\n",
      "Epoch [1/5000], Loss: 1.5001\n",
      "Epoch [101/5000], Loss: 0.0739\n",
      "Epoch [201/5000], Loss: 0.0314\n",
      "Epoch [301/5000], Loss: 0.0233\n",
      "Epoch [401/5000], Loss: 0.0205\n",
      "Epoch [501/5000], Loss: 0.0198\n",
      "Epoch [601/5000], Loss: 0.0194\n",
      "Epoch [701/5000], Loss: 0.0176\n",
      "Epoch [801/5000], Loss: 0.0166\n",
      "Epoch [901/5000], Loss: 0.0169\n",
      "Epoch [1001/5000], Loss: 0.0164\n",
      "Epoch [1101/5000], Loss: 0.0160\n",
      "Epoch [1201/5000], Loss: 0.0165\n",
      "Epoch [1301/5000], Loss: 0.0167\n",
      "Epoch [1401/5000], Loss: 0.0159\n",
      "Epoch [1501/5000], Loss: 0.0163\n",
      "Epoch [1601/5000], Loss: 0.0156\n",
      "Epoch [1701/5000], Loss: 0.0153\n",
      "Epoch [1801/5000], Loss: 0.0154\n",
      "Epoch [1901/5000], Loss: 0.0161\n",
      "Epoch [2001/5000], Loss: 0.0161\n",
      "Epoch [2101/5000], Loss: 0.0157\n",
      "Epoch [2201/5000], Loss: 0.0155\n",
      "Epoch [2301/5000], Loss: 0.0159\n",
      "Epoch [2401/5000], Loss: 0.0167\n",
      "Epoch [2501/5000], Loss: 0.0150\n",
      "Epoch [2601/5000], Loss: 0.0150\n",
      "Epoch [2701/5000], Loss: 0.0150\n",
      "Epoch [2801/5000], Loss: 0.0150\n",
      "Epoch [2901/5000], Loss: 0.0152\n",
      "Epoch [3001/5000], Loss: 0.0160\n",
      "Epoch [3101/5000], Loss: 0.0158\n",
      "Epoch [3201/5000], Loss: 0.0148\n",
      "Epoch [3301/5000], Loss: 0.0147\n",
      "Epoch [3401/5000], Loss: 0.0145\n",
      "Epoch [3501/5000], Loss: 0.0161\n",
      "Epoch [3601/5000], Loss: 0.0144\n",
      "Epoch [3701/5000], Loss: 0.0146\n",
      "Epoch [3801/5000], Loss: 0.0152\n",
      "Epoch [3901/5000], Loss: 0.0147\n",
      "Epoch [4001/5000], Loss: 0.0158\n",
      "Epoch [4101/5000], Loss: 0.0148\n",
      "Epoch [4201/5000], Loss: 0.0143\n",
      "Epoch [4301/5000], Loss: 0.0141\n",
      "Epoch [4401/5000], Loss: 0.0156\n",
      "Epoch [4501/5000], Loss: 0.0143\n",
      "Epoch [4601/5000], Loss: 0.0148\n",
      "Epoch [4701/5000], Loss: 0.0153\n",
      "Epoch [4801/5000], Loss: 0.0158\n",
      "Epoch [4901/5000], Loss: 0.0144\n",
      "Train: 0.015540458 Test: 0.017964309172126134\n",
      "Epoch [1/5000], Loss: 1.3826\n",
      "Epoch [101/5000], Loss: 0.0423\n",
      "Epoch [201/5000], Loss: 0.0288\n",
      "Epoch [301/5000], Loss: 0.0230\n",
      "Epoch [401/5000], Loss: 0.0210\n",
      "Epoch [501/5000], Loss: 0.0189\n",
      "Epoch [601/5000], Loss: 0.0173\n",
      "Epoch [701/5000], Loss: 0.0156\n",
      "Epoch [801/5000], Loss: 0.0148\n",
      "Epoch [901/5000], Loss: 0.0143\n",
      "Epoch [1001/5000], Loss: 0.0140\n",
      "Epoch [1101/5000], Loss: 0.0147\n",
      "Epoch [1201/5000], Loss: 0.0144\n",
      "Epoch [1301/5000], Loss: 0.0143\n",
      "Epoch [1401/5000], Loss: 0.0138\n",
      "Epoch [1501/5000], Loss: 0.0130\n",
      "Epoch [1601/5000], Loss: 0.0136\n",
      "Epoch [1701/5000], Loss: 0.0139\n",
      "Epoch [1801/5000], Loss: 0.0137\n",
      "Epoch [1901/5000], Loss: 0.0127\n",
      "Epoch [2001/5000], Loss: 0.0123\n",
      "Epoch [2101/5000], Loss: 0.0130\n",
      "Epoch [2201/5000], Loss: 0.0132\n",
      "Epoch [2301/5000], Loss: 0.0116\n",
      "Epoch [2401/5000], Loss: 0.0116\n",
      "Epoch [2501/5000], Loss: 0.0117\n",
      "Epoch [2601/5000], Loss: 0.0121\n",
      "Epoch [2701/5000], Loss: 0.0113\n",
      "Epoch [2801/5000], Loss: 0.0114\n",
      "Epoch [2901/5000], Loss: 0.0114\n",
      "Epoch [3001/5000], Loss: 0.0106\n",
      "Epoch [3101/5000], Loss: 0.0112\n",
      "Epoch [3201/5000], Loss: 0.0121\n",
      "Epoch [3301/5000], Loss: 0.0108\n",
      "Epoch [3401/5000], Loss: 0.0106\n",
      "Epoch [3501/5000], Loss: 0.0101\n",
      "Epoch [3601/5000], Loss: 0.0124\n",
      "Epoch [3701/5000], Loss: 0.0102\n",
      "Epoch [3801/5000], Loss: 0.0100\n",
      "Epoch [3901/5000], Loss: 0.0114\n",
      "Epoch [4001/5000], Loss: 0.0099\n",
      "Epoch [4101/5000], Loss: 0.0099\n",
      "Epoch [4201/5000], Loss: 0.0101\n",
      "Epoch [4301/5000], Loss: 0.0122\n",
      "Epoch [4401/5000], Loss: 0.0100\n",
      "Epoch [4501/5000], Loss: 0.0098\n",
      "Epoch [4601/5000], Loss: 0.0102\n",
      "Epoch [4701/5000], Loss: 0.0098\n",
      "Epoch [4801/5000], Loss: 0.0100\n",
      "Epoch [4901/5000], Loss: 0.0110\n",
      "Train: 0.010240007 Test: 0.012800401908479414\n",
      "Epoch [1/5000], Loss: 1.6166\n",
      "Epoch [101/5000], Loss: 0.1428\n",
      "Epoch [201/5000], Loss: 0.0514\n",
      "Epoch [301/5000], Loss: 0.0389\n",
      "Epoch [401/5000], Loss: 0.0381\n",
      "Epoch [501/5000], Loss: 0.0309\n",
      "Epoch [601/5000], Loss: 0.0285\n",
      "Epoch [701/5000], Loss: 0.0277\n",
      "Epoch [801/5000], Loss: 0.0282\n",
      "Epoch [901/5000], Loss: 0.0266\n",
      "Epoch [1001/5000], Loss: 0.0264\n",
      "Epoch [1101/5000], Loss: 0.0275\n",
      "Epoch [1201/5000], Loss: 0.0276\n",
      "Epoch [1301/5000], Loss: 0.0259\n",
      "Epoch [1401/5000], Loss: 0.0266\n",
      "Epoch [1501/5000], Loss: 0.0255\n",
      "Epoch [1601/5000], Loss: 0.0259\n",
      "Epoch [1701/5000], Loss: 0.0255\n",
      "Epoch [1801/5000], Loss: 0.0257\n",
      "Epoch [1901/5000], Loss: 0.0262\n",
      "Epoch [2001/5000], Loss: 0.0252\n",
      "Epoch [2101/5000], Loss: 0.0255\n",
      "Epoch [2201/5000], Loss: 0.0263\n",
      "Epoch [2301/5000], Loss: 0.0260\n",
      "Epoch [2401/5000], Loss: 0.0277\n",
      "Epoch [2501/5000], Loss: 0.0259\n",
      "Epoch [2601/5000], Loss: 0.0258\n",
      "Epoch [2701/5000], Loss: 0.0255\n",
      "Epoch [2801/5000], Loss: 0.0251\n",
      "Epoch [2901/5000], Loss: 0.0271\n",
      "Epoch [3001/5000], Loss: 0.0249\n",
      "Epoch [3101/5000], Loss: 0.0250\n",
      "Epoch [3201/5000], Loss: 0.0253\n",
      "Epoch [3301/5000], Loss: 0.0294\n",
      "Epoch [3401/5000], Loss: 0.0247\n",
      "Epoch [3501/5000], Loss: 0.0248\n",
      "Epoch [3601/5000], Loss: 0.0294\n",
      "Epoch [3701/5000], Loss: 0.0247\n",
      "Epoch [3801/5000], Loss: 0.0259\n",
      "Epoch [3901/5000], Loss: 0.0253\n",
      "Epoch [4001/5000], Loss: 0.0251\n",
      "Epoch [4101/5000], Loss: 0.0246\n",
      "Epoch [4201/5000], Loss: 0.0249\n",
      "Epoch [4301/5000], Loss: 0.0248\n",
      "Epoch [4401/5000], Loss: 0.0260\n",
      "Epoch [4501/5000], Loss: 0.0246\n",
      "Epoch [4601/5000], Loss: 0.0245\n",
      "Epoch [4701/5000], Loss: 0.0258\n",
      "Epoch [4801/5000], Loss: 0.0247\n",
      "Epoch [4901/5000], Loss: 0.0245\n",
      "Train: 0.024478832 Test: 0.026647489489355566\n",
      "Epoch [1/5000], Loss: 0.5768\n",
      "Epoch [101/5000], Loss: 0.0283\n",
      "Epoch [201/5000], Loss: 0.0209\n",
      "Epoch [301/5000], Loss: 0.0161\n",
      "Epoch [401/5000], Loss: 0.0131\n",
      "Epoch [501/5000], Loss: 0.0120\n",
      "Epoch [601/5000], Loss: 0.0112\n",
      "Epoch [701/5000], Loss: 0.0110\n",
      "Epoch [801/5000], Loss: 0.0097\n",
      "Epoch [901/5000], Loss: 0.0095\n",
      "Epoch [1001/5000], Loss: 0.0085\n",
      "Epoch [1101/5000], Loss: 0.0088\n",
      "Epoch [1201/5000], Loss: 0.0080\n",
      "Epoch [1301/5000], Loss: 0.0082\n",
      "Epoch [1401/5000], Loss: 0.0079\n",
      "Epoch [1501/5000], Loss: 0.0081\n",
      "Epoch [1601/5000], Loss: 0.0072\n",
      "Epoch [1701/5000], Loss: 0.0070\n",
      "Epoch [1801/5000], Loss: 0.0073\n",
      "Epoch [1901/5000], Loss: 0.0067\n",
      "Epoch [2001/5000], Loss: 0.0069\n",
      "Epoch [2101/5000], Loss: 0.0064\n",
      "Epoch [2201/5000], Loss: 0.0059\n",
      "Epoch [2301/5000], Loss: 0.0060\n",
      "Epoch [2401/5000], Loss: 0.0057\n",
      "Epoch [2501/5000], Loss: 0.0057\n",
      "Epoch [2601/5000], Loss: 0.0058\n",
      "Epoch [2701/5000], Loss: 0.0057\n",
      "Epoch [2801/5000], Loss: 0.0059\n",
      "Epoch [2901/5000], Loss: 0.0059\n",
      "Epoch [3001/5000], Loss: 0.0055\n",
      "Epoch [3101/5000], Loss: 0.0056\n",
      "Epoch [3201/5000], Loss: 0.0053\n",
      "Epoch [3301/5000], Loss: 0.0067\n",
      "Epoch [3401/5000], Loss: 0.0050\n",
      "Epoch [3501/5000], Loss: 0.0050\n",
      "Epoch [3601/5000], Loss: 0.0048\n",
      "Epoch [3701/5000], Loss: 0.0047\n",
      "Epoch [3801/5000], Loss: 0.0050\n",
      "Epoch [3901/5000], Loss: 0.0047\n",
      "Epoch [4001/5000], Loss: 0.0050\n",
      "Epoch [4101/5000], Loss: 0.0048\n",
      "Epoch [4201/5000], Loss: 0.0052\n",
      "Epoch [4301/5000], Loss: 0.0053\n",
      "Epoch [4401/5000], Loss: 0.0046\n",
      "Epoch [4501/5000], Loss: 0.0044\n",
      "Epoch [4601/5000], Loss: 0.0046\n",
      "Epoch [4701/5000], Loss: 0.0048\n",
      "Epoch [4801/5000], Loss: 0.0043\n",
      "Epoch [4901/5000], Loss: 0.0044\n",
      "Train: 0.004919494 Test: 0.006888233133411042\n",
      "Epoch [1/5000], Loss: 1.0912\n",
      "Epoch [101/5000], Loss: 0.0313\n",
      "Epoch [201/5000], Loss: 0.0212\n",
      "Epoch [301/5000], Loss: 0.0176\n",
      "Epoch [401/5000], Loss: 0.0142\n",
      "Epoch [501/5000], Loss: 0.0118\n",
      "Epoch [601/5000], Loss: 0.0109\n",
      "Epoch [701/5000], Loss: 0.0101\n",
      "Epoch [801/5000], Loss: 0.0100\n",
      "Epoch [901/5000], Loss: 0.0097\n",
      "Epoch [1001/5000], Loss: 0.0086\n",
      "Epoch [1101/5000], Loss: 0.0089\n",
      "Epoch [1201/5000], Loss: 0.0081\n",
      "Epoch [1301/5000], Loss: 0.0078\n",
      "Epoch [1401/5000], Loss: 0.0084\n",
      "Epoch [1501/5000], Loss: 0.0075\n",
      "Epoch [1601/5000], Loss: 0.0084\n",
      "Epoch [1701/5000], Loss: 0.0078\n",
      "Epoch [1801/5000], Loss: 0.0074\n",
      "Epoch [1901/5000], Loss: 0.0069\n",
      "Epoch [2001/5000], Loss: 0.0066\n",
      "Epoch [2101/5000], Loss: 0.0066\n",
      "Epoch [2201/5000], Loss: 0.0064\n",
      "Epoch [2301/5000], Loss: 0.0061\n",
      "Epoch [2401/5000], Loss: 0.0060\n",
      "Epoch [2501/5000], Loss: 0.0059\n",
      "Epoch [2601/5000], Loss: 0.0062\n",
      "Epoch [2701/5000], Loss: 0.0061\n",
      "Epoch [2801/5000], Loss: 0.0061\n",
      "Epoch [2901/5000], Loss: 0.0062\n",
      "Epoch [3001/5000], Loss: 0.0055\n",
      "Epoch [3101/5000], Loss: 0.0053\n",
      "Epoch [3201/5000], Loss: 0.0056\n",
      "Epoch [3301/5000], Loss: 0.0053\n",
      "Epoch [3401/5000], Loss: 0.0054\n",
      "Epoch [3501/5000], Loss: 0.0052\n",
      "Epoch [3601/5000], Loss: 0.0049\n",
      "Epoch [3701/5000], Loss: 0.0050\n",
      "Epoch [3801/5000], Loss: 0.0054\n",
      "Epoch [3901/5000], Loss: 0.0051\n",
      "Epoch [4001/5000], Loss: 0.0051\n",
      "Epoch [4101/5000], Loss: 0.0077\n",
      "Epoch [4201/5000], Loss: 0.0046\n",
      "Epoch [4301/5000], Loss: 0.0049\n",
      "Epoch [4401/5000], Loss: 0.0048\n",
      "Epoch [4501/5000], Loss: 0.0061\n",
      "Epoch [4601/5000], Loss: 0.0044\n",
      "Epoch [4701/5000], Loss: 0.0047\n",
      "Epoch [4801/5000], Loss: 0.0047\n",
      "Epoch [4901/5000], Loss: 0.0047\n",
      "Train: 0.0045816754 Test: 0.006395232267477565\n",
      "Epoch [1/5000], Loss: 1.1408\n",
      "Epoch [101/5000], Loss: 0.0575\n",
      "Epoch [201/5000], Loss: 0.0327\n",
      "Epoch [301/5000], Loss: 0.0234\n",
      "Epoch [401/5000], Loss: 0.0195\n",
      "Epoch [501/5000], Loss: 0.0164\n",
      "Epoch [601/5000], Loss: 0.0199\n",
      "Epoch [701/5000], Loss: 0.0154\n",
      "Epoch [801/5000], Loss: 0.0149\n",
      "Epoch [901/5000], Loss: 0.0149\n",
      "Epoch [1001/5000], Loss: 0.0146\n",
      "Epoch [1101/5000], Loss: 0.0134\n",
      "Epoch [1201/5000], Loss: 0.0142\n",
      "Epoch [1301/5000], Loss: 0.0138\n",
      "Epoch [1401/5000], Loss: 0.0184\n",
      "Epoch [1501/5000], Loss: 0.0242\n",
      "Epoch [1601/5000], Loss: 0.0139\n",
      "Epoch [1701/5000], Loss: 0.0149\n",
      "Epoch [1801/5000], Loss: 0.0142\n",
      "Epoch [1901/5000], Loss: 0.0123\n",
      "Epoch [2001/5000], Loss: 0.0124\n",
      "Epoch [2101/5000], Loss: 0.0121\n",
      "Epoch [2201/5000], Loss: 0.0122\n",
      "Epoch [2301/5000], Loss: 0.0146\n",
      "Epoch [2401/5000], Loss: 0.0122\n",
      "Epoch [2501/5000], Loss: 0.0118\n",
      "Epoch [2601/5000], Loss: 0.0139\n",
      "Epoch [2701/5000], Loss: 0.0155\n",
      "Epoch [2801/5000], Loss: 0.0144\n",
      "Epoch [2901/5000], Loss: 0.0137\n",
      "Epoch [3001/5000], Loss: 0.0139\n",
      "Epoch [3101/5000], Loss: 0.0136\n",
      "Epoch [3201/5000], Loss: 0.0127\n",
      "Epoch [3301/5000], Loss: 0.0140\n",
      "Epoch [3401/5000], Loss: 0.0129\n",
      "Epoch [3501/5000], Loss: 0.0127\n",
      "Epoch [3601/5000], Loss: 0.0125\n",
      "Epoch [3701/5000], Loss: 0.0128\n",
      "Epoch [3801/5000], Loss: 0.0125\n",
      "Epoch [3901/5000], Loss: 0.0137\n",
      "Epoch [4001/5000], Loss: 0.0134\n",
      "Epoch [4101/5000], Loss: 0.0158\n",
      "Epoch [4201/5000], Loss: 0.0134\n",
      "Epoch [4301/5000], Loss: 0.0130\n",
      "Epoch [4401/5000], Loss: 0.0121\n",
      "Epoch [4501/5000], Loss: 0.0124\n",
      "Epoch [4601/5000], Loss: 0.0111\n",
      "Epoch [4701/5000], Loss: 0.0109\n",
      "Epoch [4801/5000], Loss: 0.0122\n",
      "Epoch [4901/5000], Loss: 0.0107\n",
      "Train: 0.010988627 Test: 0.012417357255571448\n",
      "Epoch [1/5000], Loss: 1.0525\n",
      "Epoch [101/5000], Loss: 0.0388\n",
      "Epoch [201/5000], Loss: 0.0200\n",
      "Epoch [301/5000], Loss: 0.0157\n",
      "Epoch [401/5000], Loss: 0.0146\n",
      "Epoch [501/5000], Loss: 0.0131\n",
      "Epoch [601/5000], Loss: 0.0143\n",
      "Epoch [701/5000], Loss: 0.0114\n",
      "Epoch [801/5000], Loss: 0.0112\n",
      "Epoch [901/5000], Loss: 0.0104\n",
      "Epoch [1001/5000], Loss: 0.0110\n",
      "Epoch [1101/5000], Loss: 0.0106\n",
      "Epoch [1201/5000], Loss: 0.0099\n",
      "Epoch [1301/5000], Loss: 0.0093\n",
      "Epoch [1401/5000], Loss: 0.0084\n",
      "Epoch [1501/5000], Loss: 0.0091\n",
      "Epoch [1601/5000], Loss: 0.0081\n",
      "Epoch [1701/5000], Loss: 0.0080\n",
      "Epoch [1801/5000], Loss: 0.0091\n",
      "Epoch [1901/5000], Loss: 0.0078\n",
      "Epoch [2001/5000], Loss: 0.0073\n",
      "Epoch [2101/5000], Loss: 0.0077\n",
      "Epoch [2201/5000], Loss: 0.0082\n",
      "Epoch [2301/5000], Loss: 0.0069\n",
      "Epoch [2401/5000], Loss: 0.0074\n",
      "Epoch [2501/5000], Loss: 0.0075\n",
      "Epoch [2601/5000], Loss: 0.0067\n",
      "Epoch [2701/5000], Loss: 0.0064\n",
      "Epoch [2801/5000], Loss: 0.0067\n",
      "Epoch [2901/5000], Loss: 0.0063\n",
      "Epoch [3001/5000], Loss: 0.0062\n",
      "Epoch [3101/5000], Loss: 0.0061\n",
      "Epoch [3201/5000], Loss: 0.0068\n",
      "Epoch [3301/5000], Loss: 0.0057\n",
      "Epoch [3401/5000], Loss: 0.0063\n",
      "Epoch [3501/5000], Loss: 0.0084\n",
      "Epoch [3601/5000], Loss: 0.0055\n",
      "Epoch [3701/5000], Loss: 0.0068\n",
      "Epoch [3801/5000], Loss: 0.0057\n",
      "Epoch [3901/5000], Loss: 0.0053\n",
      "Epoch [4001/5000], Loss: 0.0053\n",
      "Epoch [4101/5000], Loss: 0.0064\n",
      "Epoch [4201/5000], Loss: 0.0071\n",
      "Epoch [4301/5000], Loss: 0.0062\n",
      "Epoch [4401/5000], Loss: 0.0070\n",
      "Epoch [4501/5000], Loss: 0.0059\n",
      "Epoch [4601/5000], Loss: 0.0085\n",
      "Epoch [4701/5000], Loss: 0.0055\n",
      "Epoch [4801/5000], Loss: 0.0054\n",
      "Epoch [4901/5000], Loss: 0.0052\n",
      "Train: 0.004967832 Test: 0.00680921475723146\n",
      "Epoch [1/5000], Loss: 0.8106\n",
      "Epoch [101/5000], Loss: 0.0460\n",
      "Epoch [201/5000], Loss: 0.0294\n",
      "Epoch [301/5000], Loss: 0.0229\n",
      "Epoch [401/5000], Loss: 0.0199\n",
      "Epoch [501/5000], Loss: 0.0182\n",
      "Epoch [601/5000], Loss: 0.0162\n",
      "Epoch [701/5000], Loss: 0.0158\n",
      "Epoch [801/5000], Loss: 0.0138\n",
      "Epoch [901/5000], Loss: 0.0142\n",
      "Epoch [1001/5000], Loss: 0.0131\n",
      "Epoch [1101/5000], Loss: 0.0126\n",
      "Epoch [1201/5000], Loss: 0.0126\n",
      "Epoch [1301/5000], Loss: 0.0120\n",
      "Epoch [1401/5000], Loss: 0.0114\n",
      "Epoch [1501/5000], Loss: 0.0116\n",
      "Epoch [1601/5000], Loss: 0.0114\n",
      "Epoch [1701/5000], Loss: 0.0108\n",
      "Epoch [1801/5000], Loss: 0.0101\n",
      "Epoch [1901/5000], Loss: 0.0100\n",
      "Epoch [2001/5000], Loss: 0.0094\n",
      "Epoch [2101/5000], Loss: 0.0196\n",
      "Epoch [2201/5000], Loss: 0.0095\n",
      "Epoch [2301/5000], Loss: 0.0083\n",
      "Epoch [2401/5000], Loss: 0.0083\n",
      "Epoch [2501/5000], Loss: 0.0088\n",
      "Epoch [2601/5000], Loss: 0.0084\n",
      "Epoch [2701/5000], Loss: 0.0078\n",
      "Epoch [2801/5000], Loss: 0.0091\n",
      "Epoch [2901/5000], Loss: 0.0075\n",
      "Epoch [3001/5000], Loss: 0.0075\n",
      "Epoch [3101/5000], Loss: 0.0072\n",
      "Epoch [3201/5000], Loss: 0.0075\n",
      "Epoch [3301/5000], Loss: 0.0071\n",
      "Epoch [3401/5000], Loss: 0.0073\n",
      "Epoch [3501/5000], Loss: 0.0071\n",
      "Epoch [3601/5000], Loss: 0.0067\n",
      "Epoch [3701/5000], Loss: 0.0080\n",
      "Epoch [3801/5000], Loss: 0.0066\n",
      "Epoch [3901/5000], Loss: 0.0064\n",
      "Epoch [4001/5000], Loss: 0.0069\n",
      "Epoch [4101/5000], Loss: 0.0073\n",
      "Epoch [4201/5000], Loss: 0.0065\n",
      "Epoch [4301/5000], Loss: 0.0071\n",
      "Epoch [4401/5000], Loss: 0.0062\n",
      "Epoch [4501/5000], Loss: 0.0061\n",
      "Epoch [4601/5000], Loss: 0.0063\n",
      "Epoch [4701/5000], Loss: 0.0067\n",
      "Epoch [4801/5000], Loss: 0.0063\n",
      "Epoch [4901/5000], Loss: 0.0061\n",
      "Train: 0.005725187 Test: 0.007494720318521185\n",
      "Epoch [1/5000], Loss: 1.2501\n",
      "Epoch [101/5000], Loss: 0.0419\n",
      "Epoch [201/5000], Loss: 0.0293\n",
      "Epoch [301/5000], Loss: 0.0267\n",
      "Epoch [401/5000], Loss: 0.0257\n",
      "Epoch [501/5000], Loss: 0.0250\n",
      "Epoch [601/5000], Loss: 0.0198\n",
      "Epoch [701/5000], Loss: 0.0163\n",
      "Epoch [801/5000], Loss: 0.0153\n",
      "Epoch [901/5000], Loss: 0.0149\n",
      "Epoch [1001/5000], Loss: 0.0142\n",
      "Epoch [1101/5000], Loss: 0.0136\n",
      "Epoch [1201/5000], Loss: 0.0141\n",
      "Epoch [1301/5000], Loss: 0.0124\n",
      "Epoch [1401/5000], Loss: 0.0129\n",
      "Epoch [1501/5000], Loss: 0.0125\n",
      "Epoch [1601/5000], Loss: 0.0117\n",
      "Epoch [1701/5000], Loss: 0.0117\n",
      "Epoch [1801/5000], Loss: 0.0114\n",
      "Epoch [1901/5000], Loss: 0.0113\n",
      "Epoch [2001/5000], Loss: 0.0114\n",
      "Epoch [2101/5000], Loss: 0.0121\n",
      "Epoch [2201/5000], Loss: 0.0116\n",
      "Epoch [2301/5000], Loss: 0.0120\n",
      "Epoch [2401/5000], Loss: 0.0107\n",
      "Epoch [2501/5000], Loss: 0.0108\n",
      "Epoch [2601/5000], Loss: 0.0119\n",
      "Epoch [2701/5000], Loss: 0.0106\n",
      "Epoch [2801/5000], Loss: 0.0102\n",
      "Epoch [2901/5000], Loss: 0.0105\n",
      "Epoch [3001/5000], Loss: 0.0111\n",
      "Epoch [3101/5000], Loss: 0.0108\n",
      "Epoch [3201/5000], Loss: 0.0109\n",
      "Epoch [3301/5000], Loss: 0.0099\n",
      "Epoch [3401/5000], Loss: 0.0103\n",
      "Epoch [3501/5000], Loss: 0.0094\n",
      "Epoch [3601/5000], Loss: 0.0093\n",
      "Epoch [3701/5000], Loss: 0.0098\n",
      "Epoch [3801/5000], Loss: 0.0097\n",
      "Epoch [3901/5000], Loss: 0.0094\n",
      "Epoch [4001/5000], Loss: 0.0097\n",
      "Epoch [4101/5000], Loss: 0.0095\n",
      "Epoch [4201/5000], Loss: 0.0094\n",
      "Epoch [4301/5000], Loss: 0.0103\n",
      "Epoch [4401/5000], Loss: 0.0092\n",
      "Epoch [4501/5000], Loss: 0.0090\n",
      "Epoch [4601/5000], Loss: 0.0118\n",
      "Epoch [4701/5000], Loss: 0.0087\n",
      "Epoch [4801/5000], Loss: 0.0099\n",
      "Epoch [4901/5000], Loss: 0.0090\n",
      "Train: 0.009229705 Test: 0.011350986018441383\n",
      "Epoch [1/5000], Loss: 0.8467\n",
      "Epoch [101/5000], Loss: 0.0434\n",
      "Epoch [201/5000], Loss: 0.0270\n",
      "Epoch [301/5000], Loss: 0.0220\n",
      "Epoch [401/5000], Loss: 0.0192\n",
      "Epoch [501/5000], Loss: 0.0164\n",
      "Epoch [601/5000], Loss: 0.0145\n",
      "Epoch [701/5000], Loss: 0.0124\n",
      "Epoch [801/5000], Loss: 0.0109\n",
      "Epoch [901/5000], Loss: 0.0103\n",
      "Epoch [1001/5000], Loss: 0.0105\n",
      "Epoch [1101/5000], Loss: 0.0098\n",
      "Epoch [1201/5000], Loss: 0.0100\n",
      "Epoch [1301/5000], Loss: 0.0090\n",
      "Epoch [1401/5000], Loss: 0.0101\n",
      "Epoch [1501/5000], Loss: 0.0092\n",
      "Epoch [1601/5000], Loss: 0.0088\n",
      "Epoch [1701/5000], Loss: 0.0084\n",
      "Epoch [1801/5000], Loss: 0.0082\n",
      "Epoch [1901/5000], Loss: 0.0085\n",
      "Epoch [2001/5000], Loss: 0.0081\n",
      "Epoch [2101/5000], Loss: 0.0081\n",
      "Epoch [2201/5000], Loss: 0.0085\n",
      "Epoch [2301/5000], Loss: 0.0076\n",
      "Epoch [2401/5000], Loss: 0.0074\n",
      "Epoch [2501/5000], Loss: 0.0075\n",
      "Epoch [2601/5000], Loss: 0.0073\n",
      "Epoch [2701/5000], Loss: 0.0073\n",
      "Epoch [2801/5000], Loss: 0.0071\n",
      "Epoch [2901/5000], Loss: 0.0069\n",
      "Epoch [3001/5000], Loss: 0.0080\n",
      "Epoch [3101/5000], Loss: 0.0073\n",
      "Epoch [3201/5000], Loss: 0.0067\n",
      "Epoch [3301/5000], Loss: 0.0062\n",
      "Epoch [3401/5000], Loss: 0.0067\n",
      "Epoch [3501/5000], Loss: 0.0064\n",
      "Epoch [3601/5000], Loss: 0.0060\n",
      "Epoch [3701/5000], Loss: 0.0062\n",
      "Epoch [3801/5000], Loss: 0.0057\n",
      "Epoch [3901/5000], Loss: 0.0059\n",
      "Epoch [4001/5000], Loss: 0.0058\n",
      "Epoch [4101/5000], Loss: 0.0056\n",
      "Epoch [4201/5000], Loss: 0.0054\n",
      "Epoch [4301/5000], Loss: 0.0065\n",
      "Epoch [4401/5000], Loss: 0.0054\n",
      "Epoch [4501/5000], Loss: 0.0063\n",
      "Epoch [4601/5000], Loss: 0.0053\n",
      "Epoch [4701/5000], Loss: 0.0061\n",
      "Epoch [4801/5000], Loss: 0.0057\n",
      "Epoch [4901/5000], Loss: 0.0053\n",
      "Train: 0.0058993236 Test: 0.007611385257023667\n",
      "Epoch [1/5000], Loss: 1.1862\n",
      "Epoch [101/5000], Loss: 0.0468\n",
      "Epoch [201/5000], Loss: 0.0324\n",
      "Epoch [301/5000], Loss: 0.0278\n",
      "Epoch [401/5000], Loss: 0.0272\n",
      "Epoch [501/5000], Loss: 0.0261\n",
      "Epoch [601/5000], Loss: 0.0255\n",
      "Epoch [701/5000], Loss: 0.0254\n",
      "Epoch [801/5000], Loss: 0.0260\n",
      "Epoch [901/5000], Loss: 0.0253\n",
      "Epoch [1001/5000], Loss: 0.0249\n",
      "Epoch [1101/5000], Loss: 0.0246\n",
      "Epoch [1201/5000], Loss: 0.0250\n",
      "Epoch [1301/5000], Loss: 0.0246\n",
      "Epoch [1401/5000], Loss: 0.0246\n",
      "Epoch [1501/5000], Loss: 0.0243\n",
      "Epoch [1601/5000], Loss: 0.0246\n",
      "Epoch [1701/5000], Loss: 0.0247\n",
      "Epoch [1801/5000], Loss: 0.0241\n",
      "Epoch [1901/5000], Loss: 0.0244\n",
      "Epoch [2001/5000], Loss: 0.0244\n",
      "Epoch [2101/5000], Loss: 0.0239\n",
      "Epoch [2201/5000], Loss: 0.0234\n",
      "Epoch [2301/5000], Loss: 0.0240\n",
      "Epoch [2401/5000], Loss: 0.0238\n",
      "Epoch [2501/5000], Loss: 0.0248\n",
      "Epoch [2601/5000], Loss: 0.0239\n",
      "Epoch [2701/5000], Loss: 0.0226\n",
      "Epoch [2801/5000], Loss: 0.0224\n",
      "Epoch [2901/5000], Loss: 0.0225\n",
      "Epoch [3001/5000], Loss: 0.0233\n",
      "Epoch [3101/5000], Loss: 0.0245\n",
      "Epoch [3201/5000], Loss: 0.0217\n",
      "Epoch [3301/5000], Loss: 0.0224\n",
      "Epoch [3401/5000], Loss: 0.0219\n",
      "Epoch [3501/5000], Loss: 0.0214\n",
      "Epoch [3601/5000], Loss: 0.0222\n",
      "Epoch [3701/5000], Loss: 0.0214\n",
      "Epoch [3801/5000], Loss: 0.0214\n",
      "Epoch [3901/5000], Loss: 0.0217\n",
      "Epoch [4001/5000], Loss: 0.0211\n",
      "Epoch [4101/5000], Loss: 0.0209\n",
      "Epoch [4201/5000], Loss: 0.0209\n",
      "Epoch [4301/5000], Loss: 0.0207\n",
      "Epoch [4401/5000], Loss: 0.0208\n",
      "Epoch [4501/5000], Loss: 0.0207\n",
      "Epoch [4601/5000], Loss: 0.0210\n",
      "Epoch [4701/5000], Loss: 0.0209\n",
      "Epoch [4801/5000], Loss: 0.0203\n",
      "Epoch [4901/5000], Loss: 0.0202\n",
      "Train: 0.020829963 Test: 0.022096484733367273\n",
      "Epoch [1/5000], Loss: 0.7841\n",
      "Epoch [101/5000], Loss: 0.0303\n",
      "Epoch [201/5000], Loss: 0.0199\n",
      "Epoch [301/5000], Loss: 0.0166\n",
      "Epoch [401/5000], Loss: 0.0142\n",
      "Epoch [501/5000], Loss: 0.0132\n",
      "Epoch [601/5000], Loss: 0.0121\n",
      "Epoch [701/5000], Loss: 0.0107\n",
      "Epoch [801/5000], Loss: 0.0105\n",
      "Epoch [901/5000], Loss: 0.0109\n",
      "Epoch [1001/5000], Loss: 0.0099\n",
      "Epoch [1101/5000], Loss: 0.0094\n",
      "Epoch [1201/5000], Loss: 0.0087\n",
      "Epoch [1301/5000], Loss: 0.0081\n",
      "Epoch [1401/5000], Loss: 0.0074\n",
      "Epoch [1501/5000], Loss: 0.0077\n",
      "Epoch [1601/5000], Loss: 0.0070\n",
      "Epoch [1701/5000], Loss: 0.0075\n",
      "Epoch [1801/5000], Loss: 0.0066\n",
      "Epoch [1901/5000], Loss: 0.0064\n",
      "Epoch [2001/5000], Loss: 0.0070\n",
      "Epoch [2101/5000], Loss: 0.0065\n",
      "Epoch [2201/5000], Loss: 0.0063\n",
      "Epoch [2301/5000], Loss: 0.0062\n",
      "Epoch [2401/5000], Loss: 0.0060\n",
      "Epoch [2501/5000], Loss: 0.0058\n",
      "Epoch [2601/5000], Loss: 0.0063\n",
      "Epoch [2701/5000], Loss: 0.0066\n",
      "Epoch [2801/5000], Loss: 0.0059\n",
      "Epoch [2901/5000], Loss: 0.0060\n",
      "Epoch [3001/5000], Loss: 0.0081\n",
      "Epoch [3101/5000], Loss: 0.0060\n",
      "Epoch [3201/5000], Loss: 0.0058\n",
      "Epoch [3301/5000], Loss: 0.0059\n",
      "Epoch [3401/5000], Loss: 0.0057\n",
      "Epoch [3501/5000], Loss: 0.0054\n",
      "Epoch [3601/5000], Loss: 0.0064\n",
      "Epoch [3701/5000], Loss: 0.0051\n",
      "Epoch [3801/5000], Loss: 0.0056\n",
      "Epoch [3901/5000], Loss: 0.0053\n",
      "Epoch [4001/5000], Loss: 0.0054\n",
      "Epoch [4101/5000], Loss: 0.0055\n",
      "Epoch [4201/5000], Loss: 0.0056\n",
      "Epoch [4301/5000], Loss: 0.0049\n",
      "Epoch [4401/5000], Loss: 0.0051\n",
      "Epoch [4501/5000], Loss: 0.0048\n",
      "Epoch [4601/5000], Loss: 0.0047\n",
      "Epoch [4701/5000], Loss: 0.0049\n",
      "Epoch [4801/5000], Loss: 0.0046\n",
      "Epoch [4901/5000], Loss: 0.0047\n",
      "Train: 0.005031425 Test: 0.006950634830834863\n",
      "Epoch [1/5000], Loss: 1.2100\n",
      "Epoch [101/5000], Loss: 0.0458\n",
      "Epoch [201/5000], Loss: 0.0321\n",
      "Epoch [301/5000], Loss: 0.0274\n",
      "Epoch [401/5000], Loss: 0.0261\n",
      "Epoch [501/5000], Loss: 0.0254\n",
      "Epoch [601/5000], Loss: 0.0254\n",
      "Epoch [701/5000], Loss: 0.0244\n",
      "Epoch [801/5000], Loss: 0.0241\n",
      "Epoch [901/5000], Loss: 0.0241\n",
      "Epoch [1001/5000], Loss: 0.0233\n",
      "Epoch [1101/5000], Loss: 0.0159\n",
      "Epoch [1201/5000], Loss: 0.0147\n",
      "Epoch [1301/5000], Loss: 0.0140\n",
      "Epoch [1401/5000], Loss: 0.0137\n",
      "Epoch [1501/5000], Loss: 0.0136\n",
      "Epoch [1601/5000], Loss: 0.0130\n",
      "Epoch [1701/5000], Loss: 0.0135\n",
      "Epoch [1801/5000], Loss: 0.0128\n",
      "Epoch [1901/5000], Loss: 0.0128\n",
      "Epoch [2001/5000], Loss: 0.0122\n",
      "Epoch [2101/5000], Loss: 0.0121\n",
      "Epoch [2201/5000], Loss: 0.0128\n",
      "Epoch [2301/5000], Loss: 0.0119\n",
      "Epoch [2401/5000], Loss: 0.0122\n",
      "Epoch [2501/5000], Loss: 0.0123\n",
      "Epoch [2601/5000], Loss: 0.0128\n",
      "Epoch [2701/5000], Loss: 0.0122\n",
      "Epoch [2801/5000], Loss: 0.0114\n",
      "Epoch [2901/5000], Loss: 0.0111\n",
      "Epoch [3001/5000], Loss: 0.0117\n",
      "Epoch [3101/5000], Loss: 0.0120\n",
      "Epoch [3201/5000], Loss: 0.0106\n",
      "Epoch [3301/5000], Loss: 0.0105\n",
      "Epoch [3401/5000], Loss: 0.0111\n",
      "Epoch [3501/5000], Loss: 0.0106\n",
      "Epoch [3601/5000], Loss: 0.0103\n",
      "Epoch [3701/5000], Loss: 0.0106\n",
      "Epoch [3801/5000], Loss: 0.0113\n",
      "Epoch [3901/5000], Loss: 0.0105\n",
      "Epoch [4001/5000], Loss: 0.0111\n",
      "Epoch [4101/5000], Loss: 0.0106\n",
      "Epoch [4201/5000], Loss: 0.0109\n",
      "Epoch [4301/5000], Loss: 0.0099\n",
      "Epoch [4401/5000], Loss: 0.0105\n",
      "Epoch [4501/5000], Loss: 0.0102\n",
      "Epoch [4601/5000], Loss: 0.0102\n",
      "Epoch [4701/5000], Loss: 0.0103\n",
      "Epoch [4801/5000], Loss: 0.0103\n",
      "Epoch [4901/5000], Loss: 0.0105\n",
      "Train: 0.0097223325 Test: 0.011419911777368964\n",
      "Epoch [1/5000], Loss: 0.9970\n",
      "Epoch [101/5000], Loss: 0.0467\n",
      "Epoch [201/5000], Loss: 0.0252\n",
      "Epoch [301/5000], Loss: 0.0179\n",
      "Epoch [401/5000], Loss: 0.0160\n",
      "Epoch [501/5000], Loss: 0.0134\n",
      "Epoch [601/5000], Loss: 0.0118\n",
      "Epoch [701/5000], Loss: 0.0113\n",
      "Epoch [801/5000], Loss: 0.0116\n",
      "Epoch [901/5000], Loss: 0.0104\n",
      "Epoch [1001/5000], Loss: 0.0099\n",
      "Epoch [1101/5000], Loss: 0.0092\n",
      "Epoch [1201/5000], Loss: 0.0086\n",
      "Epoch [1301/5000], Loss: 0.0085\n",
      "Epoch [1401/5000], Loss: 0.0103\n",
      "Epoch [1501/5000], Loss: 0.0082\n",
      "Epoch [1601/5000], Loss: 0.0086\n",
      "Epoch [1701/5000], Loss: 0.0078\n",
      "Epoch [1801/5000], Loss: 0.0089\n",
      "Epoch [1901/5000], Loss: 0.0089\n",
      "Epoch [2001/5000], Loss: 0.0074\n",
      "Epoch [2101/5000], Loss: 0.0073\n",
      "Epoch [2201/5000], Loss: 0.0081\n",
      "Epoch [2301/5000], Loss: 0.0074\n",
      "Epoch [2401/5000], Loss: 0.0072\n",
      "Epoch [2501/5000], Loss: 0.0071\n",
      "Epoch [2601/5000], Loss: 0.0085\n",
      "Epoch [2701/5000], Loss: 0.0074\n",
      "Epoch [2801/5000], Loss: 0.0070\n",
      "Epoch [2901/5000], Loss: 0.0071\n",
      "Epoch [3001/5000], Loss: 0.0072\n",
      "Epoch [3101/5000], Loss: 0.0090\n",
      "Epoch [3201/5000], Loss: 0.0069\n",
      "Epoch [3301/5000], Loss: 0.0066\n",
      "Epoch [3401/5000], Loss: 0.0066\n",
      "Epoch [3501/5000], Loss: 0.0067\n",
      "Epoch [3601/5000], Loss: 0.0064\n",
      "Epoch [3701/5000], Loss: 0.0066\n",
      "Epoch [3801/5000], Loss: 0.0066\n",
      "Epoch [3901/5000], Loss: 0.0069\n",
      "Epoch [4001/5000], Loss: 0.0062\n",
      "Epoch [4101/5000], Loss: 0.0077\n",
      "Epoch [4201/5000], Loss: 0.0084\n",
      "Epoch [4301/5000], Loss: 0.0060\n",
      "Epoch [4401/5000], Loss: 0.0068\n",
      "Epoch [4501/5000], Loss: 0.0067\n",
      "Epoch [4601/5000], Loss: 0.0058\n",
      "Epoch [4701/5000], Loss: 0.0061\n",
      "Epoch [4801/5000], Loss: 0.0057\n",
      "Epoch [4901/5000], Loss: 0.0058\n",
      "Train: 0.0055828714 Test: 0.007675621650938261\n",
      "Epoch [1/5000], Loss: 0.8846\n",
      "Epoch [101/5000], Loss: 0.0352\n",
      "Epoch [201/5000], Loss: 0.0214\n",
      "Epoch [301/5000], Loss: 0.0187\n",
      "Epoch [401/5000], Loss: 0.0159\n",
      "Epoch [501/5000], Loss: 0.0151\n",
      "Epoch [601/5000], Loss: 0.0122\n",
      "Epoch [701/5000], Loss: 0.0128\n",
      "Epoch [801/5000], Loss: 0.0101\n",
      "Epoch [901/5000], Loss: 0.0096\n",
      "Epoch [1001/5000], Loss: 0.0096\n",
      "Epoch [1101/5000], Loss: 0.0091\n",
      "Epoch [1201/5000], Loss: 0.0084\n",
      "Epoch [1301/5000], Loss: 0.0078\n",
      "Epoch [1401/5000], Loss: 0.0074\n",
      "Epoch [1501/5000], Loss: 0.0075\n",
      "Epoch [1601/5000], Loss: 0.0083\n",
      "Epoch [1701/5000], Loss: 0.0073\n",
      "Epoch [1801/5000], Loss: 0.0067\n",
      "Epoch [1901/5000], Loss: 0.0065\n",
      "Epoch [2001/5000], Loss: 0.0064\n",
      "Epoch [2101/5000], Loss: 0.0062\n",
      "Epoch [2201/5000], Loss: 0.0066\n",
      "Epoch [2301/5000], Loss: 0.0062\n",
      "Epoch [2401/5000], Loss: 0.0062\n",
      "Epoch [2501/5000], Loss: 0.0071\n",
      "Epoch [2601/5000], Loss: 0.0056\n",
      "Epoch [2701/5000], Loss: 0.0062\n",
      "Epoch [2801/5000], Loss: 0.0069\n",
      "Epoch [2901/5000], Loss: 0.0053\n",
      "Epoch [3001/5000], Loss: 0.0056\n",
      "Epoch [3101/5000], Loss: 0.0053\n",
      "Epoch [3201/5000], Loss: 0.0056\n",
      "Epoch [3301/5000], Loss: 0.0053\n",
      "Epoch [3401/5000], Loss: 0.0050\n",
      "Epoch [3501/5000], Loss: 0.0049\n",
      "Epoch [3601/5000], Loss: 0.0054\n",
      "Epoch [3701/5000], Loss: 0.0052\n",
      "Epoch [3801/5000], Loss: 0.0051\n",
      "Epoch [3901/5000], Loss: 0.0050\n",
      "Epoch [4001/5000], Loss: 0.0050\n",
      "Epoch [4101/5000], Loss: 0.0049\n",
      "Epoch [4201/5000], Loss: 0.0047\n",
      "Epoch [4301/5000], Loss: 0.0048\n",
      "Epoch [4401/5000], Loss: 0.0058\n",
      "Epoch [4501/5000], Loss: 0.0046\n",
      "Epoch [4601/5000], Loss: 0.0045\n",
      "Epoch [4701/5000], Loss: 0.0062\n",
      "Epoch [4801/5000], Loss: 0.0055\n",
      "Epoch [4901/5000], Loss: 0.0050\n",
      "Train: 0.004802279 Test: 0.007070412233064847\n",
      "Epoch [1/5000], Loss: 0.7782\n",
      "Epoch [101/5000], Loss: 0.0319\n",
      "Epoch [201/5000], Loss: 0.0209\n",
      "Epoch [301/5000], Loss: 0.0171\n",
      "Epoch [401/5000], Loss: 0.0150\n",
      "Epoch [501/5000], Loss: 0.0146\n",
      "Epoch [601/5000], Loss: 0.0126\n",
      "Epoch [701/5000], Loss: 0.0112\n",
      "Epoch [801/5000], Loss: 0.0107\n",
      "Epoch [901/5000], Loss: 0.0100\n",
      "Epoch [1001/5000], Loss: 0.0097\n",
      "Epoch [1101/5000], Loss: 0.0088\n",
      "Epoch [1201/5000], Loss: 0.0088\n",
      "Epoch [1301/5000], Loss: 0.0087\n",
      "Epoch [1401/5000], Loss: 0.0079\n",
      "Epoch [1501/5000], Loss: 0.0079\n",
      "Epoch [1601/5000], Loss: 0.0078\n",
      "Epoch [1701/5000], Loss: 0.0075\n",
      "Epoch [1801/5000], Loss: 0.0076\n",
      "Epoch [1901/5000], Loss: 0.0076\n",
      "Epoch [2001/5000], Loss: 0.0070\n",
      "Epoch [2101/5000], Loss: 0.0077\n",
      "Epoch [2201/5000], Loss: 0.0067\n",
      "Epoch [2301/5000], Loss: 0.0073\n",
      "Epoch [2401/5000], Loss: 0.0064\n",
      "Epoch [2501/5000], Loss: 0.0064\n",
      "Epoch [2601/5000], Loss: 0.0065\n",
      "Epoch [2701/5000], Loss: 0.0084\n",
      "Epoch [2801/5000], Loss: 0.0065\n",
      "Epoch [2901/5000], Loss: 0.0061\n",
      "Epoch [3001/5000], Loss: 0.0065\n",
      "Epoch [3101/5000], Loss: 0.0059\n",
      "Epoch [3201/5000], Loss: 0.0060\n",
      "Epoch [3301/5000], Loss: 0.0059\n",
      "Epoch [3401/5000], Loss: 0.0057\n",
      "Epoch [3501/5000], Loss: 0.0055\n",
      "Epoch [3601/5000], Loss: 0.0064\n",
      "Epoch [3701/5000], Loss: 0.0053\n",
      "Epoch [3801/5000], Loss: 0.0056\n",
      "Epoch [3901/5000], Loss: 0.0052\n",
      "Epoch [4001/5000], Loss: 0.0051\n",
      "Epoch [4101/5000], Loss: 0.0061\n",
      "Epoch [4201/5000], Loss: 0.0053\n",
      "Epoch [4301/5000], Loss: 0.0052\n",
      "Epoch [4401/5000], Loss: 0.0049\n",
      "Epoch [4501/5000], Loss: 0.0067\n",
      "Epoch [4601/5000], Loss: 0.0050\n",
      "Epoch [4701/5000], Loss: 0.0053\n",
      "Epoch [4801/5000], Loss: 0.0045\n",
      "Epoch [4901/5000], Loss: 0.0045\n",
      "Train: 0.0055520125 Test: 0.007898085774016957\n",
      "Epoch [1/5000], Loss: 1.1391\n",
      "Epoch [101/5000], Loss: 0.0370\n",
      "Epoch [201/5000], Loss: 0.0226\n",
      "Epoch [301/5000], Loss: 0.0189\n",
      "Epoch [401/5000], Loss: 0.0176\n",
      "Epoch [501/5000], Loss: 0.0160\n",
      "Epoch [601/5000], Loss: 0.0147\n",
      "Epoch [701/5000], Loss: 0.0146\n",
      "Epoch [801/5000], Loss: 0.0142\n",
      "Epoch [901/5000], Loss: 0.0135\n",
      "Epoch [1001/5000], Loss: 0.0132\n",
      "Epoch [1101/5000], Loss: 0.0131\n",
      "Epoch [1201/5000], Loss: 0.0127\n",
      "Epoch [1301/5000], Loss: 0.0125\n",
      "Epoch [1401/5000], Loss: 0.0124\n",
      "Epoch [1501/5000], Loss: 0.0139\n",
      "Epoch [1601/5000], Loss: 0.0120\n",
      "Epoch [1701/5000], Loss: 0.0126\n",
      "Epoch [1801/5000], Loss: 0.0118\n",
      "Epoch [1901/5000], Loss: 0.0117\n",
      "Epoch [2001/5000], Loss: 0.0118\n",
      "Epoch [2101/5000], Loss: 0.0117\n",
      "Epoch [2201/5000], Loss: 0.0207\n",
      "Epoch [2301/5000], Loss: 0.0182\n",
      "Epoch [2401/5000], Loss: 0.0175\n",
      "Epoch [2501/5000], Loss: 0.0172\n",
      "Epoch [2601/5000], Loss: 0.0168\n",
      "Epoch [2701/5000], Loss: 0.0166\n",
      "Epoch [2801/5000], Loss: 0.0175\n",
      "Epoch [2901/5000], Loss: 0.0168\n",
      "Epoch [3001/5000], Loss: 0.0170\n",
      "Epoch [3101/5000], Loss: 0.0156\n",
      "Epoch [3201/5000], Loss: 0.0148\n",
      "Epoch [3301/5000], Loss: 0.0143\n",
      "Epoch [3401/5000], Loss: 0.0135\n",
      "Epoch [3501/5000], Loss: 0.0135\n",
      "Epoch [3601/5000], Loss: 0.0132\n",
      "Epoch [3701/5000], Loss: 0.0128\n",
      "Epoch [3801/5000], Loss: 0.0130\n",
      "Epoch [3901/5000], Loss: 0.0128\n",
      "Epoch [4001/5000], Loss: 0.0126\n",
      "Epoch [4101/5000], Loss: 0.0122\n",
      "Epoch [4201/5000], Loss: 0.0128\n",
      "Epoch [4301/5000], Loss: 0.0131\n",
      "Epoch [4401/5000], Loss: 0.0118\n",
      "Epoch [4501/5000], Loss: 0.0117\n",
      "Epoch [4601/5000], Loss: 0.0115\n",
      "Epoch [4701/5000], Loss: 0.0112\n",
      "Epoch [4801/5000], Loss: 0.0115\n",
      "Epoch [4901/5000], Loss: 0.0111\n",
      "Train: 0.011547913 Test: 0.012485799471687292\n",
      "Epoch [1/5000], Loss: 1.0066\n",
      "Epoch [101/5000], Loss: 0.0356\n",
      "Epoch [201/5000], Loss: 0.0218\n",
      "Epoch [301/5000], Loss: 0.0183\n",
      "Epoch [401/5000], Loss: 0.0164\n",
      "Epoch [501/5000], Loss: 0.0144\n",
      "Epoch [601/5000], Loss: 0.0130\n",
      "Epoch [701/5000], Loss: 0.0135\n",
      "Epoch [801/5000], Loss: 0.0128\n",
      "Epoch [901/5000], Loss: 0.0131\n",
      "Epoch [1001/5000], Loss: 0.0110\n",
      "Epoch [1101/5000], Loss: 0.0124\n",
      "Epoch [1201/5000], Loss: 0.0103\n",
      "Epoch [1301/5000], Loss: 0.0095\n",
      "Epoch [1401/5000], Loss: 0.0086\n",
      "Epoch [1501/5000], Loss: 0.0082\n",
      "Epoch [1601/5000], Loss: 0.0080\n",
      "Epoch [1701/5000], Loss: 0.0083\n",
      "Epoch [1801/5000], Loss: 0.0071\n",
      "Epoch [1901/5000], Loss: 0.0077\n",
      "Epoch [2001/5000], Loss: 0.0068\n",
      "Epoch [2101/5000], Loss: 0.0096\n",
      "Epoch [2201/5000], Loss: 0.0065\n",
      "Epoch [2301/5000], Loss: 0.0069\n",
      "Epoch [2401/5000], Loss: 0.0065\n",
      "Epoch [2501/5000], Loss: 0.0070\n",
      "Epoch [2601/5000], Loss: 0.0063\n",
      "Epoch [2701/5000], Loss: 0.0069\n",
      "Epoch [2801/5000], Loss: 0.0058\n",
      "Epoch [2901/5000], Loss: 0.0061\n",
      "Epoch [3001/5000], Loss: 0.0056\n",
      "Epoch [3101/5000], Loss: 0.0072\n",
      "Epoch [3201/5000], Loss: 0.0060\n",
      "Epoch [3301/5000], Loss: 0.0055\n",
      "Epoch [3401/5000], Loss: 0.0054\n",
      "Epoch [3501/5000], Loss: 0.0056\n",
      "Epoch [3601/5000], Loss: 0.0056\n",
      "Epoch [3701/5000], Loss: 0.0052\n",
      "Epoch [3801/5000], Loss: 0.0052\n",
      "Epoch [3901/5000], Loss: 0.0059\n",
      "Epoch [4001/5000], Loss: 0.0052\n",
      "Epoch [4101/5000], Loss: 0.0052\n",
      "Epoch [4201/5000], Loss: 0.0053\n",
      "Epoch [4301/5000], Loss: 0.0053\n",
      "Epoch [4401/5000], Loss: 0.0054\n",
      "Epoch [4501/5000], Loss: 0.0057\n",
      "Epoch [4601/5000], Loss: 0.0049\n",
      "Epoch [4701/5000], Loss: 0.0053\n",
      "Epoch [4801/5000], Loss: 0.0056\n",
      "Epoch [4901/5000], Loss: 0.0050\n",
      "Train: 0.0046549775 Test: 0.007085924380411721\n",
      "Epoch [1/5000], Loss: 1.5782\n",
      "Epoch [101/5000], Loss: 0.2100\n",
      "Epoch [201/5000], Loss: 0.2097\n",
      "Epoch [301/5000], Loss: 0.2097\n",
      "Epoch [401/5000], Loss: 0.2096\n",
      "Epoch [501/5000], Loss: 0.0404\n",
      "Epoch [601/5000], Loss: 0.0245\n",
      "Epoch [701/5000], Loss: 0.0214\n",
      "Epoch [801/5000], Loss: 0.0201\n",
      "Epoch [901/5000], Loss: 0.0196\n",
      "Epoch [1001/5000], Loss: 0.0190\n",
      "Epoch [1101/5000], Loss: 0.0185\n",
      "Epoch [1201/5000], Loss: 0.0179\n",
      "Epoch [1301/5000], Loss: 0.0177\n",
      "Epoch [1401/5000], Loss: 0.0175\n",
      "Epoch [1501/5000], Loss: 0.0175\n",
      "Epoch [1601/5000], Loss: 0.0172\n",
      "Epoch [1701/5000], Loss: 0.0179\n",
      "Epoch [1801/5000], Loss: 0.0171\n",
      "Epoch [1901/5000], Loss: 0.0179\n",
      "Epoch [2001/5000], Loss: 0.0189\n",
      "Epoch [2101/5000], Loss: 0.0175\n",
      "Epoch [2201/5000], Loss: 0.0174\n",
      "Epoch [2301/5000], Loss: 0.0173\n",
      "Epoch [2401/5000], Loss: 0.0169\n",
      "Epoch [2501/5000], Loss: 0.0180\n",
      "Epoch [2601/5000], Loss: 0.0180\n",
      "Epoch [2701/5000], Loss: 0.0167\n",
      "Epoch [2801/5000], Loss: 0.0175\n",
      "Epoch [2901/5000], Loss: 0.0170\n",
      "Epoch [3001/5000], Loss: 0.0167\n",
      "Epoch [3101/5000], Loss: 0.0178\n",
      "Epoch [3201/5000], Loss: 0.0165\n",
      "Epoch [3301/5000], Loss: 0.0175\n",
      "Epoch [3401/5000], Loss: 0.0165\n",
      "Epoch [3501/5000], Loss: 0.0163\n",
      "Epoch [3601/5000], Loss: 0.0164\n",
      "Epoch [3701/5000], Loss: 0.0163\n",
      "Epoch [3801/5000], Loss: 0.0163\n",
      "Epoch [3901/5000], Loss: 0.0165\n",
      "Epoch [4001/5000], Loss: 0.0166\n",
      "Epoch [4101/5000], Loss: 0.0162\n",
      "Epoch [4201/5000], Loss: 0.0177\n",
      "Epoch [4301/5000], Loss: 0.0161\n",
      "Epoch [4401/5000], Loss: 0.0170\n",
      "Epoch [4501/5000], Loss: 0.0164\n",
      "Epoch [4601/5000], Loss: 0.0164\n",
      "Epoch [4701/5000], Loss: 0.0162\n",
      "Epoch [4801/5000], Loss: 0.0163\n",
      "Epoch [4901/5000], Loss: 0.0161\n",
      "Train: 0.016242146 Test: 0.017337055907123478\n",
      "Epoch [1/5000], Loss: 0.7526\n",
      "Epoch [101/5000], Loss: 0.0294\n",
      "Epoch [201/5000], Loss: 0.0202\n",
      "Epoch [301/5000], Loss: 0.0173\n",
      "Epoch [401/5000], Loss: 0.0132\n",
      "Epoch [501/5000], Loss: 0.0126\n",
      "Epoch [601/5000], Loss: 0.0098\n",
      "Epoch [701/5000], Loss: 0.0093\n",
      "Epoch [801/5000], Loss: 0.0087\n",
      "Epoch [901/5000], Loss: 0.0083\n",
      "Epoch [1001/5000], Loss: 0.0083\n",
      "Epoch [1101/5000], Loss: 0.0079\n",
      "Epoch [1201/5000], Loss: 0.0079\n",
      "Epoch [1301/5000], Loss: 0.0076\n",
      "Epoch [1401/5000], Loss: 0.0081\n",
      "Epoch [1501/5000], Loss: 0.0082\n",
      "Epoch [1601/5000], Loss: 0.0068\n",
      "Epoch [1701/5000], Loss: 0.0076\n",
      "Epoch [1801/5000], Loss: 0.0065\n",
      "Epoch [1901/5000], Loss: 0.0064\n",
      "Epoch [2001/5000], Loss: 0.0067\n",
      "Epoch [2101/5000], Loss: 0.0065\n",
      "Epoch [2201/5000], Loss: 0.0065\n",
      "Epoch [2301/5000], Loss: 0.0061\n",
      "Epoch [2401/5000], Loss: 0.0068\n",
      "Epoch [2501/5000], Loss: 0.0060\n",
      "Epoch [2601/5000], Loss: 0.0062\n",
      "Epoch [2701/5000], Loss: 0.0063\n",
      "Epoch [2801/5000], Loss: 0.0085\n",
      "Epoch [2901/5000], Loss: 0.0055\n",
      "Epoch [3001/5000], Loss: 0.0055\n",
      "Epoch [3101/5000], Loss: 0.0062\n",
      "Epoch [3201/5000], Loss: 0.0053\n",
      "Epoch [3301/5000], Loss: 0.0052\n",
      "Epoch [3401/5000], Loss: 0.0052\n",
      "Epoch [3501/5000], Loss: 0.0061\n",
      "Epoch [3601/5000], Loss: 0.0061\n",
      "Epoch [3701/5000], Loss: 0.0059\n",
      "Epoch [3801/5000], Loss: 0.0049\n",
      "Epoch [3901/5000], Loss: 0.0054\n",
      "Epoch [4001/5000], Loss: 0.0053\n",
      "Epoch [4101/5000], Loss: 0.0066\n",
      "Epoch [4201/5000], Loss: 0.0049\n",
      "Epoch [4301/5000], Loss: 0.0049\n",
      "Epoch [4401/5000], Loss: 0.0048\n",
      "Epoch [4501/5000], Loss: 0.0047\n",
      "Epoch [4601/5000], Loss: 0.0045\n",
      "Epoch [4701/5000], Loss: 0.0045\n",
      "Epoch [4801/5000], Loss: 0.0046\n",
      "Epoch [4901/5000], Loss: 0.0047\n",
      "Train: 0.004737644 Test: 0.007053878926055642\n",
      "Epoch [1/5000], Loss: 1.3802\n",
      "Epoch [101/5000], Loss: 0.0466\n",
      "Epoch [201/5000], Loss: 0.0262\n",
      "Epoch [301/5000], Loss: 0.0206\n",
      "Epoch [401/5000], Loss: 0.0180\n",
      "Epoch [501/5000], Loss: 0.0169\n",
      "Epoch [601/5000], Loss: 0.0167\n",
      "Epoch [701/5000], Loss: 0.0162\n",
      "Epoch [801/5000], Loss: 0.0159\n",
      "Epoch [901/5000], Loss: 0.0164\n",
      "Epoch [1001/5000], Loss: 0.0160\n",
      "Epoch [1101/5000], Loss: 0.0166\n",
      "Epoch [1201/5000], Loss: 0.0163\n",
      "Epoch [1301/5000], Loss: 0.0162\n",
      "Epoch [1401/5000], Loss: 0.0154\n",
      "Epoch [1501/5000], Loss: 0.0168\n",
      "Epoch [1601/5000], Loss: 0.0136\n",
      "Epoch [1701/5000], Loss: 0.0130\n",
      "Epoch [1801/5000], Loss: 0.0124\n",
      "Epoch [1901/5000], Loss: 0.0124\n",
      "Epoch [2001/5000], Loss: 0.0133\n",
      "Epoch [2101/5000], Loss: 0.0140\n",
      "Epoch [2201/5000], Loss: 0.0118\n",
      "Epoch [2301/5000], Loss: 0.0121\n",
      "Epoch [2401/5000], Loss: 0.0128\n",
      "Epoch [2501/5000], Loss: 0.0110\n",
      "Epoch [2601/5000], Loss: 0.0103\n",
      "Epoch [2701/5000], Loss: 0.0110\n",
      "Epoch [2801/5000], Loss: 0.0098\n",
      "Epoch [2901/5000], Loss: 0.0119\n",
      "Epoch [3001/5000], Loss: 0.0094\n",
      "Epoch [3101/5000], Loss: 0.0098\n",
      "Epoch [3201/5000], Loss: 0.0100\n",
      "Epoch [3301/5000], Loss: 0.0088\n",
      "Epoch [3401/5000], Loss: 0.0096\n",
      "Epoch [3501/5000], Loss: 0.0103\n",
      "Epoch [3601/5000], Loss: 0.0088\n",
      "Epoch [3701/5000], Loss: 0.0085\n",
      "Epoch [3801/5000], Loss: 0.0111\n",
      "Epoch [3901/5000], Loss: 0.0085\n",
      "Epoch [4001/5000], Loss: 0.0084\n",
      "Epoch [4101/5000], Loss: 0.0096\n",
      "Epoch [4201/5000], Loss: 0.0102\n",
      "Epoch [4301/5000], Loss: 0.0091\n",
      "Epoch [4401/5000], Loss: 0.0089\n",
      "Epoch [4501/5000], Loss: 0.0080\n",
      "Epoch [4601/5000], Loss: 0.0088\n",
      "Epoch [4701/5000], Loss: 0.0086\n",
      "Epoch [4801/5000], Loss: 0.0092\n",
      "Epoch [4901/5000], Loss: 0.0079\n",
      "Train: 0.008121972 Test: 0.009770041896486581\n",
      "Epoch [1/5000], Loss: 0.8180\n",
      "Epoch [101/5000], Loss: 0.0382\n",
      "Epoch [201/5000], Loss: 0.0249\n",
      "Epoch [301/5000], Loss: 0.0180\n",
      "Epoch [401/5000], Loss: 0.0157\n",
      "Epoch [501/5000], Loss: 0.0144\n",
      "Epoch [601/5000], Loss: 0.0136\n",
      "Epoch [701/5000], Loss: 0.0131\n",
      "Epoch [801/5000], Loss: 0.0132\n",
      "Epoch [901/5000], Loss: 0.0123\n",
      "Epoch [1001/5000], Loss: 0.0122\n",
      "Epoch [1101/5000], Loss: 0.0122\n",
      "Epoch [1201/5000], Loss: 0.0107\n",
      "Epoch [1301/5000], Loss: 0.0093\n",
      "Epoch [1401/5000], Loss: 0.0102\n",
      "Epoch [1501/5000], Loss: 0.0091\n",
      "Epoch [1601/5000], Loss: 0.0089\n",
      "Epoch [1701/5000], Loss: 0.0080\n",
      "Epoch [1801/5000], Loss: 0.0078\n",
      "Epoch [1901/5000], Loss: 0.0072\n",
      "Epoch [2001/5000], Loss: 0.0074\n",
      "Epoch [2101/5000], Loss: 0.0103\n",
      "Epoch [2201/5000], Loss: 0.0065\n",
      "Epoch [2301/5000], Loss: 0.0066\n",
      "Epoch [2401/5000], Loss: 0.0072\n",
      "Epoch [2501/5000], Loss: 0.0061\n",
      "Epoch [2601/5000], Loss: 0.0062\n",
      "Epoch [2701/5000], Loss: 0.0069\n",
      "Epoch [2801/5000], Loss: 0.0060\n",
      "Epoch [2901/5000], Loss: 0.0058\n",
      "Epoch [3001/5000], Loss: 0.0057\n",
      "Epoch [3101/5000], Loss: 0.0056\n",
      "Epoch [3201/5000], Loss: 0.0054\n",
      "Epoch [3301/5000], Loss: 0.0056\n",
      "Epoch [3401/5000], Loss: 0.0056\n",
      "Epoch [3501/5000], Loss: 0.0056\n",
      "Epoch [3601/5000], Loss: 0.0057\n",
      "Epoch [3701/5000], Loss: 0.0052\n",
      "Epoch [3801/5000], Loss: 0.0052\n",
      "Epoch [3901/5000], Loss: 0.0055\n",
      "Epoch [4001/5000], Loss: 0.0049\n",
      "Epoch [4101/5000], Loss: 0.0054\n",
      "Epoch [4201/5000], Loss: 0.0049\n",
      "Epoch [4301/5000], Loss: 0.0051\n",
      "Epoch [4401/5000], Loss: 0.0049\n",
      "Epoch [4501/5000], Loss: 0.0047\n",
      "Epoch [4601/5000], Loss: 0.0054\n",
      "Epoch [4701/5000], Loss: 0.0047\n",
      "Epoch [4801/5000], Loss: 0.0048\n",
      "Epoch [4901/5000], Loss: 0.0045\n",
      "Train: 0.0046233167 Test: 0.007042982301368854\n",
      "Epoch [1/5000], Loss: 1.6160\n",
      "Epoch [101/5000], Loss: 0.0451\n",
      "Epoch [201/5000], Loss: 0.0228\n",
      "Epoch [301/5000], Loss: 0.0177\n",
      "Epoch [401/5000], Loss: 0.0158\n",
      "Epoch [501/5000], Loss: 0.0137\n",
      "Epoch [601/5000], Loss: 0.0120\n",
      "Epoch [701/5000], Loss: 0.0118\n",
      "Epoch [801/5000], Loss: 0.0114\n",
      "Epoch [901/5000], Loss: 0.0102\n",
      "Epoch [1001/5000], Loss: 0.0098\n",
      "Epoch [1101/5000], Loss: 0.0100\n",
      "Epoch [1201/5000], Loss: 0.0094\n",
      "Epoch [1301/5000], Loss: 0.0098\n",
      "Epoch [1401/5000], Loss: 0.0091\n",
      "Epoch [1501/5000], Loss: 0.0090\n",
      "Epoch [1601/5000], Loss: 0.0088\n",
      "Epoch [1701/5000], Loss: 0.0088\n",
      "Epoch [1801/5000], Loss: 0.0090\n",
      "Epoch [1901/5000], Loss: 0.0085\n",
      "Epoch [2001/5000], Loss: 0.0083\n",
      "Epoch [2101/5000], Loss: 0.0114\n",
      "Epoch [2201/5000], Loss: 0.0085\n",
      "Epoch [2301/5000], Loss: 0.0085\n",
      "Epoch [2401/5000], Loss: 0.0078\n",
      "Epoch [2501/5000], Loss: 0.0079\n",
      "Epoch [2601/5000], Loss: 0.0080\n",
      "Epoch [2701/5000], Loss: 0.0077\n",
      "Epoch [2801/5000], Loss: 0.0073\n",
      "Epoch [2901/5000], Loss: 0.0086\n",
      "Epoch [3001/5000], Loss: 0.0087\n",
      "Epoch [3101/5000], Loss: 0.0071\n",
      "Epoch [3201/5000], Loss: 0.0076\n",
      "Epoch [3301/5000], Loss: 0.0069\n",
      "Epoch [3401/5000], Loss: 0.0070\n",
      "Epoch [3501/5000], Loss: 0.0071\n",
      "Epoch [3601/5000], Loss: 0.0073\n",
      "Epoch [3701/5000], Loss: 0.0066\n",
      "Epoch [3801/5000], Loss: 0.0078\n",
      "Epoch [3901/5000], Loss: 0.0065\n",
      "Epoch [4001/5000], Loss: 0.0066\n",
      "Epoch [4101/5000], Loss: 0.0068\n",
      "Epoch [4201/5000], Loss: 0.0063\n",
      "Epoch [4301/5000], Loss: 0.0063\n",
      "Epoch [4401/5000], Loss: 0.0075\n",
      "Epoch [4501/5000], Loss: 0.0063\n",
      "Epoch [4601/5000], Loss: 0.0060\n",
      "Epoch [4701/5000], Loss: 0.0060\n",
      "Epoch [4801/5000], Loss: 0.0058\n",
      "Epoch [4901/5000], Loss: 0.0060\n",
      "Train: 0.00607272 Test: 0.007920195286774437\n",
      "Epoch [1/5000], Loss: 1.1474\n",
      "Epoch [101/5000], Loss: 0.0467\n",
      "Epoch [201/5000], Loss: 0.0311\n",
      "Epoch [301/5000], Loss: 0.0235\n",
      "Epoch [401/5000], Loss: 0.0204\n",
      "Epoch [501/5000], Loss: 0.0191\n",
      "Epoch [601/5000], Loss: 0.0185\n",
      "Epoch [701/5000], Loss: 0.0182\n",
      "Epoch [801/5000], Loss: 0.0187\n",
      "Epoch [901/5000], Loss: 0.0183\n",
      "Epoch [1001/5000], Loss: 0.0178\n",
      "Epoch [1101/5000], Loss: 0.0174\n",
      "Epoch [1201/5000], Loss: 0.0179\n",
      "Epoch [1301/5000], Loss: 0.0181\n",
      "Epoch [1401/5000], Loss: 0.0174\n",
      "Epoch [1501/5000], Loss: 0.0178\n",
      "Epoch [1601/5000], Loss: 0.0167\n",
      "Epoch [1701/5000], Loss: 0.0175\n",
      "Epoch [1801/5000], Loss: 0.0171\n",
      "Epoch [1901/5000], Loss: 0.0173\n",
      "Epoch [2001/5000], Loss: 0.0171\n",
      "Epoch [2101/5000], Loss: 0.0167\n",
      "Epoch [2201/5000], Loss: 0.0163\n",
      "Epoch [2301/5000], Loss: 0.0162\n",
      "Epoch [2401/5000], Loss: 0.0162\n",
      "Epoch [2501/5000], Loss: 0.0171\n",
      "Epoch [2601/5000], Loss: 0.0161\n",
      "Epoch [2701/5000], Loss: 0.0166\n",
      "Epoch [2801/5000], Loss: 0.0177\n",
      "Epoch [2901/5000], Loss: 0.0159\n",
      "Epoch [3001/5000], Loss: 0.0159\n",
      "Epoch [3101/5000], Loss: 0.0157\n",
      "Epoch [3201/5000], Loss: 0.0158\n",
      "Epoch [3301/5000], Loss: 0.0155\n",
      "Epoch [3401/5000], Loss: 0.0154\n",
      "Epoch [3501/5000], Loss: 0.0156\n",
      "Epoch [3601/5000], Loss: 0.0155\n",
      "Epoch [3701/5000], Loss: 0.0155\n",
      "Epoch [3801/5000], Loss: 0.0161\n",
      "Epoch [3901/5000], Loss: 0.0158\n",
      "Epoch [4001/5000], Loss: 0.0153\n",
      "Epoch [4101/5000], Loss: 0.0158\n",
      "Epoch [4201/5000], Loss: 0.0155\n",
      "Epoch [4301/5000], Loss: 0.0165\n",
      "Epoch [4401/5000], Loss: 0.0155\n",
      "Epoch [4501/5000], Loss: 0.0154\n",
      "Epoch [4601/5000], Loss: 0.0156\n",
      "Epoch [4701/5000], Loss: 0.0164\n",
      "Epoch [4801/5000], Loss: 0.0752\n",
      "Epoch [4901/5000], Loss: 0.0174\n",
      "Train: 0.016282098 Test: 0.016235900235779397\n",
      "Epoch [1/5000], Loss: 0.8088\n",
      "Epoch [101/5000], Loss: 0.0515\n",
      "Epoch [201/5000], Loss: 0.0265\n",
      "Epoch [301/5000], Loss: 0.0209\n",
      "Epoch [401/5000], Loss: 0.0191\n",
      "Epoch [501/5000], Loss: 0.0182\n",
      "Epoch [601/5000], Loss: 0.0178\n",
      "Epoch [701/5000], Loss: 0.0168\n",
      "Epoch [801/5000], Loss: 0.0154\n",
      "Epoch [901/5000], Loss: 0.0156\n",
      "Epoch [1001/5000], Loss: 0.0148\n",
      "Epoch [1101/5000], Loss: 0.0139\n",
      "Epoch [1201/5000], Loss: 0.0109\n",
      "Epoch [1301/5000], Loss: 0.0113\n",
      "Epoch [1401/5000], Loss: 0.0132\n",
      "Epoch [1501/5000], Loss: 0.0101\n",
      "Epoch [1601/5000], Loss: 0.0091\n",
      "Epoch [1701/5000], Loss: 0.0099\n",
      "Epoch [1801/5000], Loss: 0.0086\n",
      "Epoch [1901/5000], Loss: 0.0083\n",
      "Epoch [2001/5000], Loss: 0.0083\n",
      "Epoch [2101/5000], Loss: 0.0102\n",
      "Epoch [2201/5000], Loss: 0.0081\n",
      "Epoch [2301/5000], Loss: 0.0104\n",
      "Epoch [2401/5000], Loss: 0.0080\n",
      "Epoch [2501/5000], Loss: 0.0077\n",
      "Epoch [2601/5000], Loss: 0.0076\n",
      "Epoch [2701/5000], Loss: 0.0098\n",
      "Epoch [2801/5000], Loss: 0.0077\n",
      "Epoch [2901/5000], Loss: 0.0078\n",
      "Epoch [3001/5000], Loss: 0.0074\n",
      "Epoch [3101/5000], Loss: 0.0078\n",
      "Epoch [3201/5000], Loss: 0.0075\n",
      "Epoch [3301/5000], Loss: 0.0074\n",
      "Epoch [3401/5000], Loss: 0.0075\n",
      "Epoch [3501/5000], Loss: 0.0081\n",
      "Epoch [3601/5000], Loss: 0.0069\n",
      "Epoch [3701/5000], Loss: 0.0077\n",
      "Epoch [3801/5000], Loss: 0.0069\n",
      "Epoch [3901/5000], Loss: 0.0071\n",
      "Epoch [4001/5000], Loss: 0.0084\n",
      "Epoch [4101/5000], Loss: 0.0082\n",
      "Epoch [4201/5000], Loss: 0.0066\n",
      "Epoch [4301/5000], Loss: 0.0067\n",
      "Epoch [4401/5000], Loss: 0.0071\n",
      "Epoch [4501/5000], Loss: 0.0069\n",
      "Epoch [4601/5000], Loss: 0.0066\n",
      "Epoch [4701/5000], Loss: 0.0073\n",
      "Epoch [4801/5000], Loss: 0.0067\n",
      "Epoch [4901/5000], Loss: 0.0074\n",
      "Train: 0.007188739 Test: 0.008198485174101886\n",
      "Epoch [1/5000], Loss: 0.7549\n",
      "Epoch [101/5000], Loss: 0.0351\n",
      "Epoch [201/5000], Loss: 0.0217\n",
      "Epoch [301/5000], Loss: 0.0193\n",
      "Epoch [401/5000], Loss: 0.0182\n",
      "Epoch [501/5000], Loss: 0.0154\n",
      "Epoch [601/5000], Loss: 0.0132\n",
      "Epoch [701/5000], Loss: 0.0113\n",
      "Epoch [801/5000], Loss: 0.0102\n",
      "Epoch [901/5000], Loss: 0.0128\n",
      "Epoch [1001/5000], Loss: 0.0087\n",
      "Epoch [1101/5000], Loss: 0.0084\n",
      "Epoch [1201/5000], Loss: 0.0117\n",
      "Epoch [1301/5000], Loss: 0.0085\n",
      "Epoch [1401/5000], Loss: 0.0078\n",
      "Epoch [1501/5000], Loss: 0.0076\n",
      "Epoch [1601/5000], Loss: 0.0071\n",
      "Epoch [1701/5000], Loss: 0.0082\n",
      "Epoch [1801/5000], Loss: 0.0069\n",
      "Epoch [1901/5000], Loss: 0.0092\n",
      "Epoch [2001/5000], Loss: 0.0073\n",
      "Epoch [2101/5000], Loss: 0.0104\n",
      "Epoch [2201/5000], Loss: 0.0068\n",
      "Epoch [2301/5000], Loss: 0.0063\n",
      "Epoch [2401/5000], Loss: 0.0064\n",
      "Epoch [2501/5000], Loss: 0.0060\n",
      "Epoch [2601/5000], Loss: 0.0067\n",
      "Epoch [2701/5000], Loss: 0.0059\n",
      "Epoch [2801/5000], Loss: 0.0056\n",
      "Epoch [2901/5000], Loss: 0.0055\n",
      "Epoch [3001/5000], Loss: 0.0055\n",
      "Epoch [3101/5000], Loss: 0.0065\n",
      "Epoch [3201/5000], Loss: 0.0052\n",
      "Epoch [3301/5000], Loss: 0.0055\n",
      "Epoch [3401/5000], Loss: 0.0052\n",
      "Epoch [3501/5000], Loss: 0.0051\n",
      "Epoch [3601/5000], Loss: 0.0053\n",
      "Epoch [3701/5000], Loss: 0.0059\n",
      "Epoch [3801/5000], Loss: 0.0049\n",
      "Epoch [3901/5000], Loss: 0.0052\n",
      "Epoch [4001/5000], Loss: 0.0052\n",
      "Epoch [4101/5000], Loss: 0.0055\n",
      "Epoch [4201/5000], Loss: 0.0048\n",
      "Epoch [4301/5000], Loss: 0.0050\n",
      "Epoch [4401/5000], Loss: 0.0052\n",
      "Epoch [4501/5000], Loss: 0.0049\n",
      "Epoch [4601/5000], Loss: 0.0050\n",
      "Epoch [4701/5000], Loss: 0.0047\n",
      "Epoch [4801/5000], Loss: 0.0044\n",
      "Epoch [4901/5000], Loss: 0.0046\n",
      "Train: 0.004681619 Test: 0.006387661964668164\n",
      "Epoch [1/5000], Loss: 0.8578\n",
      "Epoch [101/5000], Loss: 0.0345\n",
      "Epoch [201/5000], Loss: 0.0205\n",
      "Epoch [301/5000], Loss: 0.0172\n",
      "Epoch [401/5000], Loss: 0.0156\n",
      "Epoch [501/5000], Loss: 0.0146\n",
      "Epoch [601/5000], Loss: 0.0142\n",
      "Epoch [701/5000], Loss: 0.0135\n",
      "Epoch [801/5000], Loss: 0.0135\n",
      "Epoch [901/5000], Loss: 0.0124\n",
      "Epoch [1001/5000], Loss: 0.0122\n",
      "Epoch [1101/5000], Loss: 0.0121\n",
      "Epoch [1201/5000], Loss: 0.0118\n",
      "Epoch [1301/5000], Loss: 0.0117\n",
      "Epoch [1401/5000], Loss: 0.0119\n",
      "Epoch [1501/5000], Loss: 0.0111\n",
      "Epoch [1601/5000], Loss: 0.0111\n",
      "Epoch [1701/5000], Loss: 0.0114\n",
      "Epoch [1801/5000], Loss: 0.0116\n",
      "Epoch [1901/5000], Loss: 0.0120\n",
      "Epoch [2001/5000], Loss: 0.0111\n",
      "Epoch [2101/5000], Loss: 0.0111\n",
      "Epoch [2201/5000], Loss: 0.0270\n",
      "Epoch [2301/5000], Loss: 0.0196\n",
      "Epoch [2401/5000], Loss: 0.0194\n",
      "Epoch [2501/5000], Loss: 0.0173\n",
      "Epoch [2601/5000], Loss: 0.0158\n",
      "Epoch [2701/5000], Loss: 0.0160\n",
      "Epoch [2801/5000], Loss: 0.0146\n",
      "Epoch [2901/5000], Loss: 0.0139\n",
      "Epoch [3001/5000], Loss: 0.0151\n",
      "Epoch [3101/5000], Loss: 0.0135\n",
      "Epoch [3201/5000], Loss: 0.0142\n",
      "Epoch [3301/5000], Loss: 0.0152\n",
      "Epoch [3401/5000], Loss: 0.0133\n",
      "Epoch [3501/5000], Loss: 0.0131\n",
      "Epoch [3601/5000], Loss: 0.0150\n",
      "Epoch [3701/5000], Loss: 0.0128\n",
      "Epoch [3801/5000], Loss: 0.0123\n",
      "Epoch [3901/5000], Loss: 0.0140\n",
      "Epoch [4001/5000], Loss: 0.0128\n",
      "Epoch [4101/5000], Loss: 0.0122\n",
      "Epoch [4201/5000], Loss: 0.0121\n",
      "Epoch [4301/5000], Loss: 0.0129\n",
      "Epoch [4401/5000], Loss: 0.0134\n",
      "Epoch [4501/5000], Loss: 0.0123\n",
      "Epoch [4601/5000], Loss: 0.0123\n",
      "Epoch [4701/5000], Loss: 0.0130\n",
      "Epoch [4801/5000], Loss: 0.0119\n",
      "Epoch [4901/5000], Loss: 0.0123\n",
      "Train: 0.01167505 Test: 0.012522442320665577\n",
      "Epoch [1/5000], Loss: 1.8286\n",
      "Epoch [101/5000], Loss: 0.0393\n",
      "Epoch [201/5000], Loss: 0.0219\n",
      "Epoch [301/5000], Loss: 0.0181\n",
      "Epoch [401/5000], Loss: 0.0166\n",
      "Epoch [501/5000], Loss: 0.0158\n",
      "Epoch [601/5000], Loss: 0.0142\n",
      "Epoch [701/5000], Loss: 0.0131\n",
      "Epoch [801/5000], Loss: 0.0122\n",
      "Epoch [901/5000], Loss: 0.0121\n",
      "Epoch [1001/5000], Loss: 0.0113\n",
      "Epoch [1101/5000], Loss: 0.0103\n",
      "Epoch [1201/5000], Loss: 0.0105\n",
      "Epoch [1301/5000], Loss: 0.0094\n",
      "Epoch [1401/5000], Loss: 0.0095\n",
      "Epoch [1501/5000], Loss: 0.0097\n",
      "Epoch [1601/5000], Loss: 0.0089\n",
      "Epoch [1701/5000], Loss: 0.0084\n",
      "Epoch [1801/5000], Loss: 0.0085\n",
      "Epoch [1901/5000], Loss: 0.0080\n",
      "Epoch [2001/5000], Loss: 0.0087\n",
      "Epoch [2101/5000], Loss: 0.0083\n",
      "Epoch [2201/5000], Loss: 0.0080\n",
      "Epoch [2301/5000], Loss: 0.0078\n",
      "Epoch [2401/5000], Loss: 0.0080\n",
      "Epoch [2501/5000], Loss: 0.0077\n",
      "Epoch [2601/5000], Loss: 0.0086\n",
      "Epoch [2701/5000], Loss: 0.0076\n",
      "Epoch [2801/5000], Loss: 0.0091\n",
      "Epoch [2901/5000], Loss: 0.0075\n",
      "Epoch [3001/5000], Loss: 0.0074\n",
      "Epoch [3101/5000], Loss: 0.0071\n",
      "Epoch [3201/5000], Loss: 0.0069\n",
      "Epoch [3301/5000], Loss: 0.0074\n",
      "Epoch [3401/5000], Loss: 0.0075\n",
      "Epoch [3501/5000], Loss: 0.0070\n",
      "Epoch [3601/5000], Loss: 0.0065\n",
      "Epoch [3701/5000], Loss: 0.0093\n",
      "Epoch [3801/5000], Loss: 0.0065\n",
      "Epoch [3901/5000], Loss: 0.0077\n",
      "Epoch [4001/5000], Loss: 0.0079\n",
      "Epoch [4101/5000], Loss: 0.0070\n",
      "Epoch [4201/5000], Loss: 0.0070\n",
      "Epoch [4301/5000], Loss: 0.0075\n",
      "Epoch [4401/5000], Loss: 0.0067\n",
      "Epoch [4501/5000], Loss: 0.0074\n",
      "Epoch [4601/5000], Loss: 0.0073\n",
      "Epoch [4701/5000], Loss: 0.0064\n",
      "Epoch [4801/5000], Loss: 0.0073\n",
      "Epoch [4901/5000], Loss: 0.0071\n",
      "Train: 0.0063934303 Test: 0.007572059983633648\n",
      "Epoch [1/5000], Loss: 1.0517\n",
      "Epoch [101/5000], Loss: 0.0364\n",
      "Epoch [201/5000], Loss: 0.0238\n",
      "Epoch [301/5000], Loss: 0.0209\n",
      "Epoch [401/5000], Loss: 0.0200\n",
      "Epoch [501/5000], Loss: 0.0193\n",
      "Epoch [601/5000], Loss: 0.0194\n",
      "Epoch [701/5000], Loss: 0.0176\n",
      "Epoch [801/5000], Loss: 0.0145\n",
      "Epoch [901/5000], Loss: 0.0138\n",
      "Epoch [1001/5000], Loss: 0.0135\n",
      "Epoch [1101/5000], Loss: 0.0140\n",
      "Epoch [1201/5000], Loss: 0.0153\n",
      "Epoch [1301/5000], Loss: 0.0133\n",
      "Epoch [1401/5000], Loss: 0.0132\n",
      "Epoch [1501/5000], Loss: 0.0123\n",
      "Epoch [1601/5000], Loss: 0.0132\n",
      "Epoch [1701/5000], Loss: 0.0126\n",
      "Epoch [1801/5000], Loss: 0.0118\n",
      "Epoch [1901/5000], Loss: 0.0122\n",
      "Epoch [2001/5000], Loss: 0.0119\n",
      "Epoch [2101/5000], Loss: 0.0115\n",
      "Epoch [2201/5000], Loss: 0.0126\n",
      "Epoch [2301/5000], Loss: 0.0112\n",
      "Epoch [2401/5000], Loss: 0.0129\n",
      "Epoch [2501/5000], Loss: 0.0118\n",
      "Epoch [2601/5000], Loss: 0.0113\n",
      "Epoch [2701/5000], Loss: 0.0108\n",
      "Epoch [2801/5000], Loss: 0.0115\n",
      "Epoch [2901/5000], Loss: 0.0125\n",
      "Epoch [3001/5000], Loss: 0.0106\n",
      "Epoch [3101/5000], Loss: 0.0121\n",
      "Epoch [3201/5000], Loss: 0.0106\n",
      "Epoch [3301/5000], Loss: 0.0106\n",
      "Epoch [3401/5000], Loss: 0.0102\n",
      "Epoch [3501/5000], Loss: 0.0108\n",
      "Epoch [3601/5000], Loss: 0.0117\n",
      "Epoch [3701/5000], Loss: 0.0108\n",
      "Epoch [3801/5000], Loss: 0.0106\n",
      "Epoch [3901/5000], Loss: 0.0111\n",
      "Epoch [4001/5000], Loss: 0.0101\n",
      "Epoch [4101/5000], Loss: 0.0094\n",
      "Epoch [4201/5000], Loss: 0.0101\n",
      "Epoch [4301/5000], Loss: 0.0093\n",
      "Epoch [4401/5000], Loss: 0.0119\n",
      "Epoch [4501/5000], Loss: 0.0096\n",
      "Epoch [4601/5000], Loss: 0.0090\n",
      "Epoch [4701/5000], Loss: 0.0095\n",
      "Epoch [4801/5000], Loss: 0.0091\n",
      "Epoch [4901/5000], Loss: 0.0086\n",
      "Train: 0.0091642635 Test: 0.010514265946845599\n",
      "Epoch [1/5000], Loss: 0.5452\n",
      "Epoch [101/5000], Loss: 0.0303\n",
      "Epoch [201/5000], Loss: 0.0171\n",
      "Epoch [301/5000], Loss: 0.0133\n",
      "Epoch [401/5000], Loss: 0.0115\n",
      "Epoch [501/5000], Loss: 0.0103\n",
      "Epoch [601/5000], Loss: 0.0097\n",
      "Epoch [701/5000], Loss: 0.0086\n",
      "Epoch [801/5000], Loss: 0.0083\n",
      "Epoch [901/5000], Loss: 0.0079\n",
      "Epoch [1001/5000], Loss: 0.0080\n",
      "Epoch [1101/5000], Loss: 0.0080\n",
      "Epoch [1201/5000], Loss: 0.0074\n",
      "Epoch [1301/5000], Loss: 0.0071\n",
      "Epoch [1401/5000], Loss: 0.0074\n",
      "Epoch [1501/5000], Loss: 0.0071\n",
      "Epoch [1601/5000], Loss: 0.0064\n",
      "Epoch [1701/5000], Loss: 0.0075\n",
      "Epoch [1801/5000], Loss: 0.0064\n",
      "Epoch [1901/5000], Loss: 0.0062\n",
      "Epoch [2001/5000], Loss: 0.0059\n",
      "Epoch [2101/5000], Loss: 0.0058\n",
      "Epoch [2201/5000], Loss: 0.0057\n",
      "Epoch [2301/5000], Loss: 0.0057\n",
      "Epoch [2401/5000], Loss: 0.0059\n",
      "Epoch [2501/5000], Loss: 0.0059\n",
      "Epoch [2601/5000], Loss: 0.0053\n",
      "Epoch [2701/5000], Loss: 0.0084\n",
      "Epoch [2801/5000], Loss: 0.0052\n",
      "Epoch [2901/5000], Loss: 0.0056\n",
      "Epoch [3001/5000], Loss: 0.0056\n",
      "Epoch [3101/5000], Loss: 0.0054\n",
      "Epoch [3201/5000], Loss: 0.0050\n",
      "Epoch [3301/5000], Loss: 0.0057\n",
      "Epoch [3401/5000], Loss: 0.0053\n",
      "Epoch [3501/5000], Loss: 0.0048\n",
      "Epoch [3601/5000], Loss: 0.0053\n",
      "Epoch [3701/5000], Loss: 0.0056\n",
      "Epoch [3801/5000], Loss: 0.0047\n",
      "Epoch [3901/5000], Loss: 0.0050\n",
      "Epoch [4001/5000], Loss: 0.0049\n",
      "Epoch [4101/5000], Loss: 0.0051\n",
      "Epoch [4201/5000], Loss: 0.0048\n",
      "Epoch [4301/5000], Loss: 0.0048\n",
      "Epoch [4401/5000], Loss: 0.0048\n",
      "Epoch [4501/5000], Loss: 0.0048\n",
      "Epoch [4601/5000], Loss: 0.0045\n",
      "Epoch [4701/5000], Loss: 0.0049\n",
      "Epoch [4801/5000], Loss: 0.0048\n",
      "Epoch [4901/5000], Loss: 0.0051\n",
      "Train: 0.004394719 Test: 0.006154843464302273\n",
      "Epoch [1/5000], Loss: 0.8832\n",
      "Epoch [101/5000], Loss: 0.0358\n",
      "Epoch [201/5000], Loss: 0.0242\n",
      "Epoch [301/5000], Loss: 0.0207\n",
      "Epoch [401/5000], Loss: 0.0161\n",
      "Epoch [501/5000], Loss: 0.0137\n",
      "Epoch [601/5000], Loss: 0.0112\n",
      "Epoch [701/5000], Loss: 0.0108\n",
      "Epoch [801/5000], Loss: 0.0100\n",
      "Epoch [901/5000], Loss: 0.0095\n",
      "Epoch [1001/5000], Loss: 0.0102\n",
      "Epoch [1101/5000], Loss: 0.0096\n",
      "Epoch [1201/5000], Loss: 0.0100\n",
      "Epoch [1301/5000], Loss: 0.0095\n",
      "Epoch [1401/5000], Loss: 0.0088\n",
      "Epoch [1501/5000], Loss: 0.0083\n",
      "Epoch [1601/5000], Loss: 0.0095\n",
      "Epoch [1701/5000], Loss: 0.0079\n",
      "Epoch [1801/5000], Loss: 0.0076\n",
      "Epoch [1901/5000], Loss: 0.0073\n",
      "Epoch [2001/5000], Loss: 0.0083\n",
      "Epoch [2101/5000], Loss: 0.0071\n",
      "Epoch [2201/5000], Loss: 0.0074\n",
      "Epoch [2301/5000], Loss: 0.0075\n",
      "Epoch [2401/5000], Loss: 0.0072\n",
      "Epoch [2501/5000], Loss: 0.0066\n",
      "Epoch [2601/5000], Loss: 0.0062\n",
      "Epoch [2701/5000], Loss: 0.0062\n",
      "Epoch [2801/5000], Loss: 0.0064\n",
      "Epoch [2901/5000], Loss: 0.0059\n",
      "Epoch [3001/5000], Loss: 0.0060\n",
      "Epoch [3101/5000], Loss: 0.0065\n",
      "Epoch [3201/5000], Loss: 0.0072\n",
      "Epoch [3301/5000], Loss: 0.0063\n",
      "Epoch [3401/5000], Loss: 0.0057\n",
      "Epoch [3501/5000], Loss: 0.0057\n",
      "Epoch [3601/5000], Loss: 0.0061\n",
      "Epoch [3701/5000], Loss: 0.0057\n",
      "Epoch [3801/5000], Loss: 0.0053\n",
      "Epoch [3901/5000], Loss: 0.0055\n",
      "Epoch [4001/5000], Loss: 0.0057\n",
      "Epoch [4101/5000], Loss: 0.0052\n",
      "Epoch [4201/5000], Loss: 0.0054\n",
      "Epoch [4301/5000], Loss: 0.0052\n",
      "Epoch [4401/5000], Loss: 0.0055\n",
      "Epoch [4501/5000], Loss: 0.0051\n",
      "Epoch [4601/5000], Loss: 0.0063\n",
      "Epoch [4701/5000], Loss: 0.0050\n",
      "Epoch [4801/5000], Loss: 0.0051\n",
      "Epoch [4901/5000], Loss: 0.0056\n",
      "Train: 0.0049528056 Test: 0.006404065493741909\n",
      "Epoch [1/5000], Loss: 1.2467\n",
      "Epoch [101/5000], Loss: 0.0487\n",
      "Epoch [201/5000], Loss: 0.0312\n",
      "Epoch [301/5000], Loss: 0.0233\n",
      "Epoch [401/5000], Loss: 0.0202\n",
      "Epoch [501/5000], Loss: 0.0188\n",
      "Epoch [601/5000], Loss: 0.0188\n",
      "Epoch [701/5000], Loss: 0.0175\n",
      "Epoch [801/5000], Loss: 0.0173\n",
      "Epoch [901/5000], Loss: 0.0175\n",
      "Epoch [1001/5000], Loss: 0.0175\n",
      "Epoch [1101/5000], Loss: 0.0170\n",
      "Epoch [1201/5000], Loss: 0.0175\n",
      "Epoch [1301/5000], Loss: 0.0173\n",
      "Epoch [1401/5000], Loss: 0.0166\n",
      "Epoch [1501/5000], Loss: 0.0166\n",
      "Epoch [1601/5000], Loss: 0.0171\n",
      "Epoch [1701/5000], Loss: 0.0170\n",
      "Epoch [1801/5000], Loss: 0.0165\n",
      "Epoch [1901/5000], Loss: 0.0164\n",
      "Epoch [2001/5000], Loss: 0.0163\n",
      "Epoch [2101/5000], Loss: 0.0164\n",
      "Epoch [2201/5000], Loss: 0.0163\n",
      "Epoch [2301/5000], Loss: 0.0167\n",
      "Epoch [2401/5000], Loss: 0.0168\n",
      "Epoch [2501/5000], Loss: 0.0164\n",
      "Epoch [2601/5000], Loss: 0.0165\n",
      "Epoch [2701/5000], Loss: 0.0161\n",
      "Epoch [2801/5000], Loss: 0.0502\n",
      "Epoch [2901/5000], Loss: 0.0235\n",
      "Epoch [3001/5000], Loss: 0.0209\n",
      "Epoch [3101/5000], Loss: 0.0199\n",
      "Epoch [3201/5000], Loss: 0.0195\n",
      "Epoch [3301/5000], Loss: 0.0190\n",
      "Epoch [3401/5000], Loss: 0.0187\n",
      "Epoch [3501/5000], Loss: 0.0188\n",
      "Epoch [3601/5000], Loss: 0.0186\n",
      "Epoch [3701/5000], Loss: 0.0183\n",
      "Epoch [3801/5000], Loss: 0.0184\n",
      "Epoch [3901/5000], Loss: 0.0185\n",
      "Epoch [4001/5000], Loss: 0.0188\n",
      "Epoch [4101/5000], Loss: 0.0207\n",
      "Epoch [4201/5000], Loss: 0.0179\n",
      "Epoch [4301/5000], Loss: 0.0173\n",
      "Epoch [4401/5000], Loss: 0.0173\n",
      "Epoch [4501/5000], Loss: 0.0177\n",
      "Epoch [4601/5000], Loss: 0.0171\n",
      "Epoch [4701/5000], Loss: 0.0170\n",
      "Epoch [4801/5000], Loss: 0.0174\n",
      "Epoch [4901/5000], Loss: 0.0169\n",
      "Train: 0.01703936 Test: 0.017116344348587043\n",
      "Epoch [1/5000], Loss: 0.7984\n",
      "Epoch [101/5000], Loss: 0.0482\n",
      "Epoch [201/5000], Loss: 0.0307\n",
      "Epoch [301/5000], Loss: 0.0238\n",
      "Epoch [401/5000], Loss: 0.0204\n",
      "Epoch [501/5000], Loss: 0.0180\n",
      "Epoch [601/5000], Loss: 0.0177\n",
      "Epoch [701/5000], Loss: 0.0174\n",
      "Epoch [801/5000], Loss: 0.0147\n",
      "Epoch [901/5000], Loss: 0.0143\n",
      "Epoch [1001/5000], Loss: 0.0134\n",
      "Epoch [1101/5000], Loss: 0.0140\n",
      "Epoch [1201/5000], Loss: 0.0133\n",
      "Epoch [1301/5000], Loss: 0.0129\n",
      "Epoch [1401/5000], Loss: 0.0129\n",
      "Epoch [1501/5000], Loss: 0.0138\n",
      "Epoch [1601/5000], Loss: 0.0124\n",
      "Epoch [1701/5000], Loss: 0.0131\n",
      "Epoch [1801/5000], Loss: 0.0128\n",
      "Epoch [1901/5000], Loss: 0.0121\n",
      "Epoch [2001/5000], Loss: 0.0120\n",
      "Epoch [2101/5000], Loss: 0.0131\n",
      "Epoch [2201/5000], Loss: 0.0119\n",
      "Epoch [2301/5000], Loss: 0.0122\n",
      "Epoch [2401/5000], Loss: 0.0130\n",
      "Epoch [2501/5000], Loss: 0.0123\n",
      "Epoch [2601/5000], Loss: 0.0124\n",
      "Epoch [2701/5000], Loss: 0.0128\n",
      "Epoch [2801/5000], Loss: 0.0128\n",
      "Epoch [2901/5000], Loss: 0.0116\n",
      "Epoch [3001/5000], Loss: 0.0117\n",
      "Epoch [3101/5000], Loss: 0.0124\n",
      "Epoch [3201/5000], Loss: 0.0115\n",
      "Epoch [3301/5000], Loss: 0.0110\n",
      "Epoch [3401/5000], Loss: 0.0109\n",
      "Epoch [3501/5000], Loss: 0.0110\n",
      "Epoch [3601/5000], Loss: 0.0116\n",
      "Epoch [3701/5000], Loss: 0.0115\n",
      "Epoch [3801/5000], Loss: 0.0112\n",
      "Epoch [3901/5000], Loss: 0.0102\n",
      "Epoch [4001/5000], Loss: 0.0105\n",
      "Epoch [4101/5000], Loss: 0.0111\n",
      "Epoch [4201/5000], Loss: 0.0098\n",
      "Epoch [4301/5000], Loss: 0.0113\n",
      "Epoch [4401/5000], Loss: 0.0096\n",
      "Epoch [4501/5000], Loss: 0.0097\n",
      "Epoch [4601/5000], Loss: 0.0110\n",
      "Epoch [4701/5000], Loss: 0.0106\n",
      "Epoch [4801/5000], Loss: 0.0096\n",
      "Epoch [4901/5000], Loss: 0.0098\n",
      "Train: 0.010275794 Test: 0.011712816528776806\n",
      "Epoch [1/5000], Loss: 0.8797\n",
      "Epoch [101/5000], Loss: 0.0549\n",
      "Epoch [201/5000], Loss: 0.0335\n",
      "Epoch [301/5000], Loss: 0.0285\n",
      "Epoch [401/5000], Loss: 0.0273\n",
      "Epoch [501/5000], Loss: 0.0262\n",
      "Epoch [601/5000], Loss: 0.0259\n",
      "Epoch [701/5000], Loss: 0.0253\n",
      "Epoch [801/5000], Loss: 0.0238\n",
      "Epoch [901/5000], Loss: 0.0184\n",
      "Epoch [1001/5000], Loss: 0.0172\n",
      "Epoch [1101/5000], Loss: 0.0179\n",
      "Epoch [1201/5000], Loss: 0.0177\n",
      "Epoch [1301/5000], Loss: 0.0176\n",
      "Epoch [1401/5000], Loss: 0.0176\n",
      "Epoch [1501/5000], Loss: 0.0171\n",
      "Epoch [1601/5000], Loss: 0.0164\n",
      "Epoch [1701/5000], Loss: 0.0163\n",
      "Epoch [1801/5000], Loss: 0.0170\n",
      "Epoch [1901/5000], Loss: 0.0168\n",
      "Epoch [2001/5000], Loss: 0.0162\n",
      "Epoch [2101/5000], Loss: 0.0161\n",
      "Epoch [2201/5000], Loss: 0.0163\n",
      "Epoch [2301/5000], Loss: 0.0218\n",
      "Epoch [2401/5000], Loss: 0.0165\n",
      "Epoch [2501/5000], Loss: 0.0158\n",
      "Epoch [2601/5000], Loss: 0.0159\n",
      "Epoch [2701/5000], Loss: 0.0164\n",
      "Epoch [2801/5000], Loss: 0.0158\n",
      "Epoch [2901/5000], Loss: 0.0162\n",
      "Epoch [3001/5000], Loss: 0.0164\n",
      "Epoch [3101/5000], Loss: 0.0159\n",
      "Epoch [3201/5000], Loss: 0.0157\n",
      "Epoch [3301/5000], Loss: 0.0156\n",
      "Epoch [3401/5000], Loss: 0.0180\n",
      "Epoch [3501/5000], Loss: 0.0160\n",
      "Epoch [3601/5000], Loss: 0.0174\n",
      "Epoch [3701/5000], Loss: 0.0159\n",
      "Epoch [3801/5000], Loss: 0.0155\n",
      "Epoch [3901/5000], Loss: 0.0155\n",
      "Epoch [4001/5000], Loss: 0.0168\n",
      "Epoch [4101/5000], Loss: 0.0153\n",
      "Epoch [4201/5000], Loss: 0.0162\n",
      "Epoch [4301/5000], Loss: 0.0168\n",
      "Epoch [4401/5000], Loss: 0.0157\n",
      "Epoch [4501/5000], Loss: 0.0154\n",
      "Epoch [4601/5000], Loss: 0.0157\n",
      "Epoch [4701/5000], Loss: 0.0171\n",
      "Epoch [4801/5000], Loss: 0.0206\n",
      "Epoch [4901/5000], Loss: 0.0152\n",
      "Train: 0.015428916 Test: 0.017887952825925563\n",
      "Epoch [1/5000], Loss: 0.9127\n",
      "Epoch [101/5000], Loss: 0.0359\n",
      "Epoch [201/5000], Loss: 0.0222\n",
      "Epoch [301/5000], Loss: 0.0176\n",
      "Epoch [401/5000], Loss: 0.0133\n",
      "Epoch [501/5000], Loss: 0.0122\n",
      "Epoch [601/5000], Loss: 0.0108\n",
      "Epoch [701/5000], Loss: 0.0118\n",
      "Epoch [801/5000], Loss: 0.0115\n",
      "Epoch [901/5000], Loss: 0.0092\n",
      "Epoch [1001/5000], Loss: 0.0091\n",
      "Epoch [1101/5000], Loss: 0.0085\n",
      "Epoch [1201/5000], Loss: 0.0083\n",
      "Epoch [1301/5000], Loss: 0.0083\n",
      "Epoch [1401/5000], Loss: 0.0076\n",
      "Epoch [1501/5000], Loss: 0.0075\n",
      "Epoch [1601/5000], Loss: 0.0073\n",
      "Epoch [1701/5000], Loss: 0.0076\n",
      "Epoch [1801/5000], Loss: 0.0070\n",
      "Epoch [1901/5000], Loss: 0.0070\n",
      "Epoch [2001/5000], Loss: 0.0069\n",
      "Epoch [2101/5000], Loss: 0.0074\n",
      "Epoch [2201/5000], Loss: 0.0065\n",
      "Epoch [2301/5000], Loss: 0.0064\n",
      "Epoch [2401/5000], Loss: 0.0068\n",
      "Epoch [2501/5000], Loss: 0.0076\n",
      "Epoch [2601/5000], Loss: 0.0062\n",
      "Epoch [2701/5000], Loss: 0.0077\n",
      "Epoch [2801/5000], Loss: 0.0062\n",
      "Epoch [2901/5000], Loss: 0.0072\n",
      "Epoch [3001/5000], Loss: 0.0064\n",
      "Epoch [3101/5000], Loss: 0.0060\n",
      "Epoch [3201/5000], Loss: 0.0060\n",
      "Epoch [3301/5000], Loss: 0.0057\n",
      "Epoch [3401/5000], Loss: 0.0057\n",
      "Epoch [3501/5000], Loss: 0.0069\n",
      "Epoch [3601/5000], Loss: 0.0056\n",
      "Epoch [3701/5000], Loss: 0.0065\n",
      "Epoch [3801/5000], Loss: 0.0060\n",
      "Epoch [3901/5000], Loss: 0.0064\n",
      "Epoch [4001/5000], Loss: 0.0057\n",
      "Epoch [4101/5000], Loss: 0.0055\n",
      "Epoch [4201/5000], Loss: 0.0058\n",
      "Epoch [4301/5000], Loss: 0.0058\n",
      "Epoch [4401/5000], Loss: 0.0058\n",
      "Epoch [4501/5000], Loss: 0.0065\n",
      "Epoch [4601/5000], Loss: 0.0056\n",
      "Epoch [4701/5000], Loss: 0.0067\n",
      "Epoch [4801/5000], Loss: 0.0057\n",
      "Epoch [4901/5000], Loss: 0.0055\n",
      "Train: 0.0061687366 Test: 0.008882024486602984\n",
      "Epoch [1/5000], Loss: 1.0409\n",
      "Epoch [101/5000], Loss: 0.0520\n",
      "Epoch [201/5000], Loss: 0.0332\n",
      "Epoch [301/5000], Loss: 0.0267\n",
      "Epoch [401/5000], Loss: 0.0200\n",
      "Epoch [501/5000], Loss: 0.0184\n",
      "Epoch [601/5000], Loss: 0.0177\n",
      "Epoch [701/5000], Loss: 0.0175\n",
      "Epoch [801/5000], Loss: 0.0170\n",
      "Epoch [901/5000], Loss: 0.0170\n",
      "Epoch [1001/5000], Loss: 0.0167\n",
      "Epoch [1101/5000], Loss: 0.0184\n",
      "Epoch [1201/5000], Loss: 0.0165\n",
      "Epoch [1301/5000], Loss: 0.0167\n",
      "Epoch [1401/5000], Loss: 0.0165\n",
      "Epoch [1501/5000], Loss: 0.0161\n",
      "Epoch [1601/5000], Loss: 0.0170\n",
      "Epoch [1701/5000], Loss: 0.0163\n",
      "Epoch [1801/5000], Loss: 0.0162\n",
      "Epoch [1901/5000], Loss: 0.0160\n",
      "Epoch [2001/5000], Loss: 0.0157\n",
      "Epoch [2101/5000], Loss: 0.0155\n",
      "Epoch [2201/5000], Loss: 0.0131\n",
      "Epoch [2301/5000], Loss: 0.0126\n",
      "Epoch [2401/5000], Loss: 0.0121\n",
      "Epoch [2501/5000], Loss: 0.0122\n",
      "Epoch [2601/5000], Loss: 0.0120\n",
      "Epoch [2701/5000], Loss: 0.0118\n",
      "Epoch [2801/5000], Loss: 0.0115\n",
      "Epoch [2901/5000], Loss: 0.0124\n",
      "Epoch [3001/5000], Loss: 0.0112\n",
      "Epoch [3101/5000], Loss: 0.0112\n",
      "Epoch [3201/5000], Loss: 0.0106\n",
      "Epoch [3301/5000], Loss: 0.0128\n",
      "Epoch [3401/5000], Loss: 0.0109\n",
      "Epoch [3501/5000], Loss: 0.0106\n",
      "Epoch [3601/5000], Loss: 0.0102\n",
      "Epoch [3701/5000], Loss: 0.0094\n",
      "Epoch [3801/5000], Loss: 0.0087\n",
      "Epoch [3901/5000], Loss: 0.0079\n",
      "Epoch [4001/5000], Loss: 0.0073\n",
      "Epoch [4101/5000], Loss: 0.0079\n",
      "Epoch [4201/5000], Loss: 0.0070\n",
      "Epoch [4301/5000], Loss: 0.0074\n",
      "Epoch [4401/5000], Loss: 0.0069\n",
      "Epoch [4501/5000], Loss: 0.0070\n",
      "Epoch [4601/5000], Loss: 0.0072\n",
      "Epoch [4701/5000], Loss: 0.0076\n",
      "Epoch [4801/5000], Loss: 0.0073\n",
      "Epoch [4901/5000], Loss: 0.0073\n",
      "Train: 0.00715178 Test: 0.009921671832581167\n",
      "Epoch [1/5000], Loss: 1.0371\n",
      "Epoch [101/5000], Loss: 0.0356\n",
      "Epoch [201/5000], Loss: 0.0213\n",
      "Epoch [301/5000], Loss: 0.0169\n",
      "Epoch [401/5000], Loss: 0.0139\n",
      "Epoch [501/5000], Loss: 0.0115\n",
      "Epoch [601/5000], Loss: 0.0103\n",
      "Epoch [701/5000], Loss: 0.0096\n",
      "Epoch [801/5000], Loss: 0.0088\n",
      "Epoch [901/5000], Loss: 0.0082\n",
      "Epoch [1001/5000], Loss: 0.0080\n",
      "Epoch [1101/5000], Loss: 0.0077\n",
      "Epoch [1201/5000], Loss: 0.0074\n",
      "Epoch [1301/5000], Loss: 0.0075\n",
      "Epoch [1401/5000], Loss: 0.0076\n",
      "Epoch [1501/5000], Loss: 0.0070\n",
      "Epoch [1601/5000], Loss: 0.0066\n",
      "Epoch [1701/5000], Loss: 0.0071\n",
      "Epoch [1801/5000], Loss: 0.0066\n",
      "Epoch [1901/5000], Loss: 0.0070\n",
      "Epoch [2001/5000], Loss: 0.0062\n",
      "Epoch [2101/5000], Loss: 0.0063\n",
      "Epoch [2201/5000], Loss: 0.0059\n",
      "Epoch [2301/5000], Loss: 0.0064\n",
      "Epoch [2401/5000], Loss: 0.0058\n",
      "Epoch [2501/5000], Loss: 0.0056\n",
      "Epoch [2601/5000], Loss: 0.0060\n",
      "Epoch [2701/5000], Loss: 0.0055\n",
      "Epoch [2801/5000], Loss: 0.0056\n",
      "Epoch [2901/5000], Loss: 0.0071\n",
      "Epoch [3001/5000], Loss: 0.0062\n",
      "Epoch [3101/5000], Loss: 0.0052\n",
      "Epoch [3201/5000], Loss: 0.0052\n",
      "Epoch [3301/5000], Loss: 0.0053\n",
      "Epoch [3401/5000], Loss: 0.0054\n",
      "Epoch [3501/5000], Loss: 0.0056\n",
      "Epoch [3601/5000], Loss: 0.0051\n",
      "Epoch [3701/5000], Loss: 0.0058\n",
      "Epoch [3801/5000], Loss: 0.0052\n",
      "Epoch [3901/5000], Loss: 0.0055\n",
      "Epoch [4001/5000], Loss: 0.0053\n",
      "Epoch [4101/5000], Loss: 0.0052\n",
      "Epoch [4201/5000], Loss: 0.0050\n",
      "Epoch [4301/5000], Loss: 0.0053\n",
      "Epoch [4401/5000], Loss: 0.0047\n",
      "Epoch [4501/5000], Loss: 0.0049\n",
      "Epoch [4601/5000], Loss: 0.0048\n",
      "Epoch [4701/5000], Loss: 0.0052\n",
      "Epoch [4801/5000], Loss: 0.0054\n",
      "Epoch [4901/5000], Loss: 0.0052\n",
      "Train: 0.0047001573 Test: 0.007642436832928298\n",
      "Epoch [1/5000], Loss: 1.6529\n",
      "Epoch [101/5000], Loss: 0.0446\n",
      "Epoch [201/5000], Loss: 0.0330\n",
      "Epoch [301/5000], Loss: 0.0287\n",
      "Epoch [401/5000], Loss: 0.0265\n",
      "Epoch [501/5000], Loss: 0.0265\n",
      "Epoch [601/5000], Loss: 0.0253\n",
      "Epoch [701/5000], Loss: 0.0258\n",
      "Epoch [801/5000], Loss: 0.0250\n",
      "Epoch [901/5000], Loss: 0.0248\n",
      "Epoch [1001/5000], Loss: 0.0245\n",
      "Epoch [1101/5000], Loss: 0.0250\n",
      "Epoch [1201/5000], Loss: 0.0244\n",
      "Epoch [1301/5000], Loss: 0.0242\n",
      "Epoch [1401/5000], Loss: 0.0258\n",
      "Epoch [1501/5000], Loss: 0.0249\n",
      "Epoch [1601/5000], Loss: 0.0245\n",
      "Epoch [1701/5000], Loss: 0.0240\n",
      "Epoch [1801/5000], Loss: 0.0239\n",
      "Epoch [1901/5000], Loss: 0.0244\n",
      "Epoch [2001/5000], Loss: 0.0238\n",
      "Epoch [2101/5000], Loss: 0.0237\n",
      "Epoch [2201/5000], Loss: 0.0234\n",
      "Epoch [2301/5000], Loss: 0.0233\n",
      "Epoch [2401/5000], Loss: 0.0232\n",
      "Epoch [2501/5000], Loss: 0.0230\n",
      "Epoch [2601/5000], Loss: 0.0228\n",
      "Epoch [2701/5000], Loss: 0.0237\n",
      "Epoch [2801/5000], Loss: 0.0230\n",
      "Epoch [2901/5000], Loss: 0.0242\n",
      "Epoch [3001/5000], Loss: 0.0239\n",
      "Epoch [3101/5000], Loss: 0.0221\n",
      "Epoch [3201/5000], Loss: 0.0224\n",
      "Epoch [3301/5000], Loss: 0.0149\n",
      "Epoch [3401/5000], Loss: 0.0145\n",
      "Epoch [3501/5000], Loss: 0.0145\n",
      "Epoch [3601/5000], Loss: 0.0139\n",
      "Epoch [3701/5000], Loss: 0.0139\n",
      "Epoch [3801/5000], Loss: 0.0137\n",
      "Epoch [3901/5000], Loss: 0.0148\n",
      "Epoch [4001/5000], Loss: 0.0136\n",
      "Epoch [4101/5000], Loss: 0.0133\n",
      "Epoch [4201/5000], Loss: 0.0152\n",
      "Epoch [4301/5000], Loss: 0.0140\n",
      "Epoch [4401/5000], Loss: 0.0132\n",
      "Epoch [4501/5000], Loss: 0.0130\n",
      "Epoch [4601/5000], Loss: 0.0127\n",
      "Epoch [4701/5000], Loss: 0.0129\n",
      "Epoch [4801/5000], Loss: 0.0130\n",
      "Epoch [4901/5000], Loss: 0.0125\n",
      "Train: 0.010902383 Test: 0.013401345610713593\n",
      "Epoch [1/5000], Loss: 0.8773\n",
      "Epoch [101/5000], Loss: 0.0367\n",
      "Epoch [201/5000], Loss: 0.0206\n",
      "Epoch [301/5000], Loss: 0.0183\n",
      "Epoch [401/5000], Loss: 0.0153\n",
      "Epoch [501/5000], Loss: 0.0130\n",
      "Epoch [601/5000], Loss: 0.0114\n",
      "Epoch [701/5000], Loss: 0.0110\n",
      "Epoch [801/5000], Loss: 0.0099\n",
      "Epoch [901/5000], Loss: 0.0096\n",
      "Epoch [1001/5000], Loss: 0.0096\n",
      "Epoch [1101/5000], Loss: 0.0098\n",
      "Epoch [1201/5000], Loss: 0.0093\n",
      "Epoch [1301/5000], Loss: 0.0092\n",
      "Epoch [1401/5000], Loss: 0.0080\n",
      "Epoch [1501/5000], Loss: 0.0079\n",
      "Epoch [1601/5000], Loss: 0.0081\n",
      "Epoch [1701/5000], Loss: 0.0078\n",
      "Epoch [1801/5000], Loss: 0.0085\n",
      "Epoch [1901/5000], Loss: 0.0073\n",
      "Epoch [2001/5000], Loss: 0.0086\n",
      "Epoch [2101/5000], Loss: 0.0102\n",
      "Epoch [2201/5000], Loss: 0.0069\n",
      "Epoch [2301/5000], Loss: 0.0068\n",
      "Epoch [2401/5000], Loss: 0.0071\n",
      "Epoch [2501/5000], Loss: 0.0069\n",
      "Epoch [2601/5000], Loss: 0.0076\n",
      "Epoch [2701/5000], Loss: 0.0076\n",
      "Epoch [2801/5000], Loss: 0.0068\n",
      "Epoch [2901/5000], Loss: 0.0069\n",
      "Epoch [3001/5000], Loss: 0.0071\n",
      "Epoch [3101/5000], Loss: 0.0062\n",
      "Epoch [3201/5000], Loss: 0.0084\n",
      "Epoch [3301/5000], Loss: 0.0065\n",
      "Epoch [3401/5000], Loss: 0.0061\n",
      "Epoch [3501/5000], Loss: 0.0062\n",
      "Epoch [3601/5000], Loss: 0.0066\n",
      "Epoch [3701/5000], Loss: 0.0068\n",
      "Epoch [3801/5000], Loss: 0.0066\n",
      "Epoch [3901/5000], Loss: 0.0060\n",
      "Epoch [4001/5000], Loss: 0.0055\n",
      "Epoch [4101/5000], Loss: 0.0059\n",
      "Epoch [4201/5000], Loss: 0.0056\n",
      "Epoch [4301/5000], Loss: 0.0054\n",
      "Epoch [4401/5000], Loss: 0.0054\n",
      "Epoch [4501/5000], Loss: 0.0053\n",
      "Epoch [4601/5000], Loss: 0.0056\n",
      "Epoch [4701/5000], Loss: 0.0053\n",
      "Epoch [4801/5000], Loss: 0.0055\n",
      "Epoch [4901/5000], Loss: 0.0052\n",
      "Train: 0.00527674 Test: 0.006949267684867962\n",
      "Epoch [1/5000], Loss: 2.0304\n",
      "Epoch [101/5000], Loss: 0.0739\n",
      "Epoch [201/5000], Loss: 0.0457\n",
      "Epoch [301/5000], Loss: 0.0372\n",
      "Epoch [401/5000], Loss: 0.0324\n",
      "Epoch [501/5000], Loss: 0.0297\n",
      "Epoch [601/5000], Loss: 0.0282\n",
      "Epoch [701/5000], Loss: 0.0276\n",
      "Epoch [801/5000], Loss: 0.0274\n",
      "Epoch [901/5000], Loss: 0.0268\n",
      "Epoch [1001/5000], Loss: 0.0266\n",
      "Epoch [1101/5000], Loss: 0.0266\n",
      "Epoch [1201/5000], Loss: 0.0267\n",
      "Epoch [1301/5000], Loss: 0.0261\n",
      "Epoch [1401/5000], Loss: 0.0262\n",
      "Epoch [1501/5000], Loss: 0.0260\n",
      "Epoch [1601/5000], Loss: 0.0277\n",
      "Epoch [1701/5000], Loss: 0.0291\n",
      "Epoch [1801/5000], Loss: 0.0259\n",
      "Epoch [1901/5000], Loss: 0.0266\n",
      "Epoch [2001/5000], Loss: 0.0261\n",
      "Epoch [2101/5000], Loss: 0.0278\n",
      "Epoch [2201/5000], Loss: 0.0265\n",
      "Epoch [2301/5000], Loss: 0.0256\n",
      "Epoch [2401/5000], Loss: 0.0268\n",
      "Epoch [2501/5000], Loss: 0.0255\n",
      "Epoch [2601/5000], Loss: 0.0257\n",
      "Epoch [2701/5000], Loss: 0.0254\n",
      "Epoch [2801/5000], Loss: 0.0255\n",
      "Epoch [2901/5000], Loss: 0.0260\n",
      "Epoch [3001/5000], Loss: 0.0265\n",
      "Epoch [3101/5000], Loss: 0.0254\n",
      "Epoch [3201/5000], Loss: 0.0259\n",
      "Epoch [3301/5000], Loss: 0.0253\n",
      "Epoch [3401/5000], Loss: 0.0253\n",
      "Epoch [3501/5000], Loss: 0.0256\n",
      "Epoch [3601/5000], Loss: 0.0256\n",
      "Epoch [3701/5000], Loss: 0.0266\n",
      "Epoch [3801/5000], Loss: 0.0266\n",
      "Epoch [3901/5000], Loss: 0.0278\n",
      "Epoch [4001/5000], Loss: 0.0254\n",
      "Epoch [4101/5000], Loss: 0.0252\n",
      "Epoch [4201/5000], Loss: 0.0260\n",
      "Epoch [4301/5000], Loss: 0.0294\n",
      "Epoch [4401/5000], Loss: 0.0256\n",
      "Epoch [4501/5000], Loss: 0.0257\n",
      "Epoch [4601/5000], Loss: 0.0252\n",
      "Epoch [4701/5000], Loss: 0.0253\n",
      "Epoch [4801/5000], Loss: 0.0253\n",
      "Epoch [4901/5000], Loss: 0.0252\n",
      "Train: 0.025162376 Test: 0.026244696910546126\n",
      "Epoch [1/5000], Loss: 0.5262\n",
      "Epoch [101/5000], Loss: 0.0351\n",
      "Epoch [201/5000], Loss: 0.0191\n",
      "Epoch [301/5000], Loss: 0.0155\n",
      "Epoch [401/5000], Loss: 0.0129\n",
      "Epoch [501/5000], Loss: 0.0109\n",
      "Epoch [601/5000], Loss: 0.0098\n",
      "Epoch [701/5000], Loss: 0.0096\n",
      "Epoch [801/5000], Loss: 0.0091\n",
      "Epoch [901/5000], Loss: 0.0088\n",
      "Epoch [1001/5000], Loss: 0.0083\n",
      "Epoch [1101/5000], Loss: 0.0079\n",
      "Epoch [1201/5000], Loss: 0.0082\n",
      "Epoch [1301/5000], Loss: 0.0075\n",
      "Epoch [1401/5000], Loss: 0.0077\n",
      "Epoch [1501/5000], Loss: 0.0091\n",
      "Epoch [1601/5000], Loss: 0.0069\n",
      "Epoch [1701/5000], Loss: 0.0070\n",
      "Epoch [1801/5000], Loss: 0.0069\n",
      "Epoch [1901/5000], Loss: 0.0068\n",
      "Epoch [2001/5000], Loss: 0.0064\n",
      "Epoch [2101/5000], Loss: 0.0068\n",
      "Epoch [2201/5000], Loss: 0.0068\n",
      "Epoch [2301/5000], Loss: 0.0063\n",
      "Epoch [2401/5000], Loss: 0.0063\n",
      "Epoch [2501/5000], Loss: 0.0058\n",
      "Epoch [2601/5000], Loss: 0.0068\n",
      "Epoch [2701/5000], Loss: 0.0059\n",
      "Epoch [2801/5000], Loss: 0.0060\n",
      "Epoch [2901/5000], Loss: 0.0060\n",
      "Epoch [3001/5000], Loss: 0.0054\n",
      "Epoch [3101/5000], Loss: 0.0058\n",
      "Epoch [3201/5000], Loss: 0.0064\n",
      "Epoch [3301/5000], Loss: 0.0052\n",
      "Epoch [3401/5000], Loss: 0.0051\n",
      "Epoch [3501/5000], Loss: 0.0054\n",
      "Epoch [3601/5000], Loss: 0.0049\n",
      "Epoch [3701/5000], Loss: 0.0050\n",
      "Epoch [3801/5000], Loss: 0.0056\n",
      "Epoch [3901/5000], Loss: 0.0050\n",
      "Epoch [4001/5000], Loss: 0.0047\n",
      "Epoch [4101/5000], Loss: 0.0050\n",
      "Epoch [4201/5000], Loss: 0.0048\n",
      "Epoch [4301/5000], Loss: 0.0052\n",
      "Epoch [4401/5000], Loss: 0.0052\n",
      "Epoch [4501/5000], Loss: 0.0047\n",
      "Epoch [4601/5000], Loss: 0.0052\n",
      "Epoch [4701/5000], Loss: 0.0046\n",
      "Epoch [4801/5000], Loss: 0.0054\n",
      "Epoch [4901/5000], Loss: 0.0056\n",
      "Train: 0.004856757 Test: 0.006739744169583718\n",
      "Epoch [1/5000], Loss: 0.8058\n",
      "Epoch [101/5000], Loss: 0.0381\n",
      "Epoch [201/5000], Loss: 0.0222\n",
      "Epoch [301/5000], Loss: 0.0176\n",
      "Epoch [401/5000], Loss: 0.0154\n",
      "Epoch [501/5000], Loss: 0.0121\n",
      "Epoch [601/5000], Loss: 0.0112\n",
      "Epoch [701/5000], Loss: 0.0106\n",
      "Epoch [801/5000], Loss: 0.0098\n",
      "Epoch [901/5000], Loss: 0.0094\n",
      "Epoch [1001/5000], Loss: 0.0094\n",
      "Epoch [1101/5000], Loss: 0.0087\n",
      "Epoch [1201/5000], Loss: 0.0100\n",
      "Epoch [1301/5000], Loss: 0.0089\n",
      "Epoch [1401/5000], Loss: 0.0082\n",
      "Epoch [1501/5000], Loss: 0.0079\n",
      "Epoch [1601/5000], Loss: 0.0086\n",
      "Epoch [1701/5000], Loss: 0.0080\n",
      "Epoch [1801/5000], Loss: 0.0076\n",
      "Epoch [1901/5000], Loss: 0.0073\n",
      "Epoch [2001/5000], Loss: 0.0073\n",
      "Epoch [2101/5000], Loss: 0.0071\n",
      "Epoch [2201/5000], Loss: 0.0078\n",
      "Epoch [2301/5000], Loss: 0.0068\n",
      "Epoch [2401/5000], Loss: 0.0069\n",
      "Epoch [2501/5000], Loss: 0.0067\n",
      "Epoch [2601/5000], Loss: 0.0073\n",
      "Epoch [2701/5000], Loss: 0.0069\n",
      "Epoch [2801/5000], Loss: 0.0065\n",
      "Epoch [2901/5000], Loss: 0.0071\n",
      "Epoch [3001/5000], Loss: 0.0063\n",
      "Epoch [3101/5000], Loss: 0.0066\n",
      "Epoch [3201/5000], Loss: 0.0064\n",
      "Epoch [3301/5000], Loss: 0.0061\n",
      "Epoch [3401/5000], Loss: 0.0064\n",
      "Epoch [3501/5000], Loss: 0.0086\n",
      "Epoch [3601/5000], Loss: 0.0059\n",
      "Epoch [3701/5000], Loss: 0.0059\n",
      "Epoch [3801/5000], Loss: 0.0067\n",
      "Epoch [3901/5000], Loss: 0.0062\n",
      "Epoch [4001/5000], Loss: 0.0062\n",
      "Epoch [4101/5000], Loss: 0.0062\n",
      "Epoch [4201/5000], Loss: 0.0056\n",
      "Epoch [4301/5000], Loss: 0.0060\n",
      "Epoch [4401/5000], Loss: 0.0063\n",
      "Epoch [4501/5000], Loss: 0.0056\n",
      "Epoch [4601/5000], Loss: 0.0066\n",
      "Epoch [4701/5000], Loss: 0.0056\n",
      "Epoch [4801/5000], Loss: 0.0055\n",
      "Epoch [4901/5000], Loss: 0.0052\n",
      "Train: 0.0051486488 Test: 0.007125584709160024\n",
      "Epoch [1/5000], Loss: 0.8273\n",
      "Epoch [101/5000], Loss: 0.0462\n",
      "Epoch [201/5000], Loss: 0.0273\n",
      "Epoch [301/5000], Loss: 0.0211\n",
      "Epoch [401/5000], Loss: 0.0186\n",
      "Epoch [501/5000], Loss: 0.0176\n",
      "Epoch [601/5000], Loss: 0.0164\n",
      "Epoch [701/5000], Loss: 0.0160\n",
      "Epoch [801/5000], Loss: 0.0151\n",
      "Epoch [901/5000], Loss: 0.0139\n",
      "Epoch [1001/5000], Loss: 0.0134\n",
      "Epoch [1101/5000], Loss: 0.0138\n",
      "Epoch [1201/5000], Loss: 0.0136\n",
      "Epoch [1301/5000], Loss: 0.0144\n",
      "Epoch [1401/5000], Loss: 0.0135\n",
      "Epoch [1501/5000], Loss: 0.0122\n",
      "Epoch [1601/5000], Loss: 0.0138\n",
      "Epoch [1701/5000], Loss: 0.0122\n",
      "Epoch [1801/5000], Loss: 0.0127\n",
      "Epoch [1901/5000], Loss: 0.0127\n",
      "Epoch [2001/5000], Loss: 0.0115\n",
      "Epoch [2101/5000], Loss: 0.0122\n",
      "Epoch [2201/5000], Loss: 0.0114\n",
      "Epoch [2301/5000], Loss: 0.0144\n",
      "Epoch [2401/5000], Loss: 0.0113\n",
      "Epoch [2501/5000], Loss: 0.0114\n",
      "Epoch [2601/5000], Loss: 0.0109\n",
      "Epoch [2701/5000], Loss: 0.0117\n",
      "Epoch [2801/5000], Loss: 0.0107\n",
      "Epoch [2901/5000], Loss: 0.0106\n",
      "Epoch [3001/5000], Loss: 0.0108\n",
      "Epoch [3101/5000], Loss: 0.0108\n",
      "Epoch [3201/5000], Loss: 0.0107\n",
      "Epoch [3301/5000], Loss: 0.0116\n",
      "Epoch [3401/5000], Loss: 0.0125\n",
      "Epoch [3501/5000], Loss: 0.0103\n",
      "Epoch [3601/5000], Loss: 0.0102\n",
      "Epoch [3701/5000], Loss: 0.0109\n",
      "Epoch [3801/5000], Loss: 0.0109\n",
      "Epoch [3901/5000], Loss: 0.0104\n",
      "Epoch [4001/5000], Loss: 0.0105\n",
      "Epoch [4101/5000], Loss: 0.0104\n",
      "Epoch [4201/5000], Loss: 0.0101\n",
      "Epoch [4301/5000], Loss: 0.0101\n",
      "Epoch [4401/5000], Loss: 0.0104\n",
      "Epoch [4501/5000], Loss: 0.0096\n",
      "Epoch [4601/5000], Loss: 0.0097\n",
      "Epoch [4701/5000], Loss: 0.0101\n",
      "Epoch [4801/5000], Loss: 0.0096\n",
      "Epoch [4901/5000], Loss: 0.0106\n",
      "Train: 0.009770232 Test: 0.011457014815447614\n",
      "Epoch [1/5000], Loss: 1.3134\n",
      "Epoch [101/5000], Loss: 0.1785\n",
      "Epoch [201/5000], Loss: 0.0406\n",
      "Epoch [301/5000], Loss: 0.0288\n",
      "Epoch [401/5000], Loss: 0.0236\n",
      "Epoch [501/5000], Loss: 0.0258\n",
      "Epoch [601/5000], Loss: 0.0202\n",
      "Epoch [701/5000], Loss: 0.0194\n",
      "Epoch [801/5000], Loss: 0.0186\n",
      "Epoch [901/5000], Loss: 0.0192\n",
      "Epoch [1001/5000], Loss: 0.0156\n",
      "Epoch [1101/5000], Loss: 0.0175\n",
      "Epoch [1201/5000], Loss: 0.0144\n",
      "Epoch [1301/5000], Loss: 0.0133\n",
      "Epoch [1401/5000], Loss: 0.0127\n",
      "Epoch [1501/5000], Loss: 0.0131\n",
      "Epoch [1601/5000], Loss: 0.0117\n",
      "Epoch [1701/5000], Loss: 0.0119\n",
      "Epoch [1801/5000], Loss: 0.0121\n",
      "Epoch [1901/5000], Loss: 0.0119\n",
      "Epoch [2001/5000], Loss: 0.0110\n",
      "Epoch [2101/5000], Loss: 0.0108\n",
      "Epoch [2201/5000], Loss: 0.0123\n",
      "Epoch [2301/5000], Loss: 0.0106\n",
      "Epoch [2401/5000], Loss: 0.0109\n",
      "Epoch [2501/5000], Loss: 0.0103\n",
      "Epoch [2601/5000], Loss: 0.0103\n",
      "Epoch [2701/5000], Loss: 0.0109\n",
      "Epoch [2801/5000], Loss: 0.0116\n",
      "Epoch [2901/5000], Loss: 0.0106\n",
      "Epoch [3001/5000], Loss: 0.0107\n",
      "Epoch [3101/5000], Loss: 0.0108\n",
      "Epoch [3201/5000], Loss: 0.0099\n",
      "Epoch [3301/5000], Loss: 0.0129\n",
      "Epoch [3401/5000], Loss: 0.0100\n",
      "Epoch [3501/5000], Loss: 0.0101\n",
      "Epoch [3601/5000], Loss: 0.0106\n",
      "Epoch [3701/5000], Loss: 0.0118\n",
      "Epoch [3801/5000], Loss: 0.0097\n",
      "Epoch [3901/5000], Loss: 0.0098\n",
      "Epoch [4001/5000], Loss: 0.0110\n",
      "Epoch [4101/5000], Loss: 0.0105\n",
      "Epoch [4201/5000], Loss: 0.0098\n",
      "Epoch [4301/5000], Loss: 0.0106\n",
      "Epoch [4401/5000], Loss: 0.0110\n",
      "Epoch [4501/5000], Loss: 0.0107\n",
      "Epoch [4601/5000], Loss: 0.0107\n",
      "Epoch [4701/5000], Loss: 0.0108\n",
      "Epoch [4801/5000], Loss: 0.0098\n",
      "Epoch [4901/5000], Loss: 0.0101\n",
      "Train: 0.010407346 Test: 0.011117667212991534\n",
      "Epoch [1/5000], Loss: 0.8888\n",
      "Epoch [101/5000], Loss: 0.0318\n",
      "Epoch [201/5000], Loss: 0.0206\n",
      "Epoch [301/5000], Loss: 0.0187\n",
      "Epoch [401/5000], Loss: 0.0175\n",
      "Epoch [501/5000], Loss: 0.0168\n",
      "Epoch [601/5000], Loss: 0.0159\n",
      "Epoch [701/5000], Loss: 0.0174\n",
      "Epoch [801/5000], Loss: 0.0175\n",
      "Epoch [901/5000], Loss: 0.0130\n",
      "Epoch [1001/5000], Loss: 0.0127\n",
      "Epoch [1101/5000], Loss: 0.0123\n",
      "Epoch [1201/5000], Loss: 0.0121\n",
      "Epoch [1301/5000], Loss: 0.0122\n",
      "Epoch [1401/5000], Loss: 0.0123\n",
      "Epoch [1501/5000], Loss: 0.0113\n",
      "Epoch [1601/5000], Loss: 0.0109\n",
      "Epoch [1701/5000], Loss: 0.0111\n",
      "Epoch [1801/5000], Loss: 0.0109\n",
      "Epoch [1901/5000], Loss: 0.0100\n",
      "Epoch [2001/5000], Loss: 0.0089\n",
      "Epoch [2101/5000], Loss: 0.0081\n",
      "Epoch [2201/5000], Loss: 0.0085\n",
      "Epoch [2301/5000], Loss: 0.0080\n",
      "Epoch [2401/5000], Loss: 0.0083\n",
      "Epoch [2501/5000], Loss: 0.0091\n",
      "Epoch [2601/5000], Loss: 0.0076\n",
      "Epoch [2701/5000], Loss: 0.0072\n",
      "Epoch [2801/5000], Loss: 0.0075\n",
      "Epoch [2901/5000], Loss: 0.0070\n",
      "Epoch [3001/5000], Loss: 0.0068\n",
      "Epoch [3101/5000], Loss: 0.0069\n",
      "Epoch [3201/5000], Loss: 0.0067\n",
      "Epoch [3301/5000], Loss: 0.0067\n",
      "Epoch [3401/5000], Loss: 0.0064\n",
      "Epoch [3501/5000], Loss: 0.0068\n",
      "Epoch [3601/5000], Loss: 0.0070\n",
      "Epoch [3701/5000], Loss: 0.0084\n",
      "Epoch [3801/5000], Loss: 0.0064\n",
      "Epoch [3901/5000], Loss: 0.0064\n",
      "Epoch [4001/5000], Loss: 0.0062\n",
      "Epoch [4101/5000], Loss: 0.0063\n",
      "Epoch [4201/5000], Loss: 0.0056\n",
      "Epoch [4301/5000], Loss: 0.0059\n",
      "Epoch [4401/5000], Loss: 0.0055\n",
      "Epoch [4501/5000], Loss: 0.0066\n",
      "Epoch [4601/5000], Loss: 0.0054\n",
      "Epoch [4701/5000], Loss: 0.0060\n",
      "Epoch [4801/5000], Loss: 0.0054\n",
      "Epoch [4901/5000], Loss: 0.0053\n",
      "Train: 0.0053265593 Test: 0.006940916382130447\n",
      "Epoch [1/5000], Loss: 1.4188\n",
      "Epoch [101/5000], Loss: 0.2101\n",
      "Epoch [201/5000], Loss: 0.2094\n",
      "Epoch [301/5000], Loss: 0.0476\n",
      "Epoch [401/5000], Loss: 0.0379\n",
      "Epoch [501/5000], Loss: 0.0355\n",
      "Epoch [601/5000], Loss: 0.0385\n",
      "Epoch [701/5000], Loss: 0.0338\n",
      "Epoch [801/5000], Loss: 0.0334\n",
      "Epoch [901/5000], Loss: 0.0332\n",
      "Epoch [1001/5000], Loss: 0.0331\n",
      "Epoch [1101/5000], Loss: 0.0332\n",
      "Epoch [1201/5000], Loss: 0.0330\n",
      "Epoch [1301/5000], Loss: 0.0327\n",
      "Epoch [1401/5000], Loss: 0.0326\n",
      "Epoch [1501/5000], Loss: 0.0328\n",
      "Epoch [1601/5000], Loss: 0.0325\n",
      "Epoch [1701/5000], Loss: 0.0326\n",
      "Epoch [1801/5000], Loss: 0.0328\n",
      "Epoch [1901/5000], Loss: 0.0331\n",
      "Epoch [2001/5000], Loss: 0.0327\n",
      "Epoch [2101/5000], Loss: 0.0323\n",
      "Epoch [2201/5000], Loss: 0.0314\n",
      "Epoch [2301/5000], Loss: 0.0288\n",
      "Epoch [2401/5000], Loss: 0.0277\n",
      "Epoch [2501/5000], Loss: 0.0257\n",
      "Epoch [2601/5000], Loss: 0.0281\n",
      "Epoch [2701/5000], Loss: 0.0279\n",
      "Epoch [2801/5000], Loss: 0.0263\n",
      "Epoch [2901/5000], Loss: 0.0256\n",
      "Epoch [3001/5000], Loss: 0.0257\n",
      "Epoch [3101/5000], Loss: 0.0259\n",
      "Epoch [3201/5000], Loss: 0.0267\n",
      "Epoch [3301/5000], Loss: 0.0288\n",
      "Epoch [3401/5000], Loss: 0.0254\n",
      "Epoch [3501/5000], Loss: 0.0254\n",
      "Epoch [3601/5000], Loss: 0.0255\n",
      "Epoch [3701/5000], Loss: 0.0253\n",
      "Epoch [3801/5000], Loss: 0.0263\n",
      "Epoch [3901/5000], Loss: 0.0273\n",
      "Epoch [4001/5000], Loss: 0.0255\n",
      "Epoch [4101/5000], Loss: 0.0265\n",
      "Epoch [4201/5000], Loss: 0.0254\n",
      "Epoch [4301/5000], Loss: 0.0264\n",
      "Epoch [4401/5000], Loss: 0.0251\n",
      "Epoch [4501/5000], Loss: 0.0251\n",
      "Epoch [4601/5000], Loss: 0.0262\n",
      "Epoch [4701/5000], Loss: 0.0286\n",
      "Epoch [4801/5000], Loss: 0.0268\n",
      "Epoch [4901/5000], Loss: 0.0255\n",
      "Train: 0.02499864 Test: 0.025846449592034037\n",
      "Epoch [1/5000], Loss: 1.4412\n",
      "Epoch [101/5000], Loss: 0.0480\n",
      "Epoch [201/5000], Loss: 0.0343\n",
      "Epoch [301/5000], Loss: 0.0251\n",
      "Epoch [401/5000], Loss: 0.0202\n",
      "Epoch [501/5000], Loss: 0.0190\n",
      "Epoch [601/5000], Loss: 0.0190\n",
      "Epoch [701/5000], Loss: 0.0185\n",
      "Epoch [801/5000], Loss: 0.0176\n",
      "Epoch [901/5000], Loss: 0.0183\n",
      "Epoch [1001/5000], Loss: 0.0173\n",
      "Epoch [1101/5000], Loss: 0.0178\n",
      "Epoch [1201/5000], Loss: 0.0173\n",
      "Epoch [1301/5000], Loss: 0.0171\n",
      "Epoch [1401/5000], Loss: 0.0170\n",
      "Epoch [1501/5000], Loss: 0.0180\n",
      "Epoch [1601/5000], Loss: 0.0177\n",
      "Epoch [1701/5000], Loss: 0.0167\n",
      "Epoch [1801/5000], Loss: 0.0171\n",
      "Epoch [1901/5000], Loss: 0.0177\n",
      "Epoch [2001/5000], Loss: 0.0170\n",
      "Epoch [2101/5000], Loss: 0.0175\n",
      "Epoch [2201/5000], Loss: 0.0175\n",
      "Epoch [2301/5000], Loss: 0.0171\n",
      "Epoch [2401/5000], Loss: 0.0192\n",
      "Epoch [2501/5000], Loss: 0.0167\n",
      "Epoch [2601/5000], Loss: 0.0161\n",
      "Epoch [2701/5000], Loss: 0.0161\n",
      "Epoch [2801/5000], Loss: 0.0160\n",
      "Epoch [2901/5000], Loss: 0.0161\n",
      "Epoch [3001/5000], Loss: 0.0162\n",
      "Epoch [3101/5000], Loss: 0.0168\n",
      "Epoch [3201/5000], Loss: 0.0163\n",
      "Epoch [3301/5000], Loss: 0.0167\n",
      "Epoch [3401/5000], Loss: 0.0159\n",
      "Epoch [3501/5000], Loss: 0.0158\n",
      "Epoch [3601/5000], Loss: 0.0159\n",
      "Epoch [3701/5000], Loss: 0.0161\n",
      "Epoch [3801/5000], Loss: 0.0163\n",
      "Epoch [3901/5000], Loss: 0.0159\n",
      "Epoch [4001/5000], Loss: 0.0174\n",
      "Epoch [4101/5000], Loss: 0.0158\n",
      "Epoch [4201/5000], Loss: 0.0156\n",
      "Epoch [4301/5000], Loss: 0.0171\n",
      "Epoch [4401/5000], Loss: 0.0155\n",
      "Epoch [4501/5000], Loss: 0.0166\n",
      "Epoch [4601/5000], Loss: 0.0160\n",
      "Epoch [4701/5000], Loss: 0.0167\n",
      "Epoch [4801/5000], Loss: 0.0168\n",
      "Epoch [4901/5000], Loss: 0.0156\n",
      "Train: 0.017235465 Test: 0.018327217304230353\n",
      "Epoch [1/5000], Loss: 0.8973\n",
      "Epoch [101/5000], Loss: 0.0440\n",
      "Epoch [201/5000], Loss: 0.0251\n",
      "Epoch [301/5000], Loss: 0.0197\n",
      "Epoch [401/5000], Loss: 0.0181\n",
      "Epoch [501/5000], Loss: 0.0177\n",
      "Epoch [601/5000], Loss: 0.0169\n",
      "Epoch [701/5000], Loss: 0.0182\n",
      "Epoch [801/5000], Loss: 0.0162\n",
      "Epoch [901/5000], Loss: 0.0152\n",
      "Epoch [1001/5000], Loss: 0.0152\n",
      "Epoch [1101/5000], Loss: 0.0134\n",
      "Epoch [1201/5000], Loss: 0.0134\n",
      "Epoch [1301/5000], Loss: 0.0138\n",
      "Epoch [1401/5000], Loss: 0.0128\n",
      "Epoch [1501/5000], Loss: 0.0127\n",
      "Epoch [1601/5000], Loss: 0.0124\n",
      "Epoch [1701/5000], Loss: 0.0128\n",
      "Epoch [1801/5000], Loss: 0.0123\n",
      "Epoch [1901/5000], Loss: 0.0129\n",
      "Epoch [2001/5000], Loss: 0.0119\n",
      "Epoch [2101/5000], Loss: 0.0122\n",
      "Epoch [2201/5000], Loss: 0.0118\n",
      "Epoch [2301/5000], Loss: 0.0132\n",
      "Epoch [2401/5000], Loss: 0.0114\n",
      "Epoch [2501/5000], Loss: 0.0128\n",
      "Epoch [2601/5000], Loss: 0.0115\n",
      "Epoch [2701/5000], Loss: 0.0111\n",
      "Epoch [2801/5000], Loss: 0.0121\n",
      "Epoch [2901/5000], Loss: 0.0116\n",
      "Epoch [3001/5000], Loss: 0.0109\n",
      "Epoch [3101/5000], Loss: 0.0114\n",
      "Epoch [3201/5000], Loss: 0.0119\n",
      "Epoch [3301/5000], Loss: 0.0108\n",
      "Epoch [3401/5000], Loss: 0.0107\n",
      "Epoch [3501/5000], Loss: 0.0108\n",
      "Epoch [3601/5000], Loss: 0.0112\n",
      "Epoch [3701/5000], Loss: 0.1046\n",
      "Epoch [3801/5000], Loss: 0.0487\n",
      "Epoch [3901/5000], Loss: 0.0633\n",
      "Epoch [4001/5000], Loss: 0.0447\n",
      "Epoch [4101/5000], Loss: 0.0370\n",
      "Epoch [4201/5000], Loss: 0.0337\n",
      "Epoch [4301/5000], Loss: 0.0302\n",
      "Epoch [4401/5000], Loss: 0.0299\n",
      "Epoch [4501/5000], Loss: 0.0298\n",
      "Epoch [4601/5000], Loss: 0.0293\n",
      "Epoch [4701/5000], Loss: 0.0291\n",
      "Epoch [4801/5000], Loss: 0.0279\n",
      "Epoch [4901/5000], Loss: 0.0279\n",
      "Train: 0.029099567 Test: 0.030092439910230338\n",
      "Epoch [1/5000], Loss: 1.2233\n",
      "Epoch [101/5000], Loss: 0.0827\n",
      "Epoch [201/5000], Loss: 0.0512\n",
      "Epoch [301/5000], Loss: 0.0419\n",
      "Epoch [401/5000], Loss: 0.0385\n",
      "Epoch [501/5000], Loss: 0.0401\n",
      "Epoch [601/5000], Loss: 0.0349\n",
      "Epoch [701/5000], Loss: 0.0424\n",
      "Epoch [801/5000], Loss: 0.0349\n",
      "Epoch [901/5000], Loss: 0.0341\n",
      "Epoch [1001/5000], Loss: 0.0340\n",
      "Epoch [1101/5000], Loss: 0.0335\n",
      "Epoch [1201/5000], Loss: 0.0342\n",
      "Epoch [1301/5000], Loss: 0.0409\n",
      "Epoch [1401/5000], Loss: 0.0330\n",
      "Epoch [1501/5000], Loss: 0.0355\n",
      "Epoch [1601/5000], Loss: 0.0328\n",
      "Epoch [1701/5000], Loss: 0.0341\n",
      "Epoch [1801/5000], Loss: 0.0326\n",
      "Epoch [1901/5000], Loss: 0.0338\n",
      "Epoch [2001/5000], Loss: 0.0347\n",
      "Epoch [2101/5000], Loss: 0.0331\n",
      "Epoch [2201/5000], Loss: 0.0334\n",
      "Epoch [2301/5000], Loss: 0.0325\n",
      "Epoch [2401/5000], Loss: 0.0324\n",
      "Epoch [2501/5000], Loss: 0.0399\n",
      "Epoch [2601/5000], Loss: 0.0368\n",
      "Epoch [2701/5000], Loss: 0.0359\n",
      "Epoch [2801/5000], Loss: 0.0353\n",
      "Epoch [2901/5000], Loss: 0.0352\n",
      "Epoch [3001/5000], Loss: 0.0345\n",
      "Epoch [3101/5000], Loss: 0.0344\n",
      "Epoch [3201/5000], Loss: 0.0349\n",
      "Epoch [3301/5000], Loss: 0.0340\n",
      "Epoch [3401/5000], Loss: 0.0338\n",
      "Epoch [3501/5000], Loss: 0.0336\n",
      "Epoch [3601/5000], Loss: 0.0339\n",
      "Epoch [3701/5000], Loss: 0.0334\n",
      "Epoch [3801/5000], Loss: 0.0333\n",
      "Epoch [3901/5000], Loss: 0.0381\n",
      "Epoch [4001/5000], Loss: 0.0332\n",
      "Epoch [4101/5000], Loss: 0.0280\n",
      "Epoch [4201/5000], Loss: 0.0271\n",
      "Epoch [4301/5000], Loss: 0.0276\n",
      "Epoch [4401/5000], Loss: 0.0265\n",
      "Epoch [4501/5000], Loss: 0.0270\n",
      "Epoch [4601/5000], Loss: 0.0265\n",
      "Epoch [4701/5000], Loss: 0.0277\n",
      "Epoch [4801/5000], Loss: 0.0307\n",
      "Epoch [4901/5000], Loss: 0.0262\n",
      "Train: 0.02642223 Test: 0.027385149400705137\n",
      "Epoch [1/5000], Loss: 1.2476\n",
      "Epoch [101/5000], Loss: 0.0558\n",
      "Epoch [201/5000], Loss: 0.0301\n",
      "Epoch [301/5000], Loss: 0.0234\n",
      "Epoch [401/5000], Loss: 0.0217\n",
      "Epoch [501/5000], Loss: 0.0191\n",
      "Epoch [601/5000], Loss: 0.0189\n",
      "Epoch [701/5000], Loss: 0.0179\n",
      "Epoch [801/5000], Loss: 0.0201\n",
      "Epoch [901/5000], Loss: 0.0171\n",
      "Epoch [1001/5000], Loss: 0.0163\n",
      "Epoch [1101/5000], Loss: 0.0153\n",
      "Epoch [1201/5000], Loss: 0.0148\n",
      "Epoch [1301/5000], Loss: 0.0146\n",
      "Epoch [1401/5000], Loss: 0.0157\n",
      "Epoch [1501/5000], Loss: 0.0160\n",
      "Epoch [1601/5000], Loss: 0.0144\n",
      "Epoch [1701/5000], Loss: 0.0151\n",
      "Epoch [1801/5000], Loss: 0.0137\n",
      "Epoch [1901/5000], Loss: 0.0160\n",
      "Epoch [2001/5000], Loss: 0.0135\n",
      "Epoch [2101/5000], Loss: 0.0145\n",
      "Epoch [2201/5000], Loss: 0.0157\n",
      "Epoch [2301/5000], Loss: 0.0149\n",
      "Epoch [2401/5000], Loss: 0.0135\n",
      "Epoch [2501/5000], Loss: 0.0130\n",
      "Epoch [2601/5000], Loss: 0.0135\n",
      "Epoch [2701/5000], Loss: 0.0132\n",
      "Epoch [2801/5000], Loss: 0.0130\n",
      "Epoch [2901/5000], Loss: 0.0116\n",
      "Epoch [3001/5000], Loss: 0.0106\n",
      "Epoch [3101/5000], Loss: 0.0092\n",
      "Epoch [3201/5000], Loss: 0.0104\n",
      "Epoch [3301/5000], Loss: 0.0096\n",
      "Epoch [3401/5000], Loss: 0.0100\n",
      "Epoch [3501/5000], Loss: 0.0088\n",
      "Epoch [3601/5000], Loss: 0.0114\n",
      "Epoch [3701/5000], Loss: 0.0087\n",
      "Epoch [3801/5000], Loss: 0.0093\n",
      "Epoch [3901/5000], Loss: 0.0096\n",
      "Epoch [4001/5000], Loss: 0.0085\n",
      "Epoch [4101/5000], Loss: 0.0097\n",
      "Epoch [4201/5000], Loss: 0.0082\n",
      "Epoch [4301/5000], Loss: 0.0088\n",
      "Epoch [4401/5000], Loss: 0.0081\n",
      "Epoch [4501/5000], Loss: 0.0084\n",
      "Epoch [4601/5000], Loss: 0.0083\n",
      "Epoch [4701/5000], Loss: 0.0081\n",
      "Epoch [4801/5000], Loss: 0.0080\n",
      "Epoch [4901/5000], Loss: 0.0105\n",
      "Train: 0.007973134 Test: 0.009532825364021634\n",
      "Epoch [1/5000], Loss: 1.0123\n",
      "Epoch [101/5000], Loss: 0.0928\n",
      "Epoch [201/5000], Loss: 0.0528\n",
      "Epoch [301/5000], Loss: 0.0314\n",
      "Epoch [401/5000], Loss: 0.0220\n",
      "Epoch [501/5000], Loss: 0.0212\n",
      "Epoch [601/5000], Loss: 0.0187\n",
      "Epoch [701/5000], Loss: 0.0162\n",
      "Epoch [801/5000], Loss: 0.0154\n",
      "Epoch [901/5000], Loss: 0.0146\n",
      "Epoch [1001/5000], Loss: 0.0144\n",
      "Epoch [1101/5000], Loss: 0.0145\n",
      "Epoch [1201/5000], Loss: 0.0134\n",
      "Epoch [1301/5000], Loss: 0.0133\n",
      "Epoch [1401/5000], Loss: 0.0134\n",
      "Epoch [1501/5000], Loss: 0.0129\n",
      "Epoch [1601/5000], Loss: 0.0131\n",
      "Epoch [1701/5000], Loss: 0.0129\n",
      "Epoch [1801/5000], Loss: 0.0128\n",
      "Epoch [1901/5000], Loss: 0.0126\n",
      "Epoch [2001/5000], Loss: 0.0126\n",
      "Epoch [2101/5000], Loss: 0.0174\n",
      "Epoch [2201/5000], Loss: 0.0124\n",
      "Epoch [2301/5000], Loss: 0.0122\n",
      "Epoch [2401/5000], Loss: 0.0149\n",
      "Epoch [2501/5000], Loss: 0.0123\n",
      "Epoch [2601/5000], Loss: 0.0133\n",
      "Epoch [2701/5000], Loss: 0.0120\n",
      "Epoch [2801/5000], Loss: 0.0121\n",
      "Epoch [2901/5000], Loss: 0.0123\n",
      "Epoch [3001/5000], Loss: 0.0124\n",
      "Epoch [3101/5000], Loss: 0.0118\n",
      "Epoch [3201/5000], Loss: 0.0131\n",
      "Epoch [3301/5000], Loss: 0.0117\n",
      "Epoch [3401/5000], Loss: 0.0123\n",
      "Epoch [3501/5000], Loss: 0.0139\n",
      "Epoch [3601/5000], Loss: 0.0246\n",
      "Epoch [3701/5000], Loss: 0.0136\n",
      "Epoch [3801/5000], Loss: 0.0129\n",
      "Epoch [3901/5000], Loss: 0.0124\n",
      "Epoch [4001/5000], Loss: 0.0128\n",
      "Epoch [4101/5000], Loss: 0.0121\n",
      "Epoch [4201/5000], Loss: 0.0130\n",
      "Epoch [4301/5000], Loss: 0.0123\n",
      "Epoch [4401/5000], Loss: 0.0118\n",
      "Epoch [4501/5000], Loss: 0.0117\n",
      "Epoch [4601/5000], Loss: 0.0129\n",
      "Epoch [4701/5000], Loss: 0.0116\n",
      "Epoch [4801/5000], Loss: 0.0118\n",
      "Epoch [4901/5000], Loss: 0.0171\n",
      "Train: 0.010100166 Test: 0.011780336642705953\n",
      "Epoch [1/5000], Loss: 0.6175\n",
      "Epoch [101/5000], Loss: 0.0353\n",
      "Epoch [201/5000], Loss: 0.0210\n",
      "Epoch [301/5000], Loss: 0.0174\n",
      "Epoch [401/5000], Loss: 0.0132\n",
      "Epoch [501/5000], Loss: 0.0134\n",
      "Epoch [601/5000], Loss: 0.0128\n",
      "Epoch [701/5000], Loss: 0.0095\n",
      "Epoch [801/5000], Loss: 0.0089\n",
      "Epoch [901/5000], Loss: 0.0089\n",
      "Epoch [1001/5000], Loss: 0.0081\n",
      "Epoch [1101/5000], Loss: 0.0076\n",
      "Epoch [1201/5000], Loss: 0.0073\n",
      "Epoch [1301/5000], Loss: 0.0074\n",
      "Epoch [1401/5000], Loss: 0.0070\n",
      "Epoch [1501/5000], Loss: 0.0071\n",
      "Epoch [1601/5000], Loss: 0.0065\n",
      "Epoch [1701/5000], Loss: 0.0064\n",
      "Epoch [1801/5000], Loss: 0.0062\n",
      "Epoch [1901/5000], Loss: 0.0063\n",
      "Epoch [2001/5000], Loss: 0.0058\n",
      "Epoch [2101/5000], Loss: 0.0061\n",
      "Epoch [2201/5000], Loss: 0.0062\n",
      "Epoch [2301/5000], Loss: 0.0063\n",
      "Epoch [2401/5000], Loss: 0.0072\n",
      "Epoch [2501/5000], Loss: 0.0058\n",
      "Epoch [2601/5000], Loss: 0.0058\n",
      "Epoch [2701/5000], Loss: 0.0063\n",
      "Epoch [2801/5000], Loss: 0.0051\n",
      "Epoch [2901/5000], Loss: 0.0074\n",
      "Epoch [3001/5000], Loss: 0.0055\n",
      "Epoch [3101/5000], Loss: 0.0051\n",
      "Epoch [3201/5000], Loss: 0.0054\n",
      "Epoch [3301/5000], Loss: 0.0051\n",
      "Epoch [3401/5000], Loss: 0.0048\n",
      "Epoch [3501/5000], Loss: 0.0049\n",
      "Epoch [3601/5000], Loss: 0.0057\n",
      "Epoch [3701/5000], Loss: 0.0050\n",
      "Epoch [3801/5000], Loss: 0.0052\n",
      "Epoch [3901/5000], Loss: 0.0046\n",
      "Epoch [4001/5000], Loss: 0.0049\n",
      "Epoch [4101/5000], Loss: 0.0047\n",
      "Epoch [4201/5000], Loss: 0.0047\n",
      "Epoch [4301/5000], Loss: 0.0046\n",
      "Epoch [4401/5000], Loss: 0.0043\n",
      "Epoch [4501/5000], Loss: 0.0042\n",
      "Epoch [4601/5000], Loss: 0.0043\n",
      "Epoch [4701/5000], Loss: 0.0041\n",
      "Epoch [4801/5000], Loss: 0.0041\n",
      "Epoch [4901/5000], Loss: 0.0041\n",
      "Train: 0.0046287496 Test: 0.0072421454546465645\n",
      "Epoch [1/5000], Loss: 0.5315\n",
      "Epoch [101/5000], Loss: 0.0307\n",
      "Epoch [201/5000], Loss: 0.0187\n",
      "Epoch [301/5000], Loss: 0.0149\n",
      "Epoch [401/5000], Loss: 0.0145\n",
      "Epoch [501/5000], Loss: 0.0103\n",
      "Epoch [601/5000], Loss: 0.0098\n",
      "Epoch [701/5000], Loss: 0.0089\n",
      "Epoch [801/5000], Loss: 0.0088\n",
      "Epoch [901/5000], Loss: 0.0116\n",
      "Epoch [1001/5000], Loss: 0.0085\n",
      "Epoch [1101/5000], Loss: 0.0093\n",
      "Epoch [1201/5000], Loss: 0.0076\n",
      "Epoch [1301/5000], Loss: 0.0075\n",
      "Epoch [1401/5000], Loss: 0.0074\n",
      "Epoch [1501/5000], Loss: 0.0074\n",
      "Epoch [1601/5000], Loss: 0.0068\n",
      "Epoch [1701/5000], Loss: 0.0065\n",
      "Epoch [1801/5000], Loss: 0.0063\n",
      "Epoch [1901/5000], Loss: 0.0068\n",
      "Epoch [2001/5000], Loss: 0.0071\n",
      "Epoch [2101/5000], Loss: 0.0065\n",
      "Epoch [2201/5000], Loss: 0.0071\n",
      "Epoch [2301/5000], Loss: 0.0059\n",
      "Epoch [2401/5000], Loss: 0.0062\n",
      "Epoch [2501/5000], Loss: 0.0057\n",
      "Epoch [2601/5000], Loss: 0.0060\n",
      "Epoch [2701/5000], Loss: 0.0055\n",
      "Epoch [2801/5000], Loss: 0.0054\n",
      "Epoch [2901/5000], Loss: 0.0055\n",
      "Epoch [3001/5000], Loss: 0.0053\n",
      "Epoch [3101/5000], Loss: 0.0057\n",
      "Epoch [3201/5000], Loss: 0.0052\n",
      "Epoch [3301/5000], Loss: 0.0055\n",
      "Epoch [3401/5000], Loss: 0.0055\n",
      "Epoch [3501/5000], Loss: 0.0054\n",
      "Epoch [3601/5000], Loss: 0.0050\n",
      "Epoch [3701/5000], Loss: 0.0050\n",
      "Epoch [3801/5000], Loss: 0.0050\n",
      "Epoch [3901/5000], Loss: 0.0049\n",
      "Epoch [4001/5000], Loss: 0.0052\n",
      "Epoch [4101/5000], Loss: 0.0050\n",
      "Epoch [4201/5000], Loss: 0.0052\n",
      "Epoch [4301/5000], Loss: 0.0050\n",
      "Epoch [4401/5000], Loss: 0.0049\n",
      "Epoch [4501/5000], Loss: 0.0047\n",
      "Epoch [4601/5000], Loss: 0.0053\n",
      "Epoch [4701/5000], Loss: 0.0049\n",
      "Epoch [4801/5000], Loss: 0.0047\n",
      "Epoch [4901/5000], Loss: 0.0051\n",
      "Train: 0.0058253775 Test: 0.00806930904957955\n",
      "Epoch [1/5000], Loss: 1.2269\n",
      "Epoch [101/5000], Loss: 0.0538\n",
      "Epoch [201/5000], Loss: 0.0250\n",
      "Epoch [301/5000], Loss: 0.0196\n",
      "Epoch [401/5000], Loss: 0.0193\n",
      "Epoch [501/5000], Loss: 0.0175\n",
      "Epoch [601/5000], Loss: 0.0175\n",
      "Epoch [701/5000], Loss: 0.0170\n",
      "Epoch [801/5000], Loss: 0.0182\n",
      "Epoch [901/5000], Loss: 0.0154\n",
      "Epoch [1001/5000], Loss: 0.0135\n",
      "Epoch [1101/5000], Loss: 0.0132\n",
      "Epoch [1201/5000], Loss: 0.0132\n",
      "Epoch [1301/5000], Loss: 0.0132\n",
      "Epoch [1401/5000], Loss: 0.0146\n",
      "Epoch [1501/5000], Loss: 0.0131\n",
      "Epoch [1601/5000], Loss: 0.0157\n",
      "Epoch [1701/5000], Loss: 0.0124\n",
      "Epoch [1801/5000], Loss: 0.0120\n",
      "Epoch [1901/5000], Loss: 0.0118\n",
      "Epoch [2001/5000], Loss: 0.0142\n",
      "Epoch [2101/5000], Loss: 0.0104\n",
      "Epoch [2201/5000], Loss: 0.0105\n",
      "Epoch [2301/5000], Loss: 0.0099\n",
      "Epoch [2401/5000], Loss: 0.0101\n",
      "Epoch [2501/5000], Loss: 0.0105\n",
      "Epoch [2601/5000], Loss: 0.0099\n",
      "Epoch [2701/5000], Loss: 0.0100\n",
      "Epoch [2801/5000], Loss: 0.0101\n",
      "Epoch [2901/5000], Loss: 0.0096\n",
      "Epoch [3001/5000], Loss: 0.0098\n",
      "Epoch [3101/5000], Loss: 0.0090\n",
      "Epoch [3201/5000], Loss: 0.0089\n",
      "Epoch [3301/5000], Loss: 0.0084\n",
      "Epoch [3401/5000], Loss: 0.0086\n",
      "Epoch [3501/5000], Loss: 0.0088\n",
      "Epoch [3601/5000], Loss: 0.0109\n",
      "Epoch [3701/5000], Loss: 0.0085\n",
      "Epoch [3801/5000], Loss: 0.0079\n",
      "Epoch [3901/5000], Loss: 0.0074\n",
      "Epoch [4001/5000], Loss: 0.0081\n",
      "Epoch [4101/5000], Loss: 0.0074\n",
      "Epoch [4201/5000], Loss: 0.0069\n",
      "Epoch [4301/5000], Loss: 0.0077\n",
      "Epoch [4401/5000], Loss: 0.0078\n",
      "Epoch [4501/5000], Loss: 0.0070\n",
      "Epoch [4601/5000], Loss: 0.0085\n",
      "Epoch [4701/5000], Loss: 0.0078\n",
      "Epoch [4801/5000], Loss: 0.0077\n",
      "Epoch [4901/5000], Loss: 0.0087\n",
      "Train: 0.008766 Test: 0.009986963297304292\n",
      "Epoch [1/5000], Loss: 0.8301\n",
      "Epoch [101/5000], Loss: 0.0353\n",
      "Epoch [201/5000], Loss: 0.0233\n",
      "Epoch [301/5000], Loss: 0.0190\n",
      "Epoch [401/5000], Loss: 0.0167\n",
      "Epoch [501/5000], Loss: 0.0133\n",
      "Epoch [601/5000], Loss: 0.0134\n",
      "Epoch [701/5000], Loss: 0.0118\n",
      "Epoch [801/5000], Loss: 0.0107\n",
      "Epoch [901/5000], Loss: 0.0100\n",
      "Epoch [1001/5000], Loss: 0.0113\n",
      "Epoch [1101/5000], Loss: 0.0095\n",
      "Epoch [1201/5000], Loss: 0.0091\n",
      "Epoch [1301/5000], Loss: 0.0091\n",
      "Epoch [1401/5000], Loss: 0.0116\n",
      "Epoch [1501/5000], Loss: 0.0082\n",
      "Epoch [1601/5000], Loss: 0.0081\n",
      "Epoch [1701/5000], Loss: 0.0084\n",
      "Epoch [1801/5000], Loss: 0.0079\n",
      "Epoch [1901/5000], Loss: 0.0080\n",
      "Epoch [2001/5000], Loss: 0.0081\n",
      "Epoch [2101/5000], Loss: 0.0073\n",
      "Epoch [2201/5000], Loss: 0.0078\n",
      "Epoch [2301/5000], Loss: 0.0074\n",
      "Epoch [2401/5000], Loss: 0.0070\n",
      "Epoch [2501/5000], Loss: 0.0079\n",
      "Epoch [2601/5000], Loss: 0.0077\n",
      "Epoch [2701/5000], Loss: 0.0068\n",
      "Epoch [2801/5000], Loss: 0.0067\n",
      "Epoch [2901/5000], Loss: 0.0068\n",
      "Epoch [3001/5000], Loss: 0.0066\n",
      "Epoch [3101/5000], Loss: 0.0067\n",
      "Epoch [3201/5000], Loss: 0.0071\n",
      "Epoch [3301/5000], Loss: 0.0073\n",
      "Epoch [3401/5000], Loss: 0.0064\n",
      "Epoch [3501/5000], Loss: 0.0064\n",
      "Epoch [3601/5000], Loss: 0.0060\n",
      "Epoch [3701/5000], Loss: 0.0059\n",
      "Epoch [3801/5000], Loss: 0.0058\n",
      "Epoch [3901/5000], Loss: 0.0065\n",
      "Epoch [4001/5000], Loss: 0.0060\n",
      "Epoch [4101/5000], Loss: 0.0058\n",
      "Epoch [4201/5000], Loss: 0.0077\n",
      "Epoch [4301/5000], Loss: 0.0057\n",
      "Epoch [4401/5000], Loss: 0.0065\n",
      "Epoch [4501/5000], Loss: 0.0055\n",
      "Epoch [4601/5000], Loss: 0.0055\n",
      "Epoch [4701/5000], Loss: 0.0056\n",
      "Epoch [4801/5000], Loss: 0.0060\n",
      "Epoch [4901/5000], Loss: 0.0055\n",
      "Train: 0.009886156 Test: 0.011523767711087833\n",
      "Epoch [1/5000], Loss: 1.0078\n",
      "Epoch [101/5000], Loss: 0.0648\n",
      "Epoch [201/5000], Loss: 0.0326\n",
      "Epoch [301/5000], Loss: 0.0227\n",
      "Epoch [401/5000], Loss: 0.0205\n",
      "Epoch [501/5000], Loss: 0.0178\n",
      "Epoch [601/5000], Loss: 0.0172\n",
      "Epoch [701/5000], Loss: 0.0163\n",
      "Epoch [801/5000], Loss: 0.0158\n",
      "Epoch [901/5000], Loss: 0.0170\n",
      "Epoch [1001/5000], Loss: 0.0159\n",
      "Epoch [1101/5000], Loss: 0.0136\n",
      "Epoch [1201/5000], Loss: 0.0165\n",
      "Epoch [1301/5000], Loss: 0.0116\n",
      "Epoch [1401/5000], Loss: 0.0122\n",
      "Epoch [1501/5000], Loss: 0.0112\n",
      "Epoch [1601/5000], Loss: 0.0157\n",
      "Epoch [1701/5000], Loss: 0.0115\n",
      "Epoch [1801/5000], Loss: 0.0109\n",
      "Epoch [1901/5000], Loss: 0.0113\n",
      "Epoch [2001/5000], Loss: 0.0114\n",
      "Epoch [2101/5000], Loss: 0.0114\n",
      "Epoch [2201/5000], Loss: 0.0110\n",
      "Epoch [2301/5000], Loss: 0.0107\n",
      "Epoch [2401/5000], Loss: 0.0110\n",
      "Epoch [2501/5000], Loss: 0.0110\n",
      "Epoch [2601/5000], Loss: 0.0104\n",
      "Epoch [2701/5000], Loss: 0.0132\n",
      "Epoch [2801/5000], Loss: 0.0098\n",
      "Epoch [2901/5000], Loss: 0.0102\n",
      "Epoch [3001/5000], Loss: 0.0100\n",
      "Epoch [3101/5000], Loss: 0.0097\n",
      "Epoch [3201/5000], Loss: 0.0112\n",
      "Epoch [3301/5000], Loss: 0.0107\n",
      "Epoch [3401/5000], Loss: 0.0106\n",
      "Epoch [3501/5000], Loss: 0.0096\n",
      "Epoch [3601/5000], Loss: 0.0100\n",
      "Epoch [3701/5000], Loss: 0.0101\n",
      "Epoch [3801/5000], Loss: 0.0098\n",
      "Epoch [3901/5000], Loss: 0.0109\n",
      "Epoch [4001/5000], Loss: 0.0103\n",
      "Epoch [4101/5000], Loss: 0.0089\n",
      "Epoch [4201/5000], Loss: 0.0095\n",
      "Epoch [4301/5000], Loss: 0.0091\n",
      "Epoch [4401/5000], Loss: 0.0103\n",
      "Epoch [4501/5000], Loss: 0.0106\n",
      "Epoch [4601/5000], Loss: 0.0099\n",
      "Epoch [4701/5000], Loss: 0.0092\n",
      "Epoch [4801/5000], Loss: 0.0089\n",
      "Epoch [4901/5000], Loss: 0.0096\n",
      "Train: 0.0099390475 Test: 0.010862468134336133\n",
      "Epoch [1/5000], Loss: 0.9281\n",
      "Epoch [101/5000], Loss: 0.0324\n",
      "Epoch [201/5000], Loss: 0.0214\n",
      "Epoch [301/5000], Loss: 0.0180\n",
      "Epoch [401/5000], Loss: 0.0161\n",
      "Epoch [501/5000], Loss: 0.0148\n",
      "Epoch [601/5000], Loss: 0.0152\n",
      "Epoch [701/5000], Loss: 0.0132\n",
      "Epoch [801/5000], Loss: 0.0131\n",
      "Epoch [901/5000], Loss: 0.0127\n",
      "Epoch [1001/5000], Loss: 0.0136\n",
      "Epoch [1101/5000], Loss: 0.0114\n",
      "Epoch [1201/5000], Loss: 0.0120\n",
      "Epoch [1301/5000], Loss: 0.0110\n",
      "Epoch [1401/5000], Loss: 0.0111\n",
      "Epoch [1501/5000], Loss: 0.0109\n",
      "Epoch [1601/5000], Loss: 0.0108\n",
      "Epoch [1701/5000], Loss: 0.0106\n",
      "Epoch [1801/5000], Loss: 0.0104\n",
      "Epoch [1901/5000], Loss: 0.0118\n",
      "Epoch [2001/5000], Loss: 0.0097\n",
      "Epoch [2101/5000], Loss: 0.0095\n",
      "Epoch [2201/5000], Loss: 0.0096\n",
      "Epoch [2301/5000], Loss: 0.0103\n",
      "Epoch [2401/5000], Loss: 0.0100\n",
      "Epoch [2501/5000], Loss: 0.0094\n",
      "Epoch [2601/5000], Loss: 0.0095\n",
      "Epoch [2701/5000], Loss: 0.0089\n",
      "Epoch [2801/5000], Loss: 0.0090\n",
      "Epoch [2901/5000], Loss: 0.0088\n",
      "Epoch [3001/5000], Loss: 0.0106\n",
      "Epoch [3101/5000], Loss: 0.0084\n",
      "Epoch [3201/5000], Loss: 0.0082\n",
      "Epoch [3301/5000], Loss: 0.0079\n",
      "Epoch [3401/5000], Loss: 0.0059\n",
      "Epoch [3501/5000], Loss: 0.0061\n",
      "Epoch [3601/5000], Loss: 0.0065\n",
      "Epoch [3701/5000], Loss: 0.0061\n",
      "Epoch [3801/5000], Loss: 0.0058\n",
      "Epoch [3901/5000], Loss: 0.0055\n",
      "Epoch [4001/5000], Loss: 0.0053\n",
      "Epoch [4101/5000], Loss: 0.0059\n",
      "Epoch [4201/5000], Loss: 0.0055\n",
      "Epoch [4301/5000], Loss: 0.0050\n",
      "Epoch [4401/5000], Loss: 0.0048\n",
      "Epoch [4501/5000], Loss: 0.0065\n",
      "Epoch [4601/5000], Loss: 0.0053\n",
      "Epoch [4701/5000], Loss: 0.0050\n",
      "Epoch [4801/5000], Loss: 0.0047\n",
      "Epoch [4901/5000], Loss: 0.0046\n",
      "Train: 0.0053508845 Test: 0.007502418020885747\n",
      "Epoch [1/5000], Loss: 1.4284\n",
      "Epoch [101/5000], Loss: 0.1775\n",
      "Epoch [201/5000], Loss: 0.0430\n",
      "Epoch [301/5000], Loss: 0.0292\n",
      "Epoch [401/5000], Loss: 0.0219\n",
      "Epoch [501/5000], Loss: 0.0271\n",
      "Epoch [601/5000], Loss: 0.0178\n",
      "Epoch [701/5000], Loss: 0.0191\n",
      "Epoch [801/5000], Loss: 0.0161\n",
      "Epoch [901/5000], Loss: 0.0156\n",
      "Epoch [1001/5000], Loss: 0.0208\n",
      "Epoch [1101/5000], Loss: 0.0162\n",
      "Epoch [1201/5000], Loss: 0.0148\n",
      "Epoch [1301/5000], Loss: 0.0195\n",
      "Epoch [1401/5000], Loss: 0.0175\n",
      "Epoch [1501/5000], Loss: 0.0174\n",
      "Epoch [1601/5000], Loss: 0.0178\n",
      "Epoch [1701/5000], Loss: 0.0187\n",
      "Epoch [1801/5000], Loss: 0.0175\n",
      "Epoch [1901/5000], Loss: 0.0176\n",
      "Epoch [2001/5000], Loss: 0.0167\n",
      "Epoch [2101/5000], Loss: 0.0183\n",
      "Epoch [2201/5000], Loss: 0.0182\n",
      "Epoch [2301/5000], Loss: 0.0169\n",
      "Epoch [2401/5000], Loss: 0.0169\n",
      "Epoch [2501/5000], Loss: 0.0177\n",
      "Epoch [2601/5000], Loss: 0.0176\n",
      "Epoch [2701/5000], Loss: 0.0169\n",
      "Epoch [2801/5000], Loss: 0.0167\n",
      "Epoch [2901/5000], Loss: 0.0172\n",
      "Epoch [3001/5000], Loss: 0.0167\n",
      "Epoch [3101/5000], Loss: 0.0179\n",
      "Epoch [3201/5000], Loss: 0.0165\n",
      "Epoch [3301/5000], Loss: 0.0166\n",
      "Epoch [3401/5000], Loss: 0.0170\n",
      "Epoch [3501/5000], Loss: 0.0172\n",
      "Epoch [3601/5000], Loss: 0.0170\n",
      "Epoch [3701/5000], Loss: 0.0163\n",
      "Epoch [3801/5000], Loss: 0.0163\n",
      "Epoch [3901/5000], Loss: 0.0207\n",
      "Epoch [4001/5000], Loss: 0.0161\n",
      "Epoch [4101/5000], Loss: 0.0161\n",
      "Epoch [4201/5000], Loss: 0.0177\n",
      "Epoch [4301/5000], Loss: 0.0167\n",
      "Epoch [4401/5000], Loss: 0.0170\n",
      "Epoch [4501/5000], Loss: 0.0161\n",
      "Epoch [4601/5000], Loss: 0.0178\n",
      "Epoch [4701/5000], Loss: 0.0163\n",
      "Epoch [4801/5000], Loss: 0.0161\n",
      "Epoch [4901/5000], Loss: 0.0162\n",
      "Train: 0.016565604 Test: 0.017805067751214678\n"
     ]
    }
   ],
   "source": [
    "T=[]\n",
    "for i in range(1,50,2):\n",
    "    t=[]\n",
    "    for j in range(5):\n",
    "        t.append(runFNN(t=i))\n",
    "    T.append(t)\n",
    "T=np.array(T)\n",
    "np.save(\"/its/home/drs25/GonkRobot/Data/experimentdata/FNN_T\",T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 231.0197\n",
      "Validation Loss: 0.2090\n",
      "Epoch [2/300], Loss: 169.6963\n",
      "Epoch [3/300], Loss: 151.9532\n",
      "Epoch [4/300], Loss: 120.6021\n",
      "Epoch [5/300], Loss: 97.4050\n",
      "Epoch [6/300], Loss: 87.5283\n",
      "Epoch [7/300], Loss: 79.6416\n",
      "Epoch [8/300], Loss: 72.4573\n",
      "Epoch [9/300], Loss: 66.6332\n",
      "Epoch [10/300], Loss: 62.4564\n",
      "Epoch [11/300], Loss: 59.5919\n",
      "Epoch [12/300], Loss: 57.5079\n",
      "Epoch [13/300], Loss: 55.8361\n",
      "Epoch [14/300], Loss: 54.3916\n",
      "Epoch [15/300], Loss: 53.0873\n",
      "Epoch [16/300], Loss: 51.8780\n",
      "Epoch [17/300], Loss: 50.7374\n",
      "Epoch [18/300], Loss: 49.6491\n",
      "Epoch [19/300], Loss: 48.6046\n",
      "Epoch [20/300], Loss: 47.6019\n",
      "Epoch [21/300], Loss: 46.6433\n",
      "Epoch [22/300], Loss: 45.7321\n",
      "Epoch [23/300], Loss: 44.8708\n",
      "Epoch [24/300], Loss: 44.0602\n",
      "Epoch [25/300], Loss: 43.2997\n",
      "Epoch [26/300], Loss: 42.5882\n",
      "Epoch [27/300], Loss: 41.9245\n",
      "Epoch [28/300], Loss: 41.3072\n",
      "Epoch [29/300], Loss: 40.7347\n",
      "Epoch [30/300], Loss: 40.2051\n",
      "Epoch [31/300], Loss: 39.7162\n",
      "Epoch [32/300], Loss: 39.2655\n",
      "Epoch [33/300], Loss: 38.8502\n",
      "Epoch [34/300], Loss: 38.4672\n",
      "Epoch [35/300], Loss: 38.1136\n",
      "Epoch [36/300], Loss: 37.7860\n",
      "Epoch [37/300], Loss: 37.4820\n",
      "Epoch [38/300], Loss: 37.1990\n",
      "Epoch [39/300], Loss: 36.9347\n",
      "Epoch [40/300], Loss: 36.6876\n",
      "Epoch [41/300], Loss: 36.4562\n",
      "Epoch [42/300], Loss: 36.2394\n",
      "Epoch [43/300], Loss: 36.0359\n",
      "Epoch [44/300], Loss: 35.8448\n",
      "Epoch [45/300], Loss: 35.6651\n",
      "Epoch [46/300], Loss: 35.4961\n",
      "Epoch [47/300], Loss: 35.3369\n",
      "Epoch [48/300], Loss: 35.1865\n",
      "Epoch [49/300], Loss: 35.0443\n",
      "Epoch [50/300], Loss: 34.9095\n",
      "Epoch [51/300], Loss: 34.7813\n",
      "Epoch [52/300], Loss: 34.6594\n",
      "Epoch [53/300], Loss: 34.5432\n",
      "Epoch [54/300], Loss: 34.4322\n",
      "Epoch [55/300], Loss: 34.3254\n",
      "Epoch [56/300], Loss: 34.2231\n",
      "Epoch [57/300], Loss: 34.1245\n",
      "Epoch [58/300], Loss: 34.0293\n",
      "Epoch [59/300], Loss: 33.9375\n",
      "Epoch [60/300], Loss: 33.8483\n",
      "Epoch [61/300], Loss: 33.7618\n",
      "Epoch [62/300], Loss: 33.6777\n",
      "Epoch [63/300], Loss: 33.5957\n",
      "Epoch [64/300], Loss: 33.5157\n",
      "Epoch [65/300], Loss: 33.4373\n",
      "Epoch [66/300], Loss: 33.3606\n",
      "Epoch [67/300], Loss: 33.2854\n",
      "Epoch [68/300], Loss: 33.2116\n",
      "Epoch [69/300], Loss: 33.1389\n",
      "Epoch [70/300], Loss: 33.0677\n",
      "Epoch [71/300], Loss: 32.9976\n",
      "Epoch [72/300], Loss: 32.9283\n",
      "Epoch [73/300], Loss: 32.8598\n",
      "Epoch [74/300], Loss: 32.7924\n",
      "Epoch [75/300], Loss: 32.7256\n",
      "Epoch [76/300], Loss: 32.6595\n",
      "Epoch [77/300], Loss: 32.5942\n",
      "Epoch [78/300], Loss: 32.5294\n",
      "Epoch [79/300], Loss: 32.4654\n",
      "Epoch [80/300], Loss: 32.4018\n",
      "Epoch [81/300], Loss: 32.3388\n",
      "Epoch [82/300], Loss: 32.2762\n",
      "Epoch [83/300], Loss: 32.2140\n",
      "Epoch [84/300], Loss: 32.1523\n",
      "Epoch [85/300], Loss: 32.0910\n",
      "Epoch [86/300], Loss: 32.0302\n",
      "Epoch [87/300], Loss: 31.9696\n",
      "Epoch [88/300], Loss: 31.9094\n",
      "Epoch [89/300], Loss: 31.8496\n",
      "Epoch [90/300], Loss: 31.7901\n",
      "Epoch [91/300], Loss: 31.7308\n",
      "Epoch [92/300], Loss: 31.6721\n",
      "Epoch [93/300], Loss: 31.6136\n",
      "Epoch [94/300], Loss: 31.5551\n",
      "Epoch [95/300], Loss: 31.4974\n",
      "Epoch [96/300], Loss: 31.4400\n",
      "Epoch [97/300], Loss: 31.3828\n",
      "Epoch [98/300], Loss: 31.3258\n",
      "Epoch [99/300], Loss: 31.2695\n",
      "Epoch [100/300], Loss: 31.2135\n",
      "Epoch [101/300], Loss: 31.1576\n",
      "Validation Loss: 0.0352\n",
      "Epoch [102/300], Loss: 31.1023\n",
      "Epoch [103/300], Loss: 31.0475\n",
      "Epoch [104/300], Loss: 30.9931\n",
      "Epoch [105/300], Loss: 30.9391\n",
      "Epoch [106/300], Loss: 30.8856\n",
      "Epoch [107/300], Loss: 30.8325\n",
      "Epoch [108/300], Loss: 30.7801\n",
      "Epoch [109/300], Loss: 30.7282\n",
      "Epoch [110/300], Loss: 30.6768\n",
      "Epoch [111/300], Loss: 30.6260\n",
      "Epoch [112/300], Loss: 30.5757\n",
      "Epoch [113/300], Loss: 30.5261\n",
      "Epoch [114/300], Loss: 30.4771\n",
      "Epoch [115/300], Loss: 30.4285\n",
      "Epoch [116/300], Loss: 30.3809\n",
      "Epoch [117/300], Loss: 30.3337\n",
      "Epoch [118/300], Loss: 30.2874\n",
      "Epoch [119/300], Loss: 30.2415\n",
      "Epoch [120/300], Loss: 30.1965\n",
      "Epoch [121/300], Loss: 30.1521\n",
      "Epoch [122/300], Loss: 30.1083\n",
      "Epoch [123/300], Loss: 30.0651\n",
      "Epoch [124/300], Loss: 30.0230\n",
      "Epoch [125/300], Loss: 29.9811\n",
      "Epoch [126/300], Loss: 29.9403\n",
      "Epoch [127/300], Loss: 29.8998\n",
      "Epoch [128/300], Loss: 29.8601\n",
      "Epoch [129/300], Loss: 29.8211\n",
      "Epoch [130/300], Loss: 29.7829\n",
      "Epoch [131/300], Loss: 29.7450\n",
      "Epoch [132/300], Loss: 29.7078\n",
      "Epoch [133/300], Loss: 29.6711\n",
      "Epoch [134/300], Loss: 29.6353\n",
      "Epoch [135/300], Loss: 29.5997\n",
      "Epoch [136/300], Loss: 29.5646\n",
      "Epoch [137/300], Loss: 29.5306\n",
      "Epoch [138/300], Loss: 29.4965\n",
      "Epoch [139/300], Loss: 29.4632\n",
      "Epoch [140/300], Loss: 29.4302\n",
      "Epoch [141/300], Loss: 29.3980\n",
      "Epoch [142/300], Loss: 29.3659\n",
      "Epoch [143/300], Loss: 29.3347\n",
      "Epoch [144/300], Loss: 29.3034\n",
      "Epoch [145/300], Loss: 29.2729\n",
      "Epoch [146/300], Loss: 29.2424\n",
      "Epoch [147/300], Loss: 29.2124\n",
      "Epoch [148/300], Loss: 29.1827\n",
      "Epoch [149/300], Loss: 29.1536\n",
      "Epoch [150/300], Loss: 29.1247\n",
      "Epoch [151/300], Loss: 29.0959\n",
      "Epoch [152/300], Loss: 29.0677\n",
      "Epoch [153/300], Loss: 29.0398\n",
      "Epoch [154/300], Loss: 29.0117\n",
      "Epoch [155/300], Loss: 28.9844\n",
      "Epoch [156/300], Loss: 28.9574\n",
      "Epoch [157/300], Loss: 28.9302\n",
      "Epoch [158/300], Loss: 28.9035\n",
      "Epoch [159/300], Loss: 28.8772\n",
      "Epoch [160/300], Loss: 28.8509\n",
      "Epoch [161/300], Loss: 28.8246\n",
      "Epoch [162/300], Loss: 28.7990\n",
      "Epoch [163/300], Loss: 28.7733\n",
      "Epoch [164/300], Loss: 28.7479\n",
      "Epoch [165/300], Loss: 28.7226\n",
      "Epoch [166/300], Loss: 28.6977\n",
      "Epoch [167/300], Loss: 28.6726\n",
      "Epoch [168/300], Loss: 28.6481\n",
      "Epoch [169/300], Loss: 28.6233\n",
      "Epoch [170/300], Loss: 28.5992\n",
      "Epoch [171/300], Loss: 28.5751\n",
      "Epoch [172/300], Loss: 28.5511\n",
      "Epoch [173/300], Loss: 28.5271\n",
      "Epoch [174/300], Loss: 28.5034\n",
      "Epoch [175/300], Loss: 28.4800\n",
      "Epoch [176/300], Loss: 28.4562\n",
      "Epoch [177/300], Loss: 28.4330\n",
      "Epoch [178/300], Loss: 28.4100\n",
      "Epoch [179/300], Loss: 28.3871\n",
      "Epoch [180/300], Loss: 28.3639\n",
      "Epoch [181/300], Loss: 28.3413\n",
      "Epoch [182/300], Loss: 28.3187\n",
      "Epoch [183/300], Loss: 28.2962\n",
      "Epoch [184/300], Loss: 28.2737\n",
      "Epoch [185/300], Loss: 28.2513\n",
      "Epoch [186/300], Loss: 28.2291\n",
      "Epoch [187/300], Loss: 28.2072\n",
      "Epoch [188/300], Loss: 28.1852\n",
      "Epoch [189/300], Loss: 28.1634\n",
      "Epoch [190/300], Loss: 28.1415\n",
      "Epoch [191/300], Loss: 28.1199\n",
      "Epoch [192/300], Loss: 28.0983\n",
      "Epoch [193/300], Loss: 28.0772\n",
      "Epoch [194/300], Loss: 28.0557\n",
      "Epoch [195/300], Loss: 28.0343\n",
      "Epoch [196/300], Loss: 28.0131\n",
      "Epoch [197/300], Loss: 27.9920\n",
      "Epoch [198/300], Loss: 27.9708\n",
      "Epoch [199/300], Loss: 27.9499\n",
      "Epoch [200/300], Loss: 27.9293\n",
      "Epoch [201/300], Loss: 27.9083\n",
      "Validation Loss: 0.0314\n",
      "Epoch [202/300], Loss: 27.8879\n",
      "Epoch [203/300], Loss: 27.8670\n",
      "Epoch [204/300], Loss: 27.8465\n",
      "Epoch [205/300], Loss: 27.8260\n",
      "Epoch [206/300], Loss: 27.8055\n",
      "Epoch [207/300], Loss: 27.7851\n",
      "Epoch [208/300], Loss: 27.7648\n",
      "Epoch [209/300], Loss: 27.7450\n",
      "Epoch [210/300], Loss: 27.7246\n",
      "Epoch [211/300], Loss: 27.7044\n",
      "Epoch [212/300], Loss: 27.6845\n",
      "Epoch [213/300], Loss: 27.6645\n",
      "Epoch [214/300], Loss: 27.6442\n",
      "Epoch [215/300], Loss: 27.6245\n",
      "Epoch [216/300], Loss: 27.6048\n",
      "Epoch [217/300], Loss: 27.5851\n",
      "Epoch [218/300], Loss: 27.5657\n",
      "Epoch [219/300], Loss: 27.5457\n",
      "Epoch [220/300], Loss: 27.5259\n",
      "Epoch [221/300], Loss: 27.5065\n",
      "Epoch [222/300], Loss: 27.4868\n",
      "Epoch [223/300], Loss: 27.4675\n",
      "Epoch [224/300], Loss: 27.4480\n",
      "Epoch [225/300], Loss: 27.4286\n",
      "Epoch [226/300], Loss: 27.4093\n",
      "Epoch [227/300], Loss: 27.3902\n",
      "Epoch [228/300], Loss: 27.3710\n",
      "Epoch [229/300], Loss: 27.3517\n",
      "Epoch [230/300], Loss: 27.3327\n",
      "Epoch [231/300], Loss: 27.3133\n",
      "Epoch [232/300], Loss: 27.2943\n",
      "Epoch [233/300], Loss: 27.2752\n",
      "Epoch [234/300], Loss: 27.2563\n",
      "Epoch [235/300], Loss: 27.2371\n",
      "Epoch [236/300], Loss: 27.2183\n",
      "Epoch [237/300], Loss: 27.1995\n",
      "Epoch [238/300], Loss: 27.1808\n",
      "Epoch [239/300], Loss: 27.1623\n",
      "Epoch [240/300], Loss: 27.1431\n",
      "Epoch [241/300], Loss: 27.1247\n",
      "Epoch [242/300], Loss: 27.1060\n",
      "Epoch [243/300], Loss: 27.0872\n",
      "Epoch [244/300], Loss: 27.0687\n",
      "Epoch [245/300], Loss: 27.0501\n",
      "Epoch [246/300], Loss: 27.0317\n",
      "Epoch [247/300], Loss: 27.0132\n",
      "Epoch [248/300], Loss: 26.9947\n",
      "Epoch [249/300], Loss: 26.9762\n",
      "Epoch [250/300], Loss: 26.9583\n",
      "Epoch [251/300], Loss: 26.9400\n",
      "Epoch [252/300], Loss: 26.9216\n",
      "Epoch [253/300], Loss: 26.9034\n",
      "Epoch [254/300], Loss: 26.8850\n",
      "Epoch [255/300], Loss: 26.8672\n",
      "Epoch [256/300], Loss: 26.8489\n",
      "Epoch [257/300], Loss: 26.8307\n",
      "Epoch [258/300], Loss: 26.8129\n",
      "Epoch [259/300], Loss: 26.7950\n",
      "Epoch [260/300], Loss: 26.7770\n",
      "Epoch [261/300], Loss: 26.7593\n",
      "Epoch [262/300], Loss: 26.7415\n",
      "Epoch [263/300], Loss: 26.7235\n",
      "Epoch [264/300], Loss: 26.7057\n",
      "Epoch [265/300], Loss: 26.6881\n",
      "Epoch [266/300], Loss: 26.6704\n",
      "Epoch [267/300], Loss: 26.6528\n",
      "Epoch [268/300], Loss: 26.6351\n",
      "Epoch [269/300], Loss: 26.6177\n",
      "Epoch [270/300], Loss: 26.6001\n",
      "Epoch [271/300], Loss: 26.5824\n",
      "Epoch [272/300], Loss: 26.5652\n",
      "Epoch [273/300], Loss: 26.5479\n",
      "Epoch [274/300], Loss: 26.5306\n",
      "Epoch [275/300], Loss: 26.5131\n",
      "Epoch [276/300], Loss: 26.4960\n",
      "Epoch [277/300], Loss: 26.4787\n",
      "Epoch [278/300], Loss: 26.4616\n",
      "Epoch [279/300], Loss: 26.4444\n",
      "Epoch [280/300], Loss: 26.4275\n",
      "Epoch [281/300], Loss: 26.4106\n",
      "Epoch [282/300], Loss: 26.3932\n",
      "Epoch [283/300], Loss: 26.3766\n",
      "Epoch [284/300], Loss: 26.3596\n",
      "Epoch [285/300], Loss: 26.3427\n",
      "Epoch [286/300], Loss: 26.3260\n",
      "Epoch [287/300], Loss: 26.3089\n",
      "Epoch [288/300], Loss: 26.2924\n",
      "Epoch [289/300], Loss: 26.2757\n",
      "Epoch [290/300], Loss: 26.2590\n",
      "Epoch [291/300], Loss: 26.2425\n",
      "Epoch [292/300], Loss: 26.2258\n",
      "Epoch [293/300], Loss: 26.2094\n",
      "Epoch [294/300], Loss: 26.1928\n",
      "Epoch [295/300], Loss: 26.1766\n",
      "Epoch [296/300], Loss: 26.1605\n",
      "Epoch [297/300], Loss: 26.1438\n",
      "Epoch [298/300], Loss: 26.1275\n",
      "Epoch [299/300], Loss: 26.1113\n",
      "Epoch [300/300], Loss: 26.0951\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.031136015 Test: 0.02936794\n",
      "Epoch [1/300], Loss: 230.7953\n",
      "Validation Loss: 0.2097\n",
      "Epoch [2/300], Loss: 170.6747\n",
      "Epoch [3/300], Loss: 154.3814\n",
      "Epoch [4/300], Loss: 124.0747\n",
      "Epoch [5/300], Loss: 98.8966\n",
      "Epoch [6/300], Loss: 88.2765\n",
      "Epoch [7/300], Loss: 80.3242\n",
      "Epoch [8/300], Loss: 73.0467\n",
      "Epoch [9/300], Loss: 67.0991\n",
      "Epoch [10/300], Loss: 62.8214\n",
      "Epoch [11/300], Loss: 59.8966\n",
      "Epoch [12/300], Loss: 57.7771\n",
      "Epoch [13/300], Loss: 56.0780\n",
      "Epoch [14/300], Loss: 54.6086\n",
      "Epoch [15/300], Loss: 53.2824\n",
      "Epoch [16/300], Loss: 52.0573\n",
      "Epoch [17/300], Loss: 50.9103\n",
      "Epoch [18/300], Loss: 49.8274\n",
      "Epoch [19/300], Loss: 48.7997\n",
      "Epoch [20/300], Loss: 47.8209\n",
      "Epoch [21/300], Loss: 46.8855\n",
      "Epoch [22/300], Loss: 45.9896\n",
      "Epoch [23/300], Loss: 45.1303\n",
      "Epoch [24/300], Loss: 44.3068\n",
      "Epoch [25/300], Loss: 43.5199\n",
      "Epoch [26/300], Loss: 42.7712\n",
      "Epoch [27/300], Loss: 42.0619\n",
      "Epoch [28/300], Loss: 41.3930\n",
      "Epoch [29/300], Loss: 40.7646\n",
      "Epoch [30/300], Loss: 40.1765\n",
      "Epoch [31/300], Loss: 39.6282\n",
      "Epoch [32/300], Loss: 39.1189\n",
      "Epoch [33/300], Loss: 38.6477\n",
      "Epoch [34/300], Loss: 38.2134\n",
      "Epoch [35/300], Loss: 37.8141\n",
      "Epoch [36/300], Loss: 37.4485\n",
      "Epoch [37/300], Loss: 37.1141\n",
      "Epoch [38/300], Loss: 36.8089\n",
      "Epoch [39/300], Loss: 36.5303\n",
      "Epoch [40/300], Loss: 36.2758\n",
      "Epoch [41/300], Loss: 36.0429\n",
      "Epoch [42/300], Loss: 35.8292\n",
      "Epoch [43/300], Loss: 35.6324\n",
      "Epoch [44/300], Loss: 35.4505\n",
      "Epoch [45/300], Loss: 35.2818\n",
      "Epoch [46/300], Loss: 35.1244\n",
      "Epoch [47/300], Loss: 34.9771\n",
      "Epoch [48/300], Loss: 34.8386\n",
      "Epoch [49/300], Loss: 34.7081\n",
      "Epoch [50/300], Loss: 34.5846\n",
      "Epoch [51/300], Loss: 34.4674\n",
      "Epoch [52/300], Loss: 34.3558\n",
      "Epoch [53/300], Loss: 34.2496\n",
      "Epoch [54/300], Loss: 34.1481\n",
      "Epoch [55/300], Loss: 34.0509\n",
      "Epoch [56/300], Loss: 33.9577\n",
      "Epoch [57/300], Loss: 33.8682\n",
      "Epoch [58/300], Loss: 33.7818\n",
      "Epoch [59/300], Loss: 33.6986\n",
      "Epoch [60/300], Loss: 33.6181\n",
      "Epoch [61/300], Loss: 33.5401\n",
      "Epoch [62/300], Loss: 33.4646\n",
      "Epoch [63/300], Loss: 33.3910\n",
      "Epoch [64/300], Loss: 33.3195\n",
      "Epoch [65/300], Loss: 33.2497\n",
      "Epoch [66/300], Loss: 33.1813\n",
      "Epoch [67/300], Loss: 33.1143\n",
      "Epoch [68/300], Loss: 33.0486\n",
      "Epoch [69/300], Loss: 32.9839\n",
      "Epoch [70/300], Loss: 32.9205\n",
      "Epoch [71/300], Loss: 32.8579\n",
      "Epoch [72/300], Loss: 32.7959\n",
      "Epoch [73/300], Loss: 32.7346\n",
      "Epoch [74/300], Loss: 32.6741\n",
      "Epoch [75/300], Loss: 32.6141\n",
      "Epoch [76/300], Loss: 32.5543\n",
      "Epoch [77/300], Loss: 32.4951\n",
      "Epoch [78/300], Loss: 32.4362\n",
      "Epoch [79/300], Loss: 32.3775\n",
      "Epoch [80/300], Loss: 32.3192\n",
      "Epoch [81/300], Loss: 32.2609\n",
      "Epoch [82/300], Loss: 32.2030\n",
      "Epoch [83/300], Loss: 32.1452\n",
      "Epoch [84/300], Loss: 32.0874\n",
      "Epoch [85/300], Loss: 32.0298\n",
      "Epoch [86/300], Loss: 31.9721\n",
      "Epoch [87/300], Loss: 31.9145\n",
      "Epoch [88/300], Loss: 31.8572\n",
      "Epoch [89/300], Loss: 31.7995\n",
      "Epoch [90/300], Loss: 31.7423\n",
      "Epoch [91/300], Loss: 31.6847\n",
      "Epoch [92/300], Loss: 31.6278\n",
      "Epoch [93/300], Loss: 31.5704\n",
      "Epoch [94/300], Loss: 31.5133\n",
      "Epoch [95/300], Loss: 31.4564\n",
      "Epoch [96/300], Loss: 31.3996\n",
      "Epoch [97/300], Loss: 31.3429\n",
      "Epoch [98/300], Loss: 31.2864\n",
      "Epoch [99/300], Loss: 31.2299\n",
      "Epoch [100/300], Loss: 31.1739\n",
      "Epoch [101/300], Loss: 31.1179\n",
      "Validation Loss: 0.0352\n",
      "Epoch [102/300], Loss: 31.0622\n",
      "Epoch [103/300], Loss: 31.0072\n",
      "Epoch [104/300], Loss: 30.9520\n",
      "Epoch [105/300], Loss: 30.8974\n",
      "Epoch [106/300], Loss: 30.8433\n",
      "Epoch [107/300], Loss: 30.7895\n",
      "Epoch [108/300], Loss: 30.7362\n",
      "Epoch [109/300], Loss: 30.6836\n",
      "Epoch [110/300], Loss: 30.6312\n",
      "Epoch [111/300], Loss: 30.5795\n",
      "Epoch [112/300], Loss: 30.5285\n",
      "Epoch [113/300], Loss: 30.4778\n",
      "Epoch [114/300], Loss: 30.4280\n",
      "Epoch [115/300], Loss: 30.3787\n",
      "Epoch [116/300], Loss: 30.3300\n",
      "Epoch [117/300], Loss: 30.2819\n",
      "Epoch [118/300], Loss: 30.2346\n",
      "Epoch [119/300], Loss: 30.1880\n",
      "Epoch [120/300], Loss: 30.1420\n",
      "Epoch [121/300], Loss: 30.0968\n",
      "Epoch [122/300], Loss: 30.0524\n",
      "Epoch [123/300], Loss: 30.0084\n",
      "Epoch [124/300], Loss: 29.9652\n",
      "Epoch [125/300], Loss: 29.9231\n",
      "Epoch [126/300], Loss: 29.8811\n",
      "Epoch [127/300], Loss: 29.8400\n",
      "Epoch [128/300], Loss: 29.7996\n",
      "Epoch [129/300], Loss: 29.7599\n",
      "Epoch [130/300], Loss: 29.7208\n",
      "Epoch [131/300], Loss: 29.6824\n",
      "Epoch [132/300], Loss: 29.6445\n",
      "Epoch [133/300], Loss: 29.6074\n",
      "Epoch [134/300], Loss: 29.5706\n",
      "Epoch [135/300], Loss: 29.5348\n",
      "Epoch [136/300], Loss: 29.4991\n",
      "Epoch [137/300], Loss: 29.4642\n",
      "Epoch [138/300], Loss: 29.4296\n",
      "Epoch [139/300], Loss: 29.3956\n",
      "Epoch [140/300], Loss: 29.3623\n",
      "Epoch [141/300], Loss: 29.3292\n",
      "Epoch [142/300], Loss: 29.2968\n",
      "Epoch [143/300], Loss: 29.2644\n",
      "Epoch [144/300], Loss: 29.2327\n",
      "Epoch [145/300], Loss: 29.2015\n",
      "Epoch [146/300], Loss: 29.1706\n",
      "Epoch [147/300], Loss: 29.1398\n",
      "Epoch [148/300], Loss: 29.1098\n",
      "Epoch [149/300], Loss: 29.0798\n",
      "Epoch [150/300], Loss: 29.0502\n",
      "Epoch [151/300], Loss: 29.0209\n",
      "Epoch [152/300], Loss: 28.9919\n",
      "Epoch [153/300], Loss: 28.9634\n",
      "Epoch [154/300], Loss: 28.9348\n",
      "Epoch [155/300], Loss: 28.9068\n",
      "Epoch [156/300], Loss: 28.8789\n",
      "Epoch [157/300], Loss: 28.8510\n",
      "Epoch [158/300], Loss: 28.8236\n",
      "Epoch [159/300], Loss: 28.7965\n",
      "Epoch [160/300], Loss: 28.7695\n",
      "Epoch [161/300], Loss: 28.7425\n",
      "Epoch [162/300], Loss: 28.7161\n",
      "Epoch [163/300], Loss: 28.6896\n",
      "Epoch [164/300], Loss: 28.6634\n",
      "Epoch [165/300], Loss: 28.6371\n",
      "Epoch [166/300], Loss: 28.6113\n",
      "Epoch [167/300], Loss: 28.5853\n",
      "Epoch [168/300], Loss: 28.5598\n",
      "Epoch [169/300], Loss: 28.5345\n",
      "Epoch [170/300], Loss: 28.5089\n",
      "Epoch [171/300], Loss: 28.4838\n",
      "Epoch [172/300], Loss: 28.4587\n",
      "Epoch [173/300], Loss: 28.4339\n",
      "Epoch [174/300], Loss: 28.4092\n",
      "Epoch [175/300], Loss: 28.3841\n",
      "Epoch [176/300], Loss: 28.3597\n",
      "Epoch [177/300], Loss: 28.3350\n",
      "Epoch [178/300], Loss: 28.3105\n",
      "Epoch [179/300], Loss: 28.2862\n",
      "Epoch [180/300], Loss: 28.2621\n",
      "Epoch [181/300], Loss: 28.2379\n",
      "Epoch [182/300], Loss: 28.2141\n",
      "Epoch [183/300], Loss: 28.1900\n",
      "Epoch [184/300], Loss: 28.1660\n",
      "Epoch [185/300], Loss: 28.1424\n",
      "Epoch [186/300], Loss: 28.1186\n",
      "Epoch [187/300], Loss: 28.0950\n",
      "Epoch [188/300], Loss: 28.0714\n",
      "Epoch [189/300], Loss: 28.0480\n",
      "Epoch [190/300], Loss: 28.0247\n",
      "Epoch [191/300], Loss: 28.0013\n",
      "Epoch [192/300], Loss: 27.9782\n",
      "Epoch [193/300], Loss: 27.9549\n",
      "Epoch [194/300], Loss: 27.9316\n",
      "Epoch [195/300], Loss: 27.9085\n",
      "Epoch [196/300], Loss: 27.8855\n",
      "Epoch [197/300], Loss: 27.8626\n",
      "Epoch [198/300], Loss: 27.8398\n",
      "Epoch [199/300], Loss: 27.8168\n",
      "Epoch [200/300], Loss: 27.7941\n",
      "Epoch [201/300], Loss: 27.7715\n",
      "Validation Loss: 0.0313\n",
      "Epoch [202/300], Loss: 27.7487\n",
      "Epoch [203/300], Loss: 27.7258\n",
      "Epoch [204/300], Loss: 27.7037\n",
      "Epoch [205/300], Loss: 27.6809\n",
      "Epoch [206/300], Loss: 27.6583\n",
      "Epoch [207/300], Loss: 27.6361\n",
      "Epoch [208/300], Loss: 27.6134\n",
      "Epoch [209/300], Loss: 27.5913\n",
      "Epoch [210/300], Loss: 27.5690\n",
      "Epoch [211/300], Loss: 27.5466\n",
      "Epoch [212/300], Loss: 27.5243\n",
      "Epoch [213/300], Loss: 27.5024\n",
      "Epoch [214/300], Loss: 27.4800\n",
      "Epoch [215/300], Loss: 27.4583\n",
      "Epoch [216/300], Loss: 27.4359\n",
      "Epoch [217/300], Loss: 27.4143\n",
      "Epoch [218/300], Loss: 27.3921\n",
      "Epoch [219/300], Loss: 27.3700\n",
      "Epoch [220/300], Loss: 27.3482\n",
      "Epoch [221/300], Loss: 27.3263\n",
      "Epoch [222/300], Loss: 27.3045\n",
      "Epoch [223/300], Loss: 27.2828\n",
      "Epoch [224/300], Loss: 27.2613\n",
      "Epoch [225/300], Loss: 27.2393\n",
      "Epoch [226/300], Loss: 27.2175\n",
      "Epoch [227/300], Loss: 27.1962\n",
      "Epoch [228/300], Loss: 27.1746\n",
      "Epoch [229/300], Loss: 27.1530\n",
      "Epoch [230/300], Loss: 27.1315\n",
      "Epoch [231/300], Loss: 27.1100\n",
      "Epoch [232/300], Loss: 27.0883\n",
      "Epoch [233/300], Loss: 27.0669\n",
      "Epoch [234/300], Loss: 27.0457\n",
      "Epoch [235/300], Loss: 27.0242\n",
      "Epoch [236/300], Loss: 27.0028\n",
      "Epoch [237/300], Loss: 26.9817\n",
      "Epoch [238/300], Loss: 26.9603\n",
      "Epoch [239/300], Loss: 26.9392\n",
      "Epoch [240/300], Loss: 26.9180\n",
      "Epoch [241/300], Loss: 26.8967\n",
      "Epoch [242/300], Loss: 26.8755\n",
      "Epoch [243/300], Loss: 26.8545\n",
      "Epoch [244/300], Loss: 26.8333\n",
      "Epoch [245/300], Loss: 26.8125\n",
      "Epoch [246/300], Loss: 26.7915\n",
      "Epoch [247/300], Loss: 26.7704\n",
      "Epoch [248/300], Loss: 26.7495\n",
      "Epoch [249/300], Loss: 26.7285\n",
      "Epoch [250/300], Loss: 26.7076\n",
      "Epoch [251/300], Loss: 26.6869\n",
      "Epoch [252/300], Loss: 26.6661\n",
      "Epoch [253/300], Loss: 26.6456\n",
      "Epoch [254/300], Loss: 26.6245\n",
      "Epoch [255/300], Loss: 26.6040\n",
      "Epoch [256/300], Loss: 26.5835\n",
      "Epoch [257/300], Loss: 26.5628\n",
      "Epoch [258/300], Loss: 26.5422\n",
      "Epoch [259/300], Loss: 26.5215\n",
      "Epoch [260/300], Loss: 26.5009\n",
      "Epoch [261/300], Loss: 26.4805\n",
      "Epoch [262/300], Loss: 26.4601\n",
      "Epoch [263/300], Loss: 26.4396\n",
      "Epoch [264/300], Loss: 26.4192\n",
      "Epoch [265/300], Loss: 26.3989\n",
      "Epoch [266/300], Loss: 26.3789\n",
      "Epoch [267/300], Loss: 26.3583\n",
      "Epoch [268/300], Loss: 26.3382\n",
      "Epoch [269/300], Loss: 26.3179\n",
      "Epoch [270/300], Loss: 26.2981\n",
      "Epoch [271/300], Loss: 26.2779\n",
      "Epoch [272/300], Loss: 26.2577\n",
      "Epoch [273/300], Loss: 26.2378\n",
      "Epoch [274/300], Loss: 26.2177\n",
      "Epoch [275/300], Loss: 26.1976\n",
      "Epoch [276/300], Loss: 26.1781\n",
      "Epoch [277/300], Loss: 26.1580\n",
      "Epoch [278/300], Loss: 26.1385\n",
      "Epoch [279/300], Loss: 26.1186\n",
      "Epoch [280/300], Loss: 26.0988\n",
      "Epoch [281/300], Loss: 26.0790\n",
      "Epoch [282/300], Loss: 26.0595\n",
      "Epoch [283/300], Loss: 26.0399\n",
      "Epoch [284/300], Loss: 26.0203\n",
      "Epoch [285/300], Loss: 26.0008\n",
      "Epoch [286/300], Loss: 25.9814\n",
      "Epoch [287/300], Loss: 25.9619\n",
      "Epoch [288/300], Loss: 25.9426\n",
      "Epoch [289/300], Loss: 25.9231\n",
      "Epoch [290/300], Loss: 25.9042\n",
      "Epoch [291/300], Loss: 25.8849\n",
      "Epoch [292/300], Loss: 25.8657\n",
      "Epoch [293/300], Loss: 25.8466\n",
      "Epoch [294/300], Loss: 25.8275\n",
      "Epoch [295/300], Loss: 25.8083\n",
      "Epoch [296/300], Loss: 25.7894\n",
      "Epoch [297/300], Loss: 25.7705\n",
      "Epoch [298/300], Loss: 25.7517\n",
      "Epoch [299/300], Loss: 25.7326\n",
      "Epoch [300/300], Loss: 25.7139\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.03068167 Test: 0.028927818\n",
      "Epoch [1/300], Loss: 230.2128\n",
      "Validation Loss: 0.2087\n",
      "Epoch [2/300], Loss: 169.5567\n",
      "Epoch [3/300], Loss: 152.1751\n",
      "Epoch [4/300], Loss: 121.2661\n",
      "Epoch [5/300], Loss: 97.9328\n",
      "Epoch [6/300], Loss: 88.0252\n",
      "Epoch [7/300], Loss: 80.1715\n",
      "Epoch [8/300], Loss: 72.9627\n",
      "Epoch [9/300], Loss: 67.0638\n",
      "Epoch [10/300], Loss: 62.8061\n",
      "Epoch [11/300], Loss: 59.8907\n",
      "Epoch [12/300], Loss: 57.7867\n",
      "Epoch [13/300], Loss: 56.1111\n",
      "Epoch [14/300], Loss: 54.6667\n",
      "Epoch [15/300], Loss: 53.3585\n",
      "Epoch [16/300], Loss: 52.1383\n",
      "Epoch [17/300], Loss: 50.9828\n",
      "Epoch [18/300], Loss: 49.8852\n",
      "Epoch [19/300], Loss: 48.8453\n",
      "Epoch [20/300], Loss: 47.8625\n",
      "Epoch [21/300], Loss: 46.9346\n",
      "Epoch [22/300], Loss: 46.0587\n",
      "Epoch [23/300], Loss: 45.2317\n",
      "Epoch [24/300], Loss: 44.4513\n",
      "Epoch [25/300], Loss: 43.7155\n",
      "Epoch [26/300], Loss: 43.0225\n",
      "Epoch [27/300], Loss: 42.3709\n",
      "Epoch [28/300], Loss: 41.7592\n",
      "Epoch [29/300], Loss: 41.1858\n",
      "Epoch [30/300], Loss: 40.6489\n",
      "Epoch [31/300], Loss: 40.1461\n",
      "Epoch [32/300], Loss: 39.6751\n",
      "Epoch [33/300], Loss: 39.2333\n",
      "Epoch [34/300], Loss: 38.8181\n",
      "Epoch [35/300], Loss: 38.4268\n",
      "Epoch [36/300], Loss: 38.0574\n",
      "Epoch [37/300], Loss: 37.7084\n",
      "Epoch [38/300], Loss: 37.3788\n",
      "Epoch [39/300], Loss: 37.0687\n",
      "Epoch [40/300], Loss: 36.7778\n",
      "Epoch [41/300], Loss: 36.5063\n",
      "Epoch [42/300], Loss: 36.2542\n",
      "Epoch [43/300], Loss: 36.0207\n",
      "Epoch [44/300], Loss: 35.8051\n",
      "Epoch [45/300], Loss: 35.6062\n",
      "Epoch [46/300], Loss: 35.4222\n",
      "Epoch [47/300], Loss: 35.2523\n",
      "Epoch [48/300], Loss: 35.0947\n",
      "Epoch [49/300], Loss: 34.9481\n",
      "Epoch [50/300], Loss: 34.8114\n",
      "Epoch [51/300], Loss: 34.6833\n",
      "Epoch [52/300], Loss: 34.5635\n",
      "Epoch [53/300], Loss: 34.4502\n",
      "Epoch [54/300], Loss: 34.3433\n",
      "Epoch [55/300], Loss: 34.2419\n",
      "Epoch [56/300], Loss: 34.1456\n",
      "Epoch [57/300], Loss: 34.0536\n",
      "Epoch [58/300], Loss: 33.9658\n",
      "Epoch [59/300], Loss: 33.8817\n",
      "Epoch [60/300], Loss: 33.8008\n",
      "Epoch [61/300], Loss: 33.7229\n",
      "Epoch [62/300], Loss: 33.6478\n",
      "Epoch [63/300], Loss: 33.5751\n",
      "Epoch [64/300], Loss: 33.5047\n",
      "Epoch [65/300], Loss: 33.4362\n",
      "Epoch [66/300], Loss: 33.3697\n",
      "Epoch [67/300], Loss: 33.3047\n",
      "Epoch [68/300], Loss: 33.2413\n",
      "Epoch [69/300], Loss: 33.1792\n",
      "Epoch [70/300], Loss: 33.1184\n",
      "Epoch [71/300], Loss: 33.0587\n",
      "Epoch [72/300], Loss: 33.0000\n",
      "Epoch [73/300], Loss: 32.9421\n",
      "Epoch [74/300], Loss: 32.8851\n",
      "Epoch [75/300], Loss: 32.8288\n",
      "Epoch [76/300], Loss: 32.7730\n",
      "Epoch [77/300], Loss: 32.7180\n",
      "Epoch [78/300], Loss: 32.6635\n",
      "Epoch [79/300], Loss: 32.6093\n",
      "Epoch [80/300], Loss: 32.5554\n",
      "Epoch [81/300], Loss: 32.5021\n",
      "Epoch [82/300], Loss: 32.4492\n",
      "Epoch [83/300], Loss: 32.3960\n",
      "Epoch [84/300], Loss: 32.3436\n",
      "Epoch [85/300], Loss: 32.2912\n",
      "Epoch [86/300], Loss: 32.2389\n",
      "Epoch [87/300], Loss: 32.1869\n",
      "Epoch [88/300], Loss: 32.1348\n",
      "Epoch [89/300], Loss: 32.0830\n",
      "Epoch [90/300], Loss: 32.0312\n",
      "Epoch [91/300], Loss: 31.9793\n",
      "Epoch [92/300], Loss: 31.9276\n",
      "Epoch [93/300], Loss: 31.8762\n",
      "Epoch [94/300], Loss: 31.8246\n",
      "Epoch [95/300], Loss: 31.7730\n",
      "Epoch [96/300], Loss: 31.7216\n",
      "Epoch [97/300], Loss: 31.6701\n",
      "Epoch [98/300], Loss: 31.6185\n",
      "Epoch [99/300], Loss: 31.5674\n",
      "Epoch [100/300], Loss: 31.5159\n",
      "Epoch [101/300], Loss: 31.4649\n",
      "Validation Loss: 0.0356\n",
      "Epoch [102/300], Loss: 31.4138\n",
      "Epoch [103/300], Loss: 31.3629\n",
      "Epoch [104/300], Loss: 31.3122\n",
      "Epoch [105/300], Loss: 31.2616\n",
      "Epoch [106/300], Loss: 31.2111\n",
      "Epoch [107/300], Loss: 31.1610\n",
      "Epoch [108/300], Loss: 31.1109\n",
      "Epoch [109/300], Loss: 31.0611\n",
      "Epoch [110/300], Loss: 31.0117\n",
      "Epoch [111/300], Loss: 30.9626\n",
      "Epoch [112/300], Loss: 30.9136\n",
      "Epoch [113/300], Loss: 30.8652\n",
      "Epoch [114/300], Loss: 30.8171\n",
      "Epoch [115/300], Loss: 30.7694\n",
      "Epoch [116/300], Loss: 30.7223\n",
      "Epoch [117/300], Loss: 30.6756\n",
      "Epoch [118/300], Loss: 30.6294\n",
      "Epoch [119/300], Loss: 30.5835\n",
      "Epoch [120/300], Loss: 30.5385\n",
      "Epoch [121/300], Loss: 30.4939\n",
      "Epoch [122/300], Loss: 30.4496\n",
      "Epoch [123/300], Loss: 30.4064\n",
      "Epoch [124/300], Loss: 30.3637\n",
      "Epoch [125/300], Loss: 30.3213\n",
      "Epoch [126/300], Loss: 30.2802\n",
      "Epoch [127/300], Loss: 30.2391\n",
      "Epoch [128/300], Loss: 30.1988\n",
      "Epoch [129/300], Loss: 30.1594\n",
      "Epoch [130/300], Loss: 30.1206\n",
      "Epoch [131/300], Loss: 30.0823\n",
      "Epoch [132/300], Loss: 30.0452\n",
      "Epoch [133/300], Loss: 30.0082\n",
      "Epoch [134/300], Loss: 29.9723\n",
      "Epoch [135/300], Loss: 29.9368\n",
      "Epoch [136/300], Loss: 29.9021\n",
      "Epoch [137/300], Loss: 29.8679\n",
      "Epoch [138/300], Loss: 29.8345\n",
      "Epoch [139/300], Loss: 29.8017\n",
      "Epoch [140/300], Loss: 29.7693\n",
      "Epoch [141/300], Loss: 29.7376\n",
      "Epoch [142/300], Loss: 29.7066\n",
      "Epoch [143/300], Loss: 29.6762\n",
      "Epoch [144/300], Loss: 29.6460\n",
      "Epoch [145/300], Loss: 29.6168\n",
      "Epoch [146/300], Loss: 29.5876\n",
      "Epoch [147/300], Loss: 29.5593\n",
      "Epoch [148/300], Loss: 29.5313\n",
      "Epoch [149/300], Loss: 29.5041\n",
      "Epoch [150/300], Loss: 29.4770\n",
      "Epoch [151/300], Loss: 29.4505\n",
      "Epoch [152/300], Loss: 29.4243\n",
      "Epoch [153/300], Loss: 29.3984\n",
      "Epoch [154/300], Loss: 29.3730\n",
      "Epoch [155/300], Loss: 29.3482\n",
      "Epoch [156/300], Loss: 29.3234\n",
      "Epoch [157/300], Loss: 29.2991\n",
      "Epoch [158/300], Loss: 29.2753\n",
      "Epoch [159/300], Loss: 29.2518\n",
      "Epoch [160/300], Loss: 29.2281\n",
      "Epoch [161/300], Loss: 29.2052\n",
      "Epoch [162/300], Loss: 29.1826\n",
      "Epoch [163/300], Loss: 29.1601\n",
      "Epoch [164/300], Loss: 29.1379\n",
      "Epoch [165/300], Loss: 29.1159\n",
      "Epoch [166/300], Loss: 29.0943\n",
      "Epoch [167/300], Loss: 29.0729\n",
      "Epoch [168/300], Loss: 29.0516\n",
      "Epoch [169/300], Loss: 29.0303\n",
      "Epoch [170/300], Loss: 29.0097\n",
      "Epoch [171/300], Loss: 28.9889\n",
      "Epoch [172/300], Loss: 28.9683\n",
      "Epoch [173/300], Loss: 28.9482\n",
      "Epoch [174/300], Loss: 28.9282\n",
      "Epoch [175/300], Loss: 28.9082\n",
      "Epoch [176/300], Loss: 28.8884\n",
      "Epoch [177/300], Loss: 28.8686\n",
      "Epoch [178/300], Loss: 28.8495\n",
      "Epoch [179/300], Loss: 28.8300\n",
      "Epoch [180/300], Loss: 28.8110\n",
      "Epoch [181/300], Loss: 28.7920\n",
      "Epoch [182/300], Loss: 28.7731\n",
      "Epoch [183/300], Loss: 28.7542\n",
      "Epoch [184/300], Loss: 28.7357\n",
      "Epoch [185/300], Loss: 28.7173\n",
      "Epoch [186/300], Loss: 28.6987\n",
      "Epoch [187/300], Loss: 28.6802\n",
      "Epoch [188/300], Loss: 28.6624\n",
      "Epoch [189/300], Loss: 28.6443\n",
      "Epoch [190/300], Loss: 28.6263\n",
      "Epoch [191/300], Loss: 28.6083\n",
      "Epoch [192/300], Loss: 28.5906\n",
      "Epoch [193/300], Loss: 28.5730\n",
      "Epoch [194/300], Loss: 28.5554\n",
      "Epoch [195/300], Loss: 28.5379\n",
      "Epoch [196/300], Loss: 28.5203\n",
      "Epoch [197/300], Loss: 28.5030\n",
      "Epoch [198/300], Loss: 28.4858\n",
      "Epoch [199/300], Loss: 28.4686\n",
      "Epoch [200/300], Loss: 28.4515\n",
      "Epoch [201/300], Loss: 28.4343\n",
      "Validation Loss: 0.0320\n",
      "Epoch [202/300], Loss: 28.4173\n",
      "Epoch [203/300], Loss: 28.4004\n",
      "Epoch [204/300], Loss: 28.3834\n",
      "Epoch [205/300], Loss: 28.3668\n",
      "Epoch [206/300], Loss: 28.3499\n",
      "Epoch [207/300], Loss: 28.3331\n",
      "Epoch [208/300], Loss: 28.3166\n",
      "Epoch [209/300], Loss: 28.3001\n",
      "Epoch [210/300], Loss: 28.2835\n",
      "Epoch [211/300], Loss: 28.2670\n",
      "Epoch [212/300], Loss: 28.2507\n",
      "Epoch [213/300], Loss: 28.2341\n",
      "Epoch [214/300], Loss: 28.2176\n",
      "Epoch [215/300], Loss: 28.2014\n",
      "Epoch [216/300], Loss: 28.1854\n",
      "Epoch [217/300], Loss: 28.1689\n",
      "Epoch [218/300], Loss: 28.1527\n",
      "Epoch [219/300], Loss: 28.1367\n",
      "Epoch [220/300], Loss: 28.1203\n",
      "Epoch [221/300], Loss: 28.1043\n",
      "Epoch [222/300], Loss: 28.0881\n",
      "Epoch [223/300], Loss: 28.0720\n",
      "Epoch [224/300], Loss: 28.0561\n",
      "Epoch [225/300], Loss: 28.0403\n",
      "Epoch [226/300], Loss: 28.0241\n",
      "Epoch [227/300], Loss: 28.0082\n",
      "Epoch [228/300], Loss: 27.9923\n",
      "Epoch [229/300], Loss: 27.9764\n",
      "Epoch [230/300], Loss: 27.9607\n",
      "Epoch [231/300], Loss: 27.9450\n",
      "Epoch [232/300], Loss: 27.9288\n",
      "Epoch [233/300], Loss: 27.9130\n",
      "Epoch [234/300], Loss: 27.8974\n",
      "Epoch [235/300], Loss: 27.8816\n",
      "Epoch [236/300], Loss: 27.8657\n",
      "Epoch [237/300], Loss: 27.8502\n",
      "Epoch [238/300], Loss: 27.8342\n",
      "Epoch [239/300], Loss: 27.8186\n",
      "Epoch [240/300], Loss: 27.8027\n",
      "Epoch [241/300], Loss: 27.7871\n",
      "Epoch [242/300], Loss: 27.7713\n",
      "Epoch [243/300], Loss: 27.7558\n",
      "Epoch [244/300], Loss: 27.7403\n",
      "Epoch [245/300], Loss: 27.7244\n",
      "Epoch [246/300], Loss: 27.7088\n",
      "Epoch [247/300], Loss: 27.6931\n",
      "Epoch [248/300], Loss: 27.6777\n",
      "Epoch [249/300], Loss: 27.6618\n",
      "Epoch [250/300], Loss: 27.6462\n",
      "Epoch [251/300], Loss: 27.6307\n",
      "Epoch [252/300], Loss: 27.6152\n",
      "Epoch [253/300], Loss: 27.5995\n",
      "Epoch [254/300], Loss: 27.5839\n",
      "Epoch [255/300], Loss: 27.5683\n",
      "Epoch [256/300], Loss: 27.5526\n",
      "Epoch [257/300], Loss: 27.5370\n",
      "Epoch [258/300], Loss: 27.5215\n",
      "Epoch [259/300], Loss: 27.5058\n",
      "Epoch [260/300], Loss: 27.4903\n",
      "Epoch [261/300], Loss: 27.4747\n",
      "Epoch [262/300], Loss: 27.4591\n",
      "Epoch [263/300], Loss: 27.4436\n",
      "Epoch [264/300], Loss: 27.4280\n",
      "Epoch [265/300], Loss: 27.4125\n",
      "Epoch [266/300], Loss: 27.3966\n",
      "Epoch [267/300], Loss: 27.3811\n",
      "Epoch [268/300], Loss: 27.3656\n",
      "Epoch [269/300], Loss: 27.3501\n",
      "Epoch [270/300], Loss: 27.3345\n",
      "Epoch [271/300], Loss: 27.3191\n",
      "Epoch [272/300], Loss: 27.3034\n",
      "Epoch [273/300], Loss: 27.2877\n",
      "Epoch [274/300], Loss: 27.2721\n",
      "Epoch [275/300], Loss: 27.2568\n",
      "Epoch [276/300], Loss: 27.2411\n",
      "Epoch [277/300], Loss: 27.2252\n",
      "Epoch [278/300], Loss: 27.2096\n",
      "Epoch [279/300], Loss: 27.1942\n",
      "Epoch [280/300], Loss: 27.1785\n",
      "Epoch [281/300], Loss: 27.1632\n",
      "Epoch [282/300], Loss: 27.1479\n",
      "Epoch [283/300], Loss: 27.1320\n",
      "Epoch [284/300], Loss: 27.1164\n",
      "Epoch [285/300], Loss: 27.1008\n",
      "Epoch [286/300], Loss: 27.0852\n",
      "Epoch [287/300], Loss: 27.0695\n",
      "Epoch [288/300], Loss: 27.0541\n",
      "Epoch [289/300], Loss: 27.0385\n",
      "Epoch [290/300], Loss: 27.0231\n",
      "Epoch [291/300], Loss: 27.0074\n",
      "Epoch [292/300], Loss: 26.9919\n",
      "Epoch [293/300], Loss: 26.9765\n",
      "Epoch [294/300], Loss: 26.9607\n",
      "Epoch [295/300], Loss: 26.9453\n",
      "Epoch [296/300], Loss: 26.9299\n",
      "Epoch [297/300], Loss: 26.9142\n",
      "Epoch [298/300], Loss: 26.8987\n",
      "Epoch [299/300], Loss: 26.8835\n",
      "Epoch [300/300], Loss: 26.8678\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.032058228 Test: 0.030276356\n",
      "Epoch [1/300], Loss: 232.2344\n",
      "Validation Loss: 0.2097\n",
      "Epoch [2/300], Loss: 170.7275\n",
      "Epoch [3/300], Loss: 154.5309\n",
      "Epoch [4/300], Loss: 124.1560\n",
      "Epoch [5/300], Loss: 98.5637\n",
      "Epoch [6/300], Loss: 87.9084\n",
      "Epoch [7/300], Loss: 80.1023\n",
      "Epoch [8/300], Loss: 72.9880\n",
      "Epoch [9/300], Loss: 67.1513\n",
      "Epoch [10/300], Loss: 62.9037\n",
      "Epoch [11/300], Loss: 59.9605\n",
      "Epoch [12/300], Loss: 57.8130\n",
      "Epoch [13/300], Loss: 56.0903\n",
      "Epoch [14/300], Loss: 54.6001\n",
      "Epoch [15/300], Loss: 53.2508\n",
      "Epoch [16/300], Loss: 51.9957\n",
      "Epoch [17/300], Loss: 50.8099\n",
      "Epoch [18/300], Loss: 49.6812\n",
      "Epoch [19/300], Loss: 48.6054\n",
      "Epoch [20/300], Loss: 47.5806\n",
      "Epoch [21/300], Loss: 46.6055\n",
      "Epoch [22/300], Loss: 45.6775\n",
      "Epoch [23/300], Loss: 44.7943\n",
      "Epoch [24/300], Loss: 43.9534\n",
      "Epoch [25/300], Loss: 43.1527\n",
      "Epoch [26/300], Loss: 42.3906\n",
      "Epoch [27/300], Loss: 41.6663\n",
      "Epoch [28/300], Loss: 40.9795\n",
      "Epoch [29/300], Loss: 40.3308\n",
      "Epoch [30/300], Loss: 39.7211\n",
      "Epoch [31/300], Loss: 39.1513\n",
      "Epoch [32/300], Loss: 38.6222\n",
      "Epoch [33/300], Loss: 38.1338\n",
      "Epoch [34/300], Loss: 37.6858\n",
      "Epoch [35/300], Loss: 37.2774\n",
      "Epoch [36/300], Loss: 36.9068\n",
      "Epoch [37/300], Loss: 36.5717\n",
      "Epoch [38/300], Loss: 36.2696\n",
      "Epoch [39/300], Loss: 35.9974\n",
      "Epoch [40/300], Loss: 35.7521\n",
      "Epoch [41/300], Loss: 35.5307\n",
      "Epoch [42/300], Loss: 35.3303\n",
      "Epoch [43/300], Loss: 35.1482\n",
      "Epoch [44/300], Loss: 34.9820\n",
      "Epoch [45/300], Loss: 34.8296\n",
      "Epoch [46/300], Loss: 34.6890\n",
      "Epoch [47/300], Loss: 34.5590\n",
      "Epoch [48/300], Loss: 34.4378\n",
      "Epoch [49/300], Loss: 34.3246\n",
      "Epoch [50/300], Loss: 34.2181\n",
      "Epoch [51/300], Loss: 34.1178\n",
      "Epoch [52/300], Loss: 34.0228\n",
      "Epoch [53/300], Loss: 33.9326\n",
      "Epoch [54/300], Loss: 33.8465\n",
      "Epoch [55/300], Loss: 33.7645\n",
      "Epoch [56/300], Loss: 33.6856\n",
      "Epoch [57/300], Loss: 33.6098\n",
      "Epoch [58/300], Loss: 33.5368\n",
      "Epoch [59/300], Loss: 33.4660\n",
      "Epoch [60/300], Loss: 33.3977\n",
      "Epoch [61/300], Loss: 33.3314\n",
      "Epoch [62/300], Loss: 33.2668\n",
      "Epoch [63/300], Loss: 33.2038\n",
      "Epoch [64/300], Loss: 33.1424\n",
      "Epoch [65/300], Loss: 33.0822\n",
      "Epoch [66/300], Loss: 33.0233\n",
      "Epoch [67/300], Loss: 32.9653\n",
      "Epoch [68/300], Loss: 32.9083\n",
      "Epoch [69/300], Loss: 32.8522\n",
      "Epoch [70/300], Loss: 32.7968\n",
      "Epoch [71/300], Loss: 32.7421\n",
      "Epoch [72/300], Loss: 32.6880\n",
      "Epoch [73/300], Loss: 32.6345\n",
      "Epoch [74/300], Loss: 32.5813\n",
      "Epoch [75/300], Loss: 32.5287\n",
      "Epoch [76/300], Loss: 32.4764\n",
      "Epoch [77/300], Loss: 32.4242\n",
      "Epoch [78/300], Loss: 32.3724\n",
      "Epoch [79/300], Loss: 32.3210\n",
      "Epoch [80/300], Loss: 32.2696\n",
      "Epoch [81/300], Loss: 32.2184\n",
      "Epoch [82/300], Loss: 32.1673\n",
      "Epoch [83/300], Loss: 32.1164\n",
      "Epoch [84/300], Loss: 32.0657\n",
      "Epoch [85/300], Loss: 32.0145\n",
      "Epoch [86/300], Loss: 31.9640\n",
      "Epoch [87/300], Loss: 31.9134\n",
      "Epoch [88/300], Loss: 31.8629\n",
      "Epoch [89/300], Loss: 31.8124\n",
      "Epoch [90/300], Loss: 31.7620\n",
      "Epoch [91/300], Loss: 31.7117\n",
      "Epoch [92/300], Loss: 31.6614\n",
      "Epoch [93/300], Loss: 31.6110\n",
      "Epoch [94/300], Loss: 31.5610\n",
      "Epoch [95/300], Loss: 31.5111\n",
      "Epoch [96/300], Loss: 31.4613\n",
      "Epoch [97/300], Loss: 31.4116\n",
      "Epoch [98/300], Loss: 31.3619\n",
      "Epoch [99/300], Loss: 31.3124\n",
      "Epoch [100/300], Loss: 31.2633\n",
      "Epoch [101/300], Loss: 31.2142\n",
      "Validation Loss: 0.0353\n",
      "Epoch [102/300], Loss: 31.1655\n",
      "Epoch [103/300], Loss: 31.1169\n",
      "Epoch [104/300], Loss: 31.0689\n",
      "Epoch [105/300], Loss: 31.0211\n",
      "Epoch [106/300], Loss: 30.9735\n",
      "Epoch [107/300], Loss: 30.9265\n",
      "Epoch [108/300], Loss: 30.8798\n",
      "Epoch [109/300], Loss: 30.8335\n",
      "Epoch [110/300], Loss: 30.7878\n",
      "Epoch [111/300], Loss: 30.7424\n",
      "Epoch [112/300], Loss: 30.6977\n",
      "Epoch [113/300], Loss: 30.6533\n",
      "Epoch [114/300], Loss: 30.6100\n",
      "Epoch [115/300], Loss: 30.5669\n",
      "Epoch [116/300], Loss: 30.5244\n",
      "Epoch [117/300], Loss: 30.4827\n",
      "Epoch [118/300], Loss: 30.4415\n",
      "Epoch [119/300], Loss: 30.4011\n",
      "Epoch [120/300], Loss: 30.3612\n",
      "Epoch [121/300], Loss: 30.3221\n",
      "Epoch [122/300], Loss: 30.2837\n",
      "Epoch [123/300], Loss: 30.2459\n",
      "Epoch [124/300], Loss: 30.2090\n",
      "Epoch [125/300], Loss: 30.1726\n",
      "Epoch [126/300], Loss: 30.1369\n",
      "Epoch [127/300], Loss: 30.1020\n",
      "Epoch [128/300], Loss: 30.0676\n",
      "Epoch [129/300], Loss: 30.0341\n",
      "Epoch [130/300], Loss: 30.0011\n",
      "Epoch [131/300], Loss: 29.9689\n",
      "Epoch [132/300], Loss: 29.9375\n",
      "Epoch [133/300], Loss: 29.9064\n",
      "Epoch [134/300], Loss: 29.8761\n",
      "Epoch [135/300], Loss: 29.8467\n",
      "Epoch [136/300], Loss: 29.8175\n",
      "Epoch [137/300], Loss: 29.7887\n",
      "Epoch [138/300], Loss: 29.7608\n",
      "Epoch [139/300], Loss: 29.7335\n",
      "Epoch [140/300], Loss: 29.7065\n",
      "Epoch [141/300], Loss: 29.6798\n",
      "Epoch [142/300], Loss: 29.6540\n",
      "Epoch [143/300], Loss: 29.6287\n",
      "Epoch [144/300], Loss: 29.6037\n",
      "Epoch [145/300], Loss: 29.5791\n",
      "Epoch [146/300], Loss: 29.5548\n",
      "Epoch [147/300], Loss: 29.5309\n",
      "Epoch [148/300], Loss: 29.5077\n",
      "Epoch [149/300], Loss: 29.4846\n",
      "Epoch [150/300], Loss: 29.4619\n",
      "Epoch [151/300], Loss: 29.4397\n",
      "Epoch [152/300], Loss: 29.4177\n",
      "Epoch [153/300], Loss: 29.3960\n",
      "Epoch [154/300], Loss: 29.3745\n",
      "Epoch [155/300], Loss: 29.3536\n",
      "Epoch [156/300], Loss: 29.3325\n",
      "Epoch [157/300], Loss: 29.3122\n",
      "Epoch [158/300], Loss: 29.2916\n",
      "Epoch [159/300], Loss: 29.2716\n",
      "Epoch [160/300], Loss: 29.2520\n",
      "Epoch [161/300], Loss: 29.2323\n",
      "Epoch [162/300], Loss: 29.2127\n",
      "Epoch [163/300], Loss: 29.1936\n",
      "Epoch [164/300], Loss: 29.1746\n",
      "Epoch [165/300], Loss: 29.1558\n",
      "Epoch [166/300], Loss: 29.1370\n",
      "Epoch [167/300], Loss: 29.1188\n",
      "Epoch [168/300], Loss: 29.1003\n",
      "Epoch [169/300], Loss: 29.0823\n",
      "Epoch [170/300], Loss: 29.0642\n",
      "Epoch [171/300], Loss: 29.0463\n",
      "Epoch [172/300], Loss: 29.0288\n",
      "Epoch [173/300], Loss: 29.0114\n",
      "Epoch [174/300], Loss: 28.9940\n",
      "Epoch [175/300], Loss: 28.9768\n",
      "Epoch [176/300], Loss: 28.9596\n",
      "Epoch [177/300], Loss: 28.9427\n",
      "Epoch [178/300], Loss: 28.9257\n",
      "Epoch [179/300], Loss: 28.9090\n",
      "Epoch [180/300], Loss: 28.8923\n",
      "Epoch [181/300], Loss: 28.8756\n",
      "Epoch [182/300], Loss: 28.8596\n",
      "Epoch [183/300], Loss: 28.8429\n",
      "Epoch [184/300], Loss: 28.8267\n",
      "Epoch [185/300], Loss: 28.8105\n",
      "Epoch [186/300], Loss: 28.7944\n",
      "Epoch [187/300], Loss: 28.7785\n",
      "Epoch [188/300], Loss: 28.7624\n",
      "Epoch [189/300], Loss: 28.7469\n",
      "Epoch [190/300], Loss: 28.7308\n",
      "Epoch [191/300], Loss: 28.7152\n",
      "Epoch [192/300], Loss: 28.6999\n",
      "Epoch [193/300], Loss: 28.6839\n",
      "Epoch [194/300], Loss: 28.6688\n",
      "Epoch [195/300], Loss: 28.6532\n",
      "Epoch [196/300], Loss: 28.6379\n",
      "Epoch [197/300], Loss: 28.6227\n",
      "Epoch [198/300], Loss: 28.6075\n",
      "Epoch [199/300], Loss: 28.5923\n",
      "Epoch [200/300], Loss: 28.5772\n",
      "Epoch [201/300], Loss: 28.5622\n",
      "Validation Loss: 0.0322\n",
      "Epoch [202/300], Loss: 28.5473\n",
      "Epoch [203/300], Loss: 28.5324\n",
      "Epoch [204/300], Loss: 28.5175\n",
      "Epoch [205/300], Loss: 28.5024\n",
      "Epoch [206/300], Loss: 28.4877\n",
      "Epoch [207/300], Loss: 28.4731\n",
      "Epoch [208/300], Loss: 28.4583\n",
      "Epoch [209/300], Loss: 28.4436\n",
      "Epoch [210/300], Loss: 28.4290\n",
      "Epoch [211/300], Loss: 28.4144\n",
      "Epoch [212/300], Loss: 28.3999\n",
      "Epoch [213/300], Loss: 28.3854\n",
      "Epoch [214/300], Loss: 28.3709\n",
      "Epoch [215/300], Loss: 28.3565\n",
      "Epoch [216/300], Loss: 28.3422\n",
      "Epoch [217/300], Loss: 28.3280\n",
      "Epoch [218/300], Loss: 28.3135\n",
      "Epoch [219/300], Loss: 28.2992\n",
      "Epoch [220/300], Loss: 28.2847\n",
      "Epoch [221/300], Loss: 28.2705\n",
      "Epoch [222/300], Loss: 28.2563\n",
      "Epoch [223/300], Loss: 28.2421\n",
      "Epoch [224/300], Loss: 28.2279\n",
      "Epoch [225/300], Loss: 28.2138\n",
      "Epoch [226/300], Loss: 28.1996\n",
      "Epoch [227/300], Loss: 28.1856\n",
      "Epoch [228/300], Loss: 28.1714\n",
      "Epoch [229/300], Loss: 28.1575\n",
      "Epoch [230/300], Loss: 28.1432\n",
      "Epoch [231/300], Loss: 28.1294\n",
      "Epoch [232/300], Loss: 28.1154\n",
      "Epoch [233/300], Loss: 28.1015\n",
      "Epoch [234/300], Loss: 28.0877\n",
      "Epoch [235/300], Loss: 28.0735\n",
      "Epoch [236/300], Loss: 28.0594\n",
      "Epoch [237/300], Loss: 28.0456\n",
      "Epoch [238/300], Loss: 28.0318\n",
      "Epoch [239/300], Loss: 28.0180\n",
      "Epoch [240/300], Loss: 28.0040\n",
      "Epoch [241/300], Loss: 27.9900\n",
      "Epoch [242/300], Loss: 27.9764\n",
      "Epoch [243/300], Loss: 27.9623\n",
      "Epoch [244/300], Loss: 27.9484\n",
      "Epoch [245/300], Loss: 27.9347\n",
      "Epoch [246/300], Loss: 27.9211\n",
      "Epoch [247/300], Loss: 27.9074\n",
      "Epoch [248/300], Loss: 27.8934\n",
      "Epoch [249/300], Loss: 27.8797\n",
      "Epoch [250/300], Loss: 27.8659\n",
      "Epoch [251/300], Loss: 27.8519\n",
      "Epoch [252/300], Loss: 27.8381\n",
      "Epoch [253/300], Loss: 27.8246\n",
      "Epoch [254/300], Loss: 27.8107\n",
      "Epoch [255/300], Loss: 27.7969\n",
      "Epoch [256/300], Loss: 27.7830\n",
      "Epoch [257/300], Loss: 27.7691\n",
      "Epoch [258/300], Loss: 27.7554\n",
      "Epoch [259/300], Loss: 27.7416\n",
      "Epoch [260/300], Loss: 27.7280\n",
      "Epoch [261/300], Loss: 27.7140\n",
      "Epoch [262/300], Loss: 27.7000\n",
      "Epoch [263/300], Loss: 27.6863\n",
      "Epoch [264/300], Loss: 27.6724\n",
      "Epoch [265/300], Loss: 27.6587\n",
      "Epoch [266/300], Loss: 27.6447\n",
      "Epoch [267/300], Loss: 27.6307\n",
      "Epoch [268/300], Loss: 27.6170\n",
      "Epoch [269/300], Loss: 27.6030\n",
      "Epoch [270/300], Loss: 27.5891\n",
      "Epoch [271/300], Loss: 27.5753\n",
      "Epoch [272/300], Loss: 27.5609\n",
      "Epoch [273/300], Loss: 27.5472\n",
      "Epoch [274/300], Loss: 27.5331\n",
      "Epoch [275/300], Loss: 27.5191\n",
      "Epoch [276/300], Loss: 27.5050\n",
      "Epoch [277/300], Loss: 27.4911\n",
      "Epoch [278/300], Loss: 27.4769\n",
      "Epoch [279/300], Loss: 27.4626\n",
      "Epoch [280/300], Loss: 27.4487\n",
      "Epoch [281/300], Loss: 27.4345\n",
      "Epoch [282/300], Loss: 27.4203\n",
      "Epoch [283/300], Loss: 27.4059\n",
      "Epoch [284/300], Loss: 27.3916\n",
      "Epoch [285/300], Loss: 27.3776\n",
      "Epoch [286/300], Loss: 27.3632\n",
      "Epoch [287/300], Loss: 27.3488\n",
      "Epoch [288/300], Loss: 27.3344\n",
      "Epoch [289/300], Loss: 27.3199\n",
      "Epoch [290/300], Loss: 27.3054\n",
      "Epoch [291/300], Loss: 27.2910\n",
      "Epoch [292/300], Loss: 27.2763\n",
      "Epoch [293/300], Loss: 27.2620\n",
      "Epoch [294/300], Loss: 27.2473\n",
      "Epoch [295/300], Loss: 27.2326\n",
      "Epoch [296/300], Loss: 27.2179\n",
      "Epoch [297/300], Loss: 27.2031\n",
      "Epoch [298/300], Loss: 27.1885\n",
      "Epoch [299/300], Loss: 27.1736\n",
      "Epoch [300/300], Loss: 27.1586\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAvB0lEQVR4nO3df3yU1YHv8e/8TgKZxADJEAkIiiAVkYJi1h91S1ZA19VKW3HZXrQs3NrQu0q1LbsWal+9l67raquXymtfbqXu9VfZrbhylZYFgVojQpSqCFywWKCQBMFk8oMk8+PcPyYzmRlCfkBmnifk8369nldmnufMzHmOE/L1nPOcx2GMMQIAALARp9UVAAAASEdAAQAAtkNAAQAAtkNAAQAAtkNAAQAAtkNAAQAAtkNAAQAAtkNAAQAAtuO2ugJnIxqN6ujRo8rPz5fD4bC6OgAAoBeMMWpsbFRpaamczu77SAZkQDl69KjKysqsrgYAADgLhw8f1qhRo7otMyADSn5+vqTYCfr9fotrAwAAeiMYDKqsrCzxd7w7AzKgxId1/H4/AQUAgAGmN9MzmCQLAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsZ0DeLDBTdn5yUuvfP6aJgXzNu3q01dUBAGDQogclyb7aRq156xNt3ltndVUAABjUCChJ3M7Y7Z/DUWNxTQAAGNwIKEnczlhzEFAAALAWASWJ2xXrQYlEoxbXBACAwY2AksTVMcQTitCDAgCAlQgoSeJDPBGGeAAAsBQBJQmTZAEAsAcCShJXxxyUcIQ5KAAAWImAksTDEA8AALZAQEniYogHAABbIKAkcTPEAwCALRBQkjBJFgAAeyCgJOEyYwAA7IGAkoSF2gAAsAcCShIPS90DAGALBJQkXMUDAIA9EFCSJO5mzBAPAACWIqAk6bybMQEFAAArEVCSdF5mzBwUAACsREBJEp+DEjVSlF4UAAAsQ0BJ4nZ1NgcTZQEAsA4BJUl8iEdiHgoAAFYioCRxJQWUEPNQAACwDAEliSdpiCfCpcYAAFiGgJIkqQOFOSgAAFiIgJLE4XBwqTEAADZAQEkTX6yN1WQBALAOASVNfLl7ruIBAMA6BJQ0LoZ4AACwHAEljcfFHY0BALAaASVNogeFOSgAAFiGgJImPgeFHhQAAKxDQEkTv4onwhwUAAAsQ0BJwxAPAADWI6Ck6VyojYACAIBVCChpmIMCAID1CChpmIMCAID1CChp4nNQQsxBAQDAMgSUNB6WugcAwHIElDQuJskCAGA5AkqazrsZMwcFAACrEFDScJkxAADWI6CkcTEHBQAAy/UpoKxcuVJXXXWV8vPzVVxcrNtvv1379u1LKdPa2qrKykoNGzZMQ4cO1dy5c1VbW5tS5tChQ7rllluUl5en4uJiPfjggwqHw+d+Nv3AwxAPAACW61NA2bp1qyorK/X2229r48aNCoVCuummm9Tc3Jwoc//99+vVV1/V2rVrtXXrVh09elR33HFH4ngkEtEtt9yi9vZ2vfXWW/rFL36hNWvWaPny5f13VueASbIAAFjPYYw567/Ex48fV3FxsbZu3aobbrhBDQ0NGjFihJ5//nl9+ctfliTt3btXl112maqqqnTNNdfo9ddf11/+5V/q6NGjKikpkSStXr1a3/3ud3X8+HF5vd4ePzcYDKqgoEANDQ3y+/1nW/0u3ffie1q366geuuUy/e314/r1vQEAGMz68vf7nOagNDQ0SJKKiookSdXV1QqFQqqoqEiUmThxokaPHq2qqipJUlVVlSZPnpwIJ5I0a9YsBYNB7d69u8vPaWtrUzAYTNkyxe2KNQkLtQEAYJ2zDijRaFT33Xefrr32Wl1++eWSpJqaGnm9XhUWFqaULSkpUU1NTaJMcjiJH48f68rKlStVUFCQ2MrKys622j2KX8XDUvcAAFjnrANKZWWlPvzwQ7344ov9WZ8uLVu2TA0NDYnt8OHDGfss5qAAAGA999m8aMmSJVq/fr22bdumUaNGJfYHAgG1t7ervr4+pReltrZWgUAgUeadd95Jeb/4VT7xMul8Pp98Pt/ZVLXPPB1DPGGGeAAAsEyfelCMMVqyZIlefvllbd68WWPHjk05Pm3aNHk8Hm3atCmxb9++fTp06JDKy8slSeXl5frggw9UV1eXKLNx40b5/X5NmjTpXM6lX9CDAgCA9frUg1JZWannn39er7zyivLz8xNzRgoKCpSbm6uCggItXLhQS5cuVVFRkfx+v771rW+pvLxc11xzjSTppptu0qRJk/S1r31NjzzyiGpqavTQQw+psrIya70k3WEOCgAA1utTQHnqqackSTfeeGPK/meeeUZ33323JOnxxx+X0+nU3Llz1dbWplmzZulnP/tZoqzL5dL69et17733qry8XEOGDNGCBQv0wx/+8NzOpJ/E78XDVTwAAFinTwGlN0um5OTkaNWqVVq1atUZy4wZM0avvfZaXz46a1jqHgAA63EvnjTcLBAAAOsRUNK4uRcPAACWI6Ck6ZwkSw8KAABWIaCkic9BYYgHAADrEFDSeOJDPFxmDACAZQgoaRILtXGZMQAAliGgpGEOCgAA1iOgpHF3zEEJEVAAALAMASVN/DJjlroHAMA6BJQ0zEEBAMB6BJQ0bi4zBgDAcgSUNCx1DwCA9QgoaVzMQQEAwHIElDSe+BAPc1AAALAMASWNiyEeAAAsR0BJ03mZMQEFAACrEFDSxCfJhiLMQQEAwCoElDTxy4zpQQEAwDoElDTMQQEAwHoElDQeV3wlWYZ4AACwCgElDT0oAABYj4CShjkoAABYj4CSxu3iZoEAAFiNgJKm8148zEEBAMAqBJQ08TkoUSNFGeYBAMASBJQ0bldnkzBRFgAAaxBQ0sSHeCQmygIAYBUCShpXUkBhHgoAANYgoKTxJA/xcCUPAACWIKCkSepAYQ4KAAAWIaCkcTgcieXumYMCAIA1CChdiM9DCXE/HgAALEFA6QLL3QMAYC0CShcSy90TUAAAsAQBpQssdw8AgLUIKF2ID/G0hwkoAABYgYDShfwctySpqTVscU0AABicCChdKMj1SJIaToUsrgkAAIMTAaUL/o6AEmwloAAAYAUCShfoQQEAwFoElC74O+agBE8xBwUAACsQULpADwoAANYioHSBOSgAAFiLgNIFPz0oAABYioDSBX9ORw8KAQUAAEsQULrAHBQAAKxFQOmCP7fjKh5WkgUAwBIElC4wxAMAgLUIKF0oyIsFlLZwVK2hiMW1AQBg8CGgdGGo1y2HI/aYS40BAMg+AkoXnE4HwzwAAFiIgHIG8YmyDSx3DwBA1hFQziB+qTE9KAAAZB8B5QwSQzzMQQEAIOsIKGfAYm0AAFiHgHIGTJIFAMA6BJQziK+FQg8KAADZR0A5A39Ox3L3XMUDAEDWEVDOgDkoAABYh4ByBnneWA9KC0vdAwCQdQSUM8jxuCSJe/EAAGCBPgeUbdu26dZbb1VpaakcDofWrVuXcvzuu++Ww+FI2WbPnp1S5uTJk5o/f778fr8KCwu1cOFCNTU1ndOJ9DefO9Y0bQQUAACyrs8Bpbm5WVOmTNGqVavOWGb27Nk6duxYYnvhhRdSjs+fP1+7d+/Wxo0btX79em3btk2LFy/ue+0zqLMHJWpxTQAAGHzcfX3BnDlzNGfOnG7L+Hw+BQKBLo/t2bNHGzZs0I4dOzR9+nRJ0pNPPqmbb75Zjz76qEpLS/tapYzI8cSyW2uYHhQAALItI3NQtmzZouLiYk2YMEH33nuvTpw4kThWVVWlwsLCRDiRpIqKCjmdTm3fvr3L92tra1MwGEzZMi3eg9JGDwoAAFnX7wFl9uzZevbZZ7Vp0yb94z/+o7Zu3ao5c+YoEon1RNTU1Ki4uDjlNW63W0VFRaqpqenyPVeuXKmCgoLEVlZW1t/VPk18Dgo9KAAAZF+fh3h6Mm/evMTjyZMn64orrtDFF1+sLVu2aObMmWf1nsuWLdPSpUsTz4PBYMZDClfxAABgnYxfZjxu3DgNHz5cBw4ckCQFAgHV1dWllAmHwzp58uQZ5634fD75/f6ULdN88TkooaiMMRn/PAAA0CnjAeXIkSM6ceKERo4cKUkqLy9XfX29qqurE2U2b96saDSqGTNmZLo6vRbvQZGk9gjzUAAAyKY+D/E0NTUlekMk6eDBg9q1a5eKiopUVFSkhx9+WHPnzlUgENDHH3+s73znO7rkkks0a9YsSdJll12m2bNna9GiRVq9erVCoZCWLFmiefPm2eYKHknKcXcGlNZQVL6k5wAAILP63IOyc+dOTZ06VVOnTpUkLV26VFOnTtXy5cvlcrn0/vvv66/+6q906aWXauHChZo2bZp++9vfyufzJd7jueee08SJEzVz5kzdfPPNuu666/Qv//Iv/XdW/cDjcsjhiD1msTYAALKrzz0oN954Y7dzMn7961/3+B5FRUV6/vnn+/rRWeVwOJTjdulUKMJibQAAZBn34ukGi7UBAGANAko3WKwNAABrEFC6kVgLhR4UAACyioDSjcRqskySBQAgqwgo3fBxR2MAACxBQOlGDj0oAABYgoDSjcQk2TA9KAAAZBMBpRvMQQEAwBoElG5wR2MAAKxBQOlGfKE2hngAAMguAko3OhdqowcFAIBsIqB0o3OhNnpQAADIJgJKN5gkCwCANQgo3WCSLAAA1iCgdKOzB4UhHgAAsomA0o3OhdroQQEAIJsIKN3I4V48AABYgoDSDSbJAgBgDQJKN7jMGAAAaxBQupFYSZYeFAAAsoqA0g3uZgwAgDUIKN1gDgoAANYgoHSDhdoAALAGAaUbOW4uMwYAwAoElG4kJsmGIzLGWFwbAAAGDwJKN3wdQzxRI4UiBBQAALKFgNKN+CRZSWpluXsAALKGgNINn9sphyP2mImyAABkDwGlGw6HI9GL0sZEWQAAsoaA0gPuaAwAQPYRUHrApcYAAGQfAaUH8UuNTzEHBQCArCGg9CDX65YknWonoAAAkC0ElB7kdvSgtBBQAADIGgJKD/I6elC4zBgAgOwhoPQgfhUPc1AAAMgeAkoP8ryxgMIQDwAA2UNA6UGuJ36ZMQEFAIBsIaD0IDfRgxK2uCYAAAweBJQexAPKqXYWagMAIFsIKD3ITUySpQcFAIBsIaD0IC/Rg8IcFAAAsoWA0gMuMwYAIPsIKD3gMmMAALKPgNIDLjMGACD7CCg9yKUHBQCArCOg9CCXOSgAAGQdAaUH8ZsFchUPAADZQ0DpQa431kT0oAAAkD0ElB7ELzNmDgoAANlDQOlBfIinPRxVJGosrg0AAIMDAaUH8UmyEpcaAwCQLQSUHuR4OpuIYR4AALKDgNIDh8PBYm0AAGQZAaUXWO4eAIDsIqD0AjcMBAAguwgovdC53H3Y4poAADA4EFB6IT7EwxwUAACyg4DSC4khnvaoxTUBAGBwIKD0Qh5DPAAAZBUBpRe4zBgAgOwioPRCLpcZAwCQVX0OKNu2bdOtt96q0tJSORwOrVu3LuW4MUbLly/XyJEjlZubq4qKCu3fvz+lzMmTJzV//nz5/X4VFhZq4cKFampqOqcTyaRcLjMGACCr+hxQmpubNWXKFK1atarL44888oieeOIJrV69Wtu3b9eQIUM0a9Ystba2JsrMnz9fu3fv1saNG7V+/Xpt27ZNixcvPvuzyLD4HJRT9KAAAJAV7r6+YM6cOZozZ06Xx4wx+slPfqKHHnpIt912myTp2WefVUlJidatW6d58+Zpz5492rBhg3bs2KHp06dLkp588kndfPPNevTRR1VaWnoOp5MZ9KAAAJBd/ToH5eDBg6qpqVFFRUViX0FBgWbMmKGqqipJUlVVlQoLCxPhRJIqKirkdDq1ffv2Lt+3ra1NwWAwZcumHHpQAADIqn4NKDU1NZKkkpKSlP0lJSWJYzU1NSouLk457na7VVRUlCiTbuXKlSooKEhsZWVl/VntHuV19KC00IMCAEBWDIireJYtW6aGhobEdvjw4ax+fi49KAAAZFW/BpRAICBJqq2tTdlfW1ubOBYIBFRXV5dyPBwO6+TJk4ky6Xw+n/x+f8qWTQW5XknSyeb2rH4uAACDVb8GlLFjxyoQCGjTpk2JfcFgUNu3b1d5ebkkqby8XPX19aqurk6U2bx5s6LRqGbMmNGf1ek3xX6fJOl4Y5vFNQEAYHDo81U8TU1NOnDgQOL5wYMHtWvXLhUVFWn06NG677779KMf/Ujjx4/X2LFj9f3vf1+lpaW6/fbbJUmXXXaZZs+erUWLFmn16tUKhUJasmSJ5s2bZ8sreCSpOL8zoBhj5HA4LK4RAADntz4HlJ07d+rP//zPE8+XLl0qSVqwYIHWrFmj73znO2pubtbixYtVX1+v6667Ths2bFBOTk7iNc8995yWLFmimTNnyul0au7cuXriiSf64XQyY0RHQGmPRFXfEtIFQ7wW1wgAgPObwxhjrK5EXwWDQRUUFKihoSFr81Gu/OFvVN8S0q/vu0ETAvlZ+UwAAM4nffn7PSCu4rGD+DBPXWNrDyUBAMC5IqD0UnF+bIiqLshEWQAAMo2A0kudPSgEFAAAMo2A0ksj/AzxAACQLQSUXkoM8dCDAgBAxhFQeimxFgpzUAAAyDgCSi9xFQ8AANlDQOmlEj9DPAAAZAsBpZfi9+NpaY+oqS1scW0AADi/EVB6Kc/r1lBf7M4AdUGGeQAAyCQCSh/Ee1GONRBQAADIJAJKH4wuypMkHT7ZYnFNAAA4vxFQ+iAeUA4RUAAAyCgCSh8QUAAAyA4CSh+UMcQDAEBWEFD6IN6D8kcCCgAAGUVA6YN4QKlvCanhVMji2gAAcP4ioPTBEJ9bw4d6JTHMAwBAJhFQ+oh5KAAAZB4BpY+4kgcAgMwjoPQRAQUAgMwjoPQRAQUAgMwjoPTRRcOHSJIOftpscU0AADh/EVD6aGxHQPlT/Sm1hiIW1wYAgPMTAaWPhg3xKt/nljFcyQMAQKYQUPrI4XBo7IhYL8ofGOYBACAjCChn4aJhsYDyCQEFAICMIKCchbFMlAUAIKMIKGeBgAIAQGYRUM4CAQUAgMwioJyF+FoodY1tam4LW1wbAADOPwSUs1CQ69GwIbG7GtOLAgBA/yOgnKWLi4dKkvbXNVpcEwAAzj8ElLM0MZAvSdp7jIACAEB/I6CcpYkBvyRpTw0BBQCA/kZAOUsTR8Z6UPbVBC2uCQAA5x8Cylm6tCQWUGqDbTrZ3G5xbQAAOL8QUM7SUJ9bo4vyJEl76UUBAKBfEVDOARNlAQDIDALKOYgHlH1MlAUAoF8RUM7BZSNjV/L8/ki9tRUBAOA8Q0A5B9MuukCStK+2UfUtTJQFAKC/EFDOQXF+jsaNGCJjpB2ffGZ1dQAAOG8QUM7RjLHDJEnb/3DC4poAAHD+IKCco2vGFUmSth88aXFNAAA4fxBQztHVY2MBZffRBgVbQxbXBgCA8wMB5RyNLMjVmGF5ihrpd/s/tbo6AACcFwgo/WD25wKSpP/8/VGLawIAwPmBgNIPbrvyQknSpr11DPMAANAPCCj94LKR+RpfPFTt4ag2fFBjdXUAABjwCCj9wOFw6PapsV6UX+48bHFtAAAY+Ago/WTu50fJ63Jq5x8/Y00UAADOEQGlnwQKcvTl6aMkSf/7jQMW1wYAgIGNgNKP7v3CxXI5Hfrt/k/1Dgu3AQBw1ggo/aisKE9fnV4mSVr+yocKR6IW1wgAgIGJgNLPHpw1QYV5Hu2tadSatz6xujoAAAxIBJR+VjTEq+/OnihJevQ3+/SH400W1wgAgIGHgJIBd04v07WXDFNrKKr7X9qlEEM9AAD0CQElA5xOhx79yhT5c9z6/ZEG/c//u8fqKgEAMKAQUDJkZEGu/vmrV0qS1rz1if6j+oi1FQIAYAAhoGTQX0wq0f+YOV6StOzlD/T+kXprKwQAwADR7wHlBz/4gRwOR8o2ceLExPHW1lZVVlZq2LBhGjp0qObOnava2tr+roZt3DdzvCouK1Z7OKr//m/VqmtstbpKAADYXkZ6UD73uc/p2LFjie3NN99MHLv//vv16quvau3atdq6dauOHj2qO+64IxPVsAWn06HH7rxS40YM0bGGVt398x1q5I7HAAB0KyMBxe12KxAIJLbhw4dLkhoaGvSv//qveuyxx/TFL35R06ZN0zPPPKO33npLb7/9diaqYgv+HI+eufsqDR/q1UfHgvrG/6lWWzhidbUAALCtjASU/fv3q7S0VOPGjdP8+fN16NAhSVJ1dbVCoZAqKioSZSdOnKjRo0erqqrqjO/X1tamYDCYsg00Y4YN0TN3X60hXpd+d+CEHlj7vqJRY3W1AACwpX4PKDNmzNCaNWu0YcMGPfXUUzp48KCuv/56NTY2qqamRl6vV4WFhSmvKSkpUU1NzRnfc+XKlSooKEhsZWVl/V3trJg8qkCrvzZNbqdDr/7+qP7+5Q8IKQAAdKHfA8qcOXP0la98RVdccYVmzZql1157TfX19frlL3951u+5bNkyNTQ0JLbDhw/3Y42z6/rxI/T4nVfK6ZBe3HFY/7COkAIAQLqMX2ZcWFioSy+9VAcOHFAgEFB7e7vq6+tTytTW1ioQCJzxPXw+n/x+f8o2kN06pTQRUl5457AeeuVDQgoAAEkyHlCampr08ccfa+TIkZo2bZo8Ho82bdqUOL5v3z4dOnRI5eXlma6Krdx25YX6569OkcMhPb/9kB5Y+3uWxAcAoIO7v9/wgQce0K233qoxY8bo6NGjWrFihVwul+666y4VFBRo4cKFWrp0qYqKiuT3+/Wtb31L5eXluuaaa/q7Krb3pamj5JBD3177e/3qvT/ps5Z2rZr/eeV5+/0/CwAAA0q//yU8cuSI7rrrLp04cUIjRozQddddp7ffflsjRoyQJD3++ONyOp2aO3eu2traNGvWLP3sZz/r72oMGLdPvVD+XLe++dy7emPfcf3N09v19IKrVDTEa3XVAACwjMMYM+AmPwSDQRUUFKihoWHAz0eJq/7jSd3zzA4FW8MqK8rV0//tKk0I5FtdLQAA+k1f/n5zLx6bmDamSP9x759pdFGeDp88pTt+9jtt2nP+3gIAAIDuEFBsZHxJvtZVXqtrxhWpuT2iv312px7f+P8U4QofAMAgQ0CxmaIhXv3bwhn6m2tGyxjpp5v262+e3s5NBgEAgwoBxYY8Lqd+dPtk/eTOK5XndanqDyd0809/y5APAGDQIKDY2O1TL9R/LrlOEwP5+rSpXQt/sVMPrv29gtwNGQBwniOg2NwlxUO1rvJaLbp+rBwOaW31Ec1+fJs2fkRvCgDg/EVAGQByPC79wy2T9NLico0uytPRhlYtenanFq7ZoUMnWqyuHgAA/Y6AMoBcPbZIG+67XvfeeLE8Loc27a1TxeNb9b9e26PPmtutrh4AAP2GhdoGqAN1TVrxnx/qdwdOSJLyfW4tumGcFvzZRSrI9VhcOwAATteXv98ElAHMGKMt+47rkV/v055jQUnSEK9LX72qTF+/dqzKivIsriEAAJ0IKINMNGq0/oNjWrX5gPbVNkqSnA7p+vEj9OVpo/QXk0qU43FZXEsAwGBHQBmkjDHatv9TPf3bP+i3+z9N7M/PceuLE4v1F5NKdMOlI+TPYQgIAJB9BBTo4KfN+tW7R/Qf1Ud0tKFzFVq306HLLyzQVRddoKsuKtL0i4q4czIAICsIKEiIRo3ePfSZNu6p1caPavWH482nlbloWJ4mBPI1oSRfEwJ+TQjkq6woVz43w0IAgP5DQMEZHT7Zoh2fnNSOTz7Tjk9O6kBdU5flHA4p4M9R2QV5KivKU1lRrsouyFOJP0fFfp+K830qyPXI4XBk+QwAAAMVAQW9drK5XXuOBbW3plH7aoLaV9Oo/XVNammP9Phar9upEUN9icAyfKhPhXkeXZDnVWGeVxfkeVSY5+l47JU/xy23i6V3AGCwIqDgnBhjdKK5XYdPtujQyRYd+eyUDp1o0ZH6FtUF21TX2KaGU2d3PyB/jlsXDPEqP8etfJ9HQ3Pcys9xy5/j0VBf7HF+TvJ+t4b6PB373RridcvppNcGAAaivvz9dmepThhAHA6Hhg+N9YhMHX1Bl2VaQxEdb4yFleONbTre2KpPm9pV39Ku+lMhfdYSUn1Luz5raVd9c0iNbWFJUrA1rGBr+BzqJg31ujU0x60hPreGeF2xn6c9dmuIr4tjafvzPC4CDwDYEAEFZyXH4+qYm9K7xeBCkagaTsVDS0iNrSE1toYTW1Nb6vPG1pCa2jofN7aGFY4aGSM1toUTgac/5CXCSxchxtsRcHyuHkPQUK9beT6XPAxjAcA5I6AgKzwuZ6JX5mwYY9QWjirYGlJTR4hpbg+rpS2i5vawmtsiam4Lq6ktrJb2sJraImppD6u5reNYe8exjnLN7WFFOwY3W9ojammP6Hg/navX7VSe16U8j0t5PrfyvC7lemJBJrdjf/LjeJnY1lHe69KQtMc5HieTkgEMGgQUDAgOh0M5HpdyPC4V55/7+xlj1BqKdoSbeLCJnBZiYsciHaEnORDFgk88EDW3RdQeiUqS2sNRtYejqtfZzdM5E4dDyvV0hpjkUBMLMS7let3K9biU43F2tJdTPnfnc5/bJZ/HqZyUfc5E28bLuxj2AmAxAgoGJYfDodyO3omz7dVJ1x6OJoLMqY5emdgWCz+n2mPhJvnxqbQyXZVvDcWCjzGdvT2Z5nE5lON2yZcILbEQ43U75XU55XU75XHFHnsS+xyx58nHO455XA553a6On6nvkVwuts8hj8spl9Mht8shj9MpV8dPt8sht9NBTxIwCBBQgH7idTvldccuse5P0ajRqVAvA01bRKdCEbWGImoLR9UWiqg1HFFrKKq2jp+tScdbQ51lQpHOC/pCEaNQpH/n+vQnlzMWVDyuztDi7ggwiXDTw3GPq3Of2+mQ2+WUp+OnuyMcuZ2xwOTq+Ol2OuTqOO5yOBIhynXac2e3x91Oh5yO2Pu7Up47Tn9OIMMgRUABbM7pdCQm4mZSJGpSQkwswEQ6tmgsxISjCkWM2iMRhcJGbZGoQuGo2jt+hiLRjn2dZdojsePtHcdDHY/bIyaxL/lYWziqSNQoHDEKRaPqaiGESNR01Dea0Taxi5SA0xFizvw8FsDimzvpcedzp1xOJcq6nQ45k8qmPo+VdTmdKe/VVVmnIxbAnEl1S97ndjrlTHyuUoJcyuZIPaf0+sfDG1fgnd8IKAAkxf4Ixua3WF2TVJGoUTgaVTjSGVoiUaNQpGNfNKpwPNBEOh/HXxOKdJSPGoUTr4kdD0U69iW9JhQxisSPJT4rVi4UNYpGY6+PdPyMRjvLhc943CTOIxKVIh11jiRt4eiZl6SKRI0iMlLmR/cGnHhQcTnSwlJaQOoyCHW8LjmcJYfBrsomB7D0MHXGz+8o21WQSwlrPYW2rsJaF6Et8XkDPMgRUADYWuwfY5cy3IFkOWOMokYKR6OKRtVt6ImkBJ7UkHP682jaa7ou011w6i5chSNGEdNZx6jp3BcvE98XNaefTySaVjZ+LGlfd8JRI/VQBkoNL930UCXvu2XySH1r5njr6mzZJwMAEhwOh1wOyeWM36STm3VKncEtJcykBaDUfbEglRz0osZ0uS85NCWHsm6DVLef3cWW9F5dBbLkz0muU8rruwh3yUEu/rg78SDX3oe2n35R1wt1ZgsBBQBgW53BbWAOU2RTStA5Q1g6LXSlhKbUcFfi758rHM8WAQUAgPOA0+mQ9zwKcqzJDQAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbMfSgLJq1SpddNFFysnJ0YwZM/TOO+9YWR0AAGATlgWUl156SUuXLtWKFSv07rvvasqUKZo1a5bq6uqsqhIAALAJywLKY489pkWLFumee+7RpEmTtHr1auXl5ennP/+5VVUCAAA24bbiQ9vb21VdXa1ly5Yl9jmdTlVUVKiqqsqKKsXs3yi99qA0YqJUWCY5nLHH0++xrk4AAAxClgSUTz/9VJFIRCUlJSn7S0pKtHfv3tPKt7W1qa2tLfE8GAxmpmK1H0qfHYxtcZfOJqAAAJBllgSUvlq5cqUefvjhzH/Q5xdIF06T6vZKTbWSjDRsfOY/FwAApLAkoAwfPlwul0u1tbUp+2traxUIBE4rv2zZMi1dujTxPBgMqqysrP8rllckjb0htgEAAMtYMknW6/Vq2rRp2rRpU2JfNBrVpk2bVF5eflp5n88nv9+fsgEAgPOXZUM8S5cu1YIFCzR9+nRdffXV+slPfqLm5mbdcw/zPQAAGOwsCyh33nmnjh8/ruXLl6umpkZXXnmlNmzYcNrEWQAAMPg4jDHG6kr0VTAYVEFBgRoaGhjuAQBggOjL32/uxQMAAGyHgAIAAGyHgAIAAGyHgAIAAGyHgAIAAGyHgAIAAGyHgAIAAGyHgAIAAGyHgAIAAGzHsqXuz0V88dtgMGhxTQAAQG/F/273ZhH7ARlQGhsbJUllZWUW1wQAAPRVY2OjCgoKui0zIO/FE41GdfToUeXn58vhcPTreweDQZWVlenw4cPc56cHtFXv0VZ9Q3v1Hm3VN7RX72WirYwxamxsVGlpqZzO7meZDMgeFKfTqVGjRmX0M/x+P1/eXqKteo+26hvaq/doq76hvXqvv9uqp56TOCbJAgAA2yGgAAAA2yGgpPH5fFqxYoV8Pp/VVbE92qr3aKu+ob16j7bqG9qr96xuqwE5SRYAAJzf6EEBAAC2Q0ABAAC2Q0ABAAC2Q0ABAAC2Q0BJsmrVKl100UXKycnRjBkz9M4771hdJcv94Ac/kMPhSNkmTpyYON7a2qrKykoNGzZMQ4cO1dy5c1VbW2thjbNr27ZtuvXWW1VaWiqHw6F169alHDfGaPny5Ro5cqRyc3NVUVGh/fv3p5Q5efKk5s+fL7/fr8LCQi1cuFBNTU1ZPIvs6Kmt7r777tO+a7Nnz04pM1jaauXKlbrqqquUn5+v4uJi3X777dq3b19Kmd787h06dEi33HKL8vLyVFxcrAcffFDhcDibp5IVvWmvG2+88bTv1ze+8Y2UMoOhvZ566ildccUVicXXysvL9frrryeO2+l7RUDp8NJLL2np0qVasWKF3n33XU2ZMkWzZs1SXV2d1VWz3Oc+9zkdO3Yssb355puJY/fff79effVVrV27Vlu3btXRo0d1xx13WFjb7GpubtaUKVO0atWqLo8/8sgjeuKJJ7R69Wpt375dQ4YM0axZs9Ta2pooM3/+fO3evVsbN27U+vXrtW3bNi1evDhbp5A1PbWVJM2ePTvlu/bCCy+kHB8sbbV161ZVVlbq7bff1saNGxUKhXTTTTepubk5Uaan371IJKJbbrlF7e3teuutt/SLX/xCa9as0fLly604pYzqTXtJ0qJFi1K+X4888kji2GBpr1GjRunHP/6xqqurtXPnTn3xi1/Ubbfdpt27d0uy2ffKwBhjzNVXX20qKysTzyORiCktLTUrV660sFbWW7FihZkyZUqXx+rr643H4zFr165N7NuzZ4+RZKqqqrJUQ/uQZF5++eXE82g0agKBgPmnf/qnxL76+nrj8/nMCy+8YIwx5qOPPjKSzI4dOxJlXn/9deNwOMyf/vSnrNU929LbyhhjFixYYG677bYzvmawtpUxxtTV1RlJZuvWrcaY3v3uvfbaa8bpdJqamppEmaeeesr4/X7T1taW3RPIsvT2MsaYL3zhC+bv/u7vzviawdxeF1xwgXn66adt972iB0VSe3u7qqurVVFRkdjndDpVUVGhqqoqC2tmD/v371dpaanGjRun+fPn69ChQ5Kk6upqhUKhlHabOHGiRo8eTbtJOnjwoGpqalLap6CgQDNmzEi0T1VVlQoLCzV9+vREmYqKCjmdTm3fvj3rdbbali1bVFxcrAkTJujee+/ViRMnEscGc1s1NDRIkoqKiiT17nevqqpKkydPVklJSaLMrFmzFAwGE/+3fL5Kb6+45557TsOHD9fll1+uZcuWqaWlJXFsMLZXJBLRiy++qObmZpWXl9vuezUgbxbY3z799FNFIpGUBpekkpIS7d2716Ja2cOMGTO0Zs0aTZgwQceOHdPDDz+s66+/Xh9++KFqamrk9XpVWFiY8pqSkhLV1NRYU2EbibdBV9+r+LGamhoVFxenHHe73SoqKhp0bTh79mzdcccdGjt2rD7++GP9/d//vebMmaOqqiq5XK5B21bRaFT33Xefrr32Wl1++eWS1KvfvZqami6/e/Fj56uu2kuS/vqv/1pjxoxRaWmp3n//fX33u9/Vvn379Ktf/UrS4GqvDz74QOXl5WptbdXQoUP18ssva9KkSdq1a5etvlcEFHRrzpw5icdXXHGFZsyYoTFjxuiXv/ylcnNzLawZzjfz5s1LPJ48ebKuuOIKXXzxxdqyZYtmzpxpYc2sVVlZqQ8//DBl7hfO7EztlTxXafLkyRo5cqRmzpypjz/+WBdffHG2q2mpCRMmaNeuXWpoaNC///u/a8GCBdq6davV1ToNQzyShg8fLpfLddpM5draWgUCAYtqZU+FhYW69NJLdeDAAQUCAbW3t6u+vj6lDO0WE2+D7r5XgUDgtInY4XBYJ0+eHPRtOG7cOA0fPlwHDhyQNDjbasmSJVq/fr3eeOMNjRo1KrG/N797gUCgy+9e/Nj56Ezt1ZUZM2ZIUsr3a7C0l9fr1SWXXKJp06Zp5cqVmjJlin7605/a7ntFQFHsP9a0adO0adOmxL5oNKpNmzapvLzcwprZT1NTkz7++GONHDlS06ZNk8fjSWm3ffv26dChQ7SbpLFjxyoQCKS0TzAY1Pbt2xPtU15ervr6elVXVyfKbN68WdFoNPEP6GB15MgRnThxQiNHjpQ0uNrKGKMlS5bo5Zdf1ubNmzV27NiU47353SsvL9cHH3yQEuo2btwov9+vSZMmZedEsqSn9urKrl27JCnl+zVY2itdNBpVW1ub/b5X/TrldgB78cUXjc/nM2vWrDEfffSRWbx4sSksLEyZqTwYffvb3zZbtmwxBw8eNL/73e9MRUWFGT58uKmrqzPGGPONb3zDjB492mzevNns3LnTlJeXm/LycotrnT2NjY3mvffeM++9956RZB577DHz3nvvmT/+8Y/GGGN+/OMfm8LCQvPKK6+Y999/39x2221m7Nix5tSpU4n3mD17tpk6darZvn27efPNN8348ePNXXfdZdUpZUx3bdXY2GgeeOABU1VVZQ4ePGj+67/+y3z+858348ePN62trYn3GCxtde+995qCggKzZcsWc+zYscTW0tKSKNPT7144HDaXX365uemmm8yuXbvMhg0bzIgRI8yyZcusOKWM6qm9Dhw4YH74wx+anTt3moMHD5pXXnnFjBs3ztxwww2J9xgs7fW9733PbN261Rw8eNC8//775nvf+55xOBzmN7/5jTHGXt8rAkqSJ5980owePdp4vV5z9dVXm7ffftvqKlnuzjvvNCNHjjRer9dceOGF5s477zQHDhxIHD916pT55je/aS644AKTl5dnvvSlL5ljx45ZWOPseuONN4yk07YFCxYYY2KXGn//+983JSUlxufzmZkzZ5p9+/alvMeJEyfMXXfdZYYOHWr8fr+55557TGNjowVnk1ndtVVLS4u56aabzIgRI4zH4zFjxowxixYtOu1/EAZLW3XVTpLMM888kyjTm9+9Tz75xMyZM8fk5uaa4cOHm29/+9smFApl+Wwyr6f2OnTokLnhhhtMUVGR8fl85pJLLjEPPvigaWhoSHmfwdBeX//6182YMWOM1+s1I0aMMDNnzkyEE2Ps9b1yGGNM//bJAAAAnBvmoAAAANshoAAAANshoAAAANshoAAAANshoAAAANshoAAAANshoAAAANshoAAAANshoAAAANshoAAAANshoAAAANshoAAAANv5/yd3voOnPz6RAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.0323993 Test: 0.030636316\n",
      "Epoch [1/300], Loss: 229.5712\n",
      "Validation Loss: 0.2104\n",
      "Epoch [2/300], Loss: 171.7262\n",
      "Epoch [3/300], Loss: 157.1832\n",
      "Epoch [4/300], Loss: 128.1896\n",
      "Epoch [5/300], Loss: 100.2725\n",
      "Epoch [6/300], Loss: 88.4512\n",
      "Epoch [7/300], Loss: 80.6130\n",
      "Epoch [8/300], Loss: 73.5199\n",
      "Epoch [9/300], Loss: 67.6497\n",
      "Epoch [10/300], Loss: 63.3311\n",
      "Epoch [11/300], Loss: 60.3391\n",
      "Epoch [12/300], Loss: 58.1930\n",
      "Epoch [13/300], Loss: 56.5108\n",
      "Epoch [14/300], Loss: 55.0831\n",
      "Epoch [15/300], Loss: 53.8085\n",
      "Epoch [16/300], Loss: 52.6368\n",
      "Epoch [17/300], Loss: 51.5419\n",
      "Epoch [18/300], Loss: 50.5084\n",
      "Epoch [19/300], Loss: 49.5271\n",
      "Epoch [20/300], Loss: 48.5922\n",
      "Epoch [21/300], Loss: 47.7001\n",
      "Epoch [22/300], Loss: 46.8483\n",
      "Epoch [23/300], Loss: 46.0354\n",
      "Epoch [24/300], Loss: 45.2611\n",
      "Epoch [25/300], Loss: 44.5253\n",
      "Epoch [26/300], Loss: 43.8284\n",
      "Epoch [27/300], Loss: 43.1709\n",
      "Epoch [28/300], Loss: 42.5533\n",
      "Epoch [29/300], Loss: 41.9756\n",
      "Epoch [30/300], Loss: 41.4371\n",
      "Epoch [31/300], Loss: 40.9372\n",
      "Epoch [32/300], Loss: 40.4739\n",
      "Epoch [33/300], Loss: 40.0453\n",
      "Epoch [34/300], Loss: 39.6485\n",
      "Epoch [35/300], Loss: 39.2810\n",
      "Epoch [36/300], Loss: 38.9394\n",
      "Epoch [37/300], Loss: 38.6213\n",
      "Epoch [38/300], Loss: 38.3240\n",
      "Epoch [39/300], Loss: 38.0454\n",
      "Epoch [40/300], Loss: 37.7836\n",
      "Epoch [41/300], Loss: 37.5370\n",
      "Epoch [42/300], Loss: 37.3044\n",
      "Epoch [43/300], Loss: 37.0845\n",
      "Epoch [44/300], Loss: 36.8765\n",
      "Epoch [45/300], Loss: 36.6795\n",
      "Epoch [46/300], Loss: 36.4926\n",
      "Epoch [47/300], Loss: 36.3151\n",
      "Epoch [48/300], Loss: 36.1463\n",
      "Epoch [49/300], Loss: 35.9857\n",
      "Epoch [50/300], Loss: 35.8325\n",
      "Epoch [51/300], Loss: 35.6865\n",
      "Epoch [52/300], Loss: 35.5468\n",
      "Epoch [53/300], Loss: 35.4134\n",
      "Epoch [54/300], Loss: 35.2857\n",
      "Epoch [55/300], Loss: 35.1635\n",
      "Epoch [56/300], Loss: 35.0462\n",
      "Epoch [57/300], Loss: 34.9336\n",
      "Epoch [58/300], Loss: 34.8255\n",
      "Epoch [59/300], Loss: 34.7214\n",
      "Epoch [60/300], Loss: 34.6214\n",
      "Epoch [61/300], Loss: 34.5249\n",
      "Epoch [62/300], Loss: 34.4318\n",
      "Epoch [63/300], Loss: 34.3420\n",
      "Epoch [64/300], Loss: 34.2551\n",
      "Epoch [65/300], Loss: 34.1710\n",
      "Epoch [66/300], Loss: 34.0893\n",
      "Epoch [67/300], Loss: 34.0102\n",
      "Epoch [68/300], Loss: 33.9332\n",
      "Epoch [69/300], Loss: 33.8582\n",
      "Epoch [70/300], Loss: 33.7850\n",
      "Epoch [71/300], Loss: 33.7139\n",
      "Epoch [72/300], Loss: 33.6439\n",
      "Epoch [73/300], Loss: 33.5756\n",
      "Epoch [74/300], Loss: 33.5087\n",
      "Epoch [75/300], Loss: 33.4428\n",
      "Epoch [76/300], Loss: 33.3781\n",
      "Epoch [77/300], Loss: 33.3142\n",
      "Epoch [78/300], Loss: 33.2515\n",
      "Epoch [79/300], Loss: 33.1896\n",
      "Epoch [80/300], Loss: 33.1283\n",
      "Epoch [81/300], Loss: 33.0677\n",
      "Epoch [82/300], Loss: 33.0078\n",
      "Epoch [83/300], Loss: 32.9485\n",
      "Epoch [84/300], Loss: 32.8898\n",
      "Epoch [85/300], Loss: 32.8314\n",
      "Epoch [86/300], Loss: 32.7735\n",
      "Epoch [87/300], Loss: 32.7161\n",
      "Epoch [88/300], Loss: 32.6588\n",
      "Epoch [89/300], Loss: 32.6021\n",
      "Epoch [90/300], Loss: 32.5455\n",
      "Epoch [91/300], Loss: 32.4895\n",
      "Epoch [92/300], Loss: 32.4335\n",
      "Epoch [93/300], Loss: 32.3778\n",
      "Epoch [94/300], Loss: 32.3223\n",
      "Epoch [95/300], Loss: 32.2670\n",
      "Epoch [96/300], Loss: 32.2119\n",
      "Epoch [97/300], Loss: 32.1569\n",
      "Epoch [98/300], Loss: 32.1021\n",
      "Epoch [99/300], Loss: 32.0476\n",
      "Epoch [100/300], Loss: 31.9930\n",
      "Epoch [101/300], Loss: 31.9388\n",
      "Validation Loss: 0.0361\n",
      "Epoch [102/300], Loss: 31.8844\n",
      "Epoch [103/300], Loss: 31.8303\n",
      "Epoch [104/300], Loss: 31.7763\n",
      "Epoch [105/300], Loss: 31.7226\n",
      "Epoch [106/300], Loss: 31.6692\n",
      "Epoch [107/300], Loss: 31.6157\n",
      "Epoch [108/300], Loss: 31.5625\n",
      "Epoch [109/300], Loss: 31.5095\n",
      "Epoch [110/300], Loss: 31.4567\n",
      "Epoch [111/300], Loss: 31.4042\n",
      "Epoch [112/300], Loss: 31.3519\n",
      "Epoch [113/300], Loss: 31.2999\n",
      "Epoch [114/300], Loss: 31.2483\n",
      "Epoch [115/300], Loss: 31.1970\n",
      "Epoch [116/300], Loss: 31.1460\n",
      "Epoch [117/300], Loss: 31.0955\n",
      "Epoch [118/300], Loss: 31.0451\n",
      "Epoch [119/300], Loss: 30.9954\n",
      "Epoch [120/300], Loss: 30.9461\n",
      "Epoch [121/300], Loss: 30.8973\n",
      "Epoch [122/300], Loss: 30.8490\n",
      "Epoch [123/300], Loss: 30.8011\n",
      "Epoch [124/300], Loss: 30.7538\n",
      "Epoch [125/300], Loss: 30.7071\n",
      "Epoch [126/300], Loss: 30.6611\n",
      "Epoch [127/300], Loss: 30.6155\n",
      "Epoch [128/300], Loss: 30.5705\n",
      "Epoch [129/300], Loss: 30.5262\n",
      "Epoch [130/300], Loss: 30.4824\n",
      "Epoch [131/300], Loss: 30.4394\n",
      "Epoch [132/300], Loss: 30.3967\n",
      "Epoch [133/300], Loss: 30.3548\n",
      "Epoch [134/300], Loss: 30.3134\n",
      "Epoch [135/300], Loss: 30.2728\n",
      "Epoch [136/300], Loss: 30.2327\n",
      "Epoch [137/300], Loss: 30.1933\n",
      "Epoch [138/300], Loss: 30.1543\n",
      "Epoch [139/300], Loss: 30.1162\n",
      "Epoch [140/300], Loss: 30.0783\n",
      "Epoch [141/300], Loss: 30.0415\n",
      "Epoch [142/300], Loss: 30.0048\n",
      "Epoch [143/300], Loss: 29.9687\n",
      "Epoch [144/300], Loss: 29.9332\n",
      "Epoch [145/300], Loss: 29.8982\n",
      "Epoch [146/300], Loss: 29.8639\n",
      "Epoch [147/300], Loss: 29.8300\n",
      "Epoch [148/300], Loss: 29.7964\n",
      "Epoch [149/300], Loss: 29.7634\n",
      "Epoch [150/300], Loss: 29.7310\n",
      "Epoch [151/300], Loss: 29.6988\n",
      "Epoch [152/300], Loss: 29.6671\n",
      "Epoch [153/300], Loss: 29.6358\n",
      "Epoch [154/300], Loss: 29.6049\n",
      "Epoch [155/300], Loss: 29.5745\n",
      "Epoch [156/300], Loss: 29.5444\n",
      "Epoch [157/300], Loss: 29.5145\n",
      "Epoch [158/300], Loss: 29.4851\n",
      "Epoch [159/300], Loss: 29.4560\n",
      "Epoch [160/300], Loss: 29.4273\n",
      "Epoch [161/300], Loss: 29.3988\n",
      "Epoch [162/300], Loss: 29.3708\n",
      "Epoch [163/300], Loss: 29.3429\n",
      "Epoch [164/300], Loss: 29.3151\n",
      "Epoch [165/300], Loss: 29.2880\n",
      "Epoch [166/300], Loss: 29.2607\n",
      "Epoch [167/300], Loss: 29.2339\n",
      "Epoch [168/300], Loss: 29.2073\n",
      "Epoch [169/300], Loss: 29.1810\n",
      "Epoch [170/300], Loss: 29.1548\n",
      "Epoch [171/300], Loss: 29.1287\n",
      "Epoch [172/300], Loss: 29.1029\n",
      "Epoch [173/300], Loss: 29.0773\n",
      "Epoch [174/300], Loss: 29.0519\n",
      "Epoch [175/300], Loss: 29.0267\n",
      "Epoch [176/300], Loss: 29.0016\n",
      "Epoch [177/300], Loss: 28.9769\n",
      "Epoch [178/300], Loss: 28.9521\n",
      "Epoch [179/300], Loss: 28.9273\n",
      "Epoch [180/300], Loss: 28.9028\n",
      "Epoch [181/300], Loss: 28.8788\n",
      "Epoch [182/300], Loss: 28.8543\n",
      "Epoch [183/300], Loss: 28.8303\n",
      "Epoch [184/300], Loss: 28.8065\n",
      "Epoch [185/300], Loss: 28.7826\n",
      "Epoch [186/300], Loss: 28.7589\n",
      "Epoch [187/300], Loss: 28.7352\n",
      "Epoch [188/300], Loss: 28.7116\n",
      "Epoch [189/300], Loss: 28.6885\n",
      "Epoch [190/300], Loss: 28.6649\n",
      "Epoch [191/300], Loss: 28.6415\n",
      "Epoch [192/300], Loss: 28.6185\n",
      "Epoch [193/300], Loss: 28.5953\n",
      "Epoch [194/300], Loss: 28.5723\n",
      "Epoch [195/300], Loss: 28.5496\n",
      "Epoch [196/300], Loss: 28.5266\n",
      "Epoch [197/300], Loss: 28.5036\n",
      "Epoch [198/300], Loss: 28.4809\n",
      "Epoch [199/300], Loss: 28.4583\n",
      "Epoch [200/300], Loss: 28.4357\n",
      "Epoch [201/300], Loss: 28.4130\n",
      "Validation Loss: 0.0319\n",
      "Epoch [202/300], Loss: 28.3907\n",
      "Epoch [203/300], Loss: 28.3679\n",
      "Epoch [204/300], Loss: 28.3455\n",
      "Epoch [205/300], Loss: 28.3233\n",
      "Epoch [206/300], Loss: 28.3011\n",
      "Epoch [207/300], Loss: 28.2785\n",
      "Epoch [208/300], Loss: 28.2565\n",
      "Epoch [209/300], Loss: 28.2343\n",
      "Epoch [210/300], Loss: 28.2122\n",
      "Epoch [211/300], Loss: 28.1901\n",
      "Epoch [212/300], Loss: 28.1682\n",
      "Epoch [213/300], Loss: 28.1461\n",
      "Epoch [214/300], Loss: 28.1242\n",
      "Epoch [215/300], Loss: 28.1020\n",
      "Epoch [216/300], Loss: 28.0802\n",
      "Epoch [217/300], Loss: 28.0586\n",
      "Epoch [218/300], Loss: 28.0366\n",
      "Epoch [219/300], Loss: 28.0151\n",
      "Epoch [220/300], Loss: 27.9932\n",
      "Epoch [221/300], Loss: 27.9717\n",
      "Epoch [222/300], Loss: 27.9498\n",
      "Epoch [223/300], Loss: 27.9284\n",
      "Epoch [224/300], Loss: 27.9067\n",
      "Epoch [225/300], Loss: 27.8852\n",
      "Epoch [226/300], Loss: 27.8637\n",
      "Epoch [227/300], Loss: 27.8424\n",
      "Epoch [228/300], Loss: 27.8210\n",
      "Epoch [229/300], Loss: 27.7996\n",
      "Epoch [230/300], Loss: 27.7782\n",
      "Epoch [231/300], Loss: 27.7570\n",
      "Epoch [232/300], Loss: 27.7357\n",
      "Epoch [233/300], Loss: 27.7144\n",
      "Epoch [234/300], Loss: 27.6932\n",
      "Epoch [235/300], Loss: 27.6721\n",
      "Epoch [236/300], Loss: 27.6510\n",
      "Epoch [237/300], Loss: 27.6298\n",
      "Epoch [238/300], Loss: 27.6088\n",
      "Epoch [239/300], Loss: 27.5880\n",
      "Epoch [240/300], Loss: 27.5669\n",
      "Epoch [241/300], Loss: 27.5460\n",
      "Epoch [242/300], Loss: 27.5250\n",
      "Epoch [243/300], Loss: 27.5043\n",
      "Epoch [244/300], Loss: 27.4833\n",
      "Epoch [245/300], Loss: 27.4626\n",
      "Epoch [246/300], Loss: 27.4419\n",
      "Epoch [247/300], Loss: 27.4211\n",
      "Epoch [248/300], Loss: 27.4005\n",
      "Epoch [249/300], Loss: 27.3798\n",
      "Epoch [250/300], Loss: 27.3593\n",
      "Epoch [251/300], Loss: 27.3385\n",
      "Epoch [252/300], Loss: 27.3181\n",
      "Epoch [253/300], Loss: 27.2975\n",
      "Epoch [254/300], Loss: 27.2770\n",
      "Epoch [255/300], Loss: 27.2565\n",
      "Epoch [256/300], Loss: 27.2360\n",
      "Epoch [257/300], Loss: 27.2158\n",
      "Epoch [258/300], Loss: 27.1952\n",
      "Epoch [259/300], Loss: 27.1750\n",
      "Epoch [260/300], Loss: 27.1547\n",
      "Epoch [261/300], Loss: 27.1343\n",
      "Epoch [262/300], Loss: 27.1143\n",
      "Epoch [263/300], Loss: 27.0940\n",
      "Epoch [264/300], Loss: 27.0738\n",
      "Epoch [265/300], Loss: 27.0537\n",
      "Epoch [266/300], Loss: 27.0337\n",
      "Epoch [267/300], Loss: 27.0133\n",
      "Epoch [268/300], Loss: 26.9932\n",
      "Epoch [269/300], Loss: 26.9732\n",
      "Epoch [270/300], Loss: 26.9533\n",
      "Epoch [271/300], Loss: 26.9332\n",
      "Epoch [272/300], Loss: 26.9133\n",
      "Epoch [273/300], Loss: 26.8933\n",
      "Epoch [274/300], Loss: 26.8735\n",
      "Epoch [275/300], Loss: 26.8536\n",
      "Epoch [276/300], Loss: 26.8338\n",
      "Epoch [277/300], Loss: 26.8139\n",
      "Epoch [278/300], Loss: 26.7944\n",
      "Epoch [279/300], Loss: 26.7744\n",
      "Epoch [280/300], Loss: 26.7548\n",
      "Epoch [281/300], Loss: 26.7350\n",
      "Epoch [282/300], Loss: 26.7155\n",
      "Epoch [283/300], Loss: 26.6957\n",
      "Epoch [284/300], Loss: 26.6762\n",
      "Epoch [285/300], Loss: 26.6568\n",
      "Epoch [286/300], Loss: 26.6371\n",
      "Epoch [287/300], Loss: 26.6176\n",
      "Epoch [288/300], Loss: 26.5980\n",
      "Epoch [289/300], Loss: 26.5788\n",
      "Epoch [290/300], Loss: 26.5592\n",
      "Epoch [291/300], Loss: 26.5398\n",
      "Epoch [292/300], Loss: 26.5205\n",
      "Epoch [293/300], Loss: 26.5013\n",
      "Epoch [294/300], Loss: 26.4819\n",
      "Epoch [295/300], Loss: 26.4627\n",
      "Epoch [296/300], Loss: 26.4433\n",
      "Epoch [297/300], Loss: 26.4242\n",
      "Epoch [298/300], Loss: 26.4050\n",
      "Epoch [299/300], Loss: 26.3858\n",
      "Epoch [300/300], Loss: 26.3667\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.031456035 Test: 0.029555688\n",
      "Epoch [1/300], Loss: 193.4029\n",
      "Validation Loss: 0.1458\n",
      "Epoch [2/300], Loss: 94.5145\n",
      "Epoch [3/300], Loss: 73.1968\n",
      "Epoch [4/300], Loss: 61.6294\n",
      "Epoch [5/300], Loss: 55.4929\n",
      "Epoch [6/300], Loss: 51.7961\n",
      "Epoch [7/300], Loss: 48.8907\n",
      "Epoch [8/300], Loss: 46.3495\n",
      "Epoch [9/300], Loss: 44.0940\n",
      "Epoch [10/300], Loss: 42.1185\n",
      "Epoch [11/300], Loss: 40.4202\n",
      "Epoch [12/300], Loss: 38.9858\n",
      "Epoch [13/300], Loss: 37.7916\n",
      "Epoch [14/300], Loss: 36.8070\n",
      "Epoch [15/300], Loss: 35.9973\n",
      "Epoch [16/300], Loss: 35.3284\n",
      "Epoch [17/300], Loss: 34.7690\n",
      "Epoch [18/300], Loss: 34.2934\n",
      "Epoch [19/300], Loss: 33.8814\n",
      "Epoch [20/300], Loss: 33.5178\n",
      "Epoch [21/300], Loss: 33.1906\n",
      "Epoch [22/300], Loss: 32.8915\n",
      "Epoch [23/300], Loss: 32.6137\n",
      "Epoch [24/300], Loss: 32.3525\n",
      "Epoch [25/300], Loss: 32.1042\n",
      "Epoch [26/300], Loss: 31.8663\n",
      "Epoch [27/300], Loss: 31.6368\n",
      "Epoch [28/300], Loss: 31.4146\n",
      "Epoch [29/300], Loss: 31.1989\n",
      "Epoch [30/300], Loss: 30.9897\n",
      "Epoch [31/300], Loss: 30.7867\n",
      "Epoch [32/300], Loss: 30.5900\n",
      "Epoch [33/300], Loss: 30.3996\n",
      "Epoch [34/300], Loss: 30.2158\n",
      "Epoch [35/300], Loss: 30.0385\n",
      "Epoch [36/300], Loss: 29.8681\n",
      "Epoch [37/300], Loss: 29.7044\n",
      "Epoch [38/300], Loss: 29.5474\n",
      "Epoch [39/300], Loss: 29.3972\n",
      "Epoch [40/300], Loss: 29.2538\n",
      "Epoch [41/300], Loss: 29.1171\n",
      "Epoch [42/300], Loss: 28.9871\n",
      "Epoch [43/300], Loss: 28.8633\n",
      "Epoch [44/300], Loss: 28.7459\n",
      "Epoch [45/300], Loss: 28.6343\n",
      "Epoch [46/300], Loss: 28.5282\n",
      "Epoch [47/300], Loss: 28.4275\n",
      "Epoch [48/300], Loss: 28.3316\n",
      "Epoch [49/300], Loss: 28.2402\n",
      "Epoch [50/300], Loss: 28.1527\n",
      "Epoch [51/300], Loss: 28.0687\n",
      "Epoch [52/300], Loss: 27.9881\n",
      "Epoch [53/300], Loss: 27.9102\n",
      "Epoch [54/300], Loss: 27.8346\n",
      "Epoch [55/300], Loss: 27.7612\n",
      "Epoch [56/300], Loss: 27.6895\n",
      "Epoch [57/300], Loss: 27.6195\n",
      "Epoch [58/300], Loss: 27.5509\n",
      "Epoch [59/300], Loss: 27.4834\n",
      "Epoch [60/300], Loss: 27.4169\n",
      "Epoch [61/300], Loss: 27.3513\n",
      "Epoch [62/300], Loss: 27.2865\n",
      "Epoch [63/300], Loss: 27.2223\n",
      "Epoch [64/300], Loss: 27.1586\n",
      "Epoch [65/300], Loss: 27.0955\n",
      "Epoch [66/300], Loss: 27.0331\n",
      "Epoch [67/300], Loss: 26.9711\n",
      "Epoch [68/300], Loss: 26.9096\n",
      "Epoch [69/300], Loss: 26.8484\n",
      "Epoch [70/300], Loss: 26.7878\n",
      "Epoch [71/300], Loss: 26.7277\n",
      "Epoch [72/300], Loss: 26.6681\n",
      "Epoch [73/300], Loss: 26.6089\n",
      "Epoch [74/300], Loss: 26.5502\n",
      "Epoch [75/300], Loss: 26.4921\n",
      "Epoch [76/300], Loss: 26.4343\n",
      "Epoch [77/300], Loss: 26.3772\n",
      "Epoch [78/300], Loss: 26.3204\n",
      "Epoch [79/300], Loss: 26.2642\n",
      "Epoch [80/300], Loss: 26.2083\n",
      "Epoch [81/300], Loss: 26.1532\n",
      "Epoch [82/300], Loss: 26.0984\n",
      "Epoch [83/300], Loss: 26.0438\n",
      "Epoch [84/300], Loss: 25.9901\n",
      "Epoch [85/300], Loss: 25.9365\n",
      "Epoch [86/300], Loss: 25.8835\n",
      "Epoch [87/300], Loss: 25.8309\n",
      "Epoch [88/300], Loss: 25.7787\n",
      "Epoch [89/300], Loss: 25.7268\n",
      "Epoch [90/300], Loss: 25.6753\n",
      "Epoch [91/300], Loss: 25.6243\n",
      "Epoch [92/300], Loss: 25.5737\n",
      "Epoch [93/300], Loss: 25.5235\n",
      "Epoch [94/300], Loss: 25.4731\n",
      "Epoch [95/300], Loss: 25.4235\n",
      "Epoch [96/300], Loss: 25.3742\n",
      "Epoch [97/300], Loss: 25.3253\n",
      "Epoch [98/300], Loss: 25.2767\n",
      "Epoch [99/300], Loss: 25.2283\n",
      "Epoch [100/300], Loss: 25.1805\n",
      "Epoch [101/300], Loss: 25.1330\n",
      "Validation Loss: 0.0303\n",
      "Epoch [102/300], Loss: 25.0857\n",
      "Epoch [103/300], Loss: 25.0388\n",
      "Epoch [104/300], Loss: 24.9923\n",
      "Epoch [105/300], Loss: 24.9462\n",
      "Epoch [106/300], Loss: 24.9004\n",
      "Epoch [107/300], Loss: 24.8548\n",
      "Epoch [108/300], Loss: 24.8099\n",
      "Epoch [109/300], Loss: 24.7652\n",
      "Epoch [110/300], Loss: 24.7210\n",
      "Epoch [111/300], Loss: 24.6769\n",
      "Epoch [112/300], Loss: 24.6334\n",
      "Epoch [113/300], Loss: 24.5901\n",
      "Epoch [114/300], Loss: 24.5473\n",
      "Epoch [115/300], Loss: 24.5050\n",
      "Epoch [116/300], Loss: 24.4629\n",
      "Epoch [117/300], Loss: 24.4210\n",
      "Epoch [118/300], Loss: 24.3796\n",
      "Epoch [119/300], Loss: 24.3387\n",
      "Epoch [120/300], Loss: 24.2980\n",
      "Epoch [121/300], Loss: 24.2574\n",
      "Epoch [122/300], Loss: 24.2174\n",
      "Epoch [123/300], Loss: 24.1778\n",
      "Epoch [124/300], Loss: 24.1385\n",
      "Epoch [125/300], Loss: 24.0991\n",
      "Epoch [126/300], Loss: 24.0604\n",
      "Epoch [127/300], Loss: 24.0221\n",
      "Epoch [128/300], Loss: 23.9839\n",
      "Epoch [129/300], Loss: 23.9460\n",
      "Epoch [130/300], Loss: 23.9085\n",
      "Epoch [131/300], Loss: 23.8712\n",
      "Epoch [132/300], Loss: 23.8341\n",
      "Epoch [133/300], Loss: 23.7973\n",
      "Epoch [134/300], Loss: 23.7608\n",
      "Epoch [135/300], Loss: 23.7245\n",
      "Epoch [136/300], Loss: 23.6885\n",
      "Epoch [137/300], Loss: 23.6526\n",
      "Epoch [138/300], Loss: 23.6172\n",
      "Epoch [139/300], Loss: 23.5821\n",
      "Epoch [140/300], Loss: 23.5469\n",
      "Epoch [141/300], Loss: 23.5119\n",
      "Epoch [142/300], Loss: 23.4773\n",
      "Epoch [143/300], Loss: 23.4429\n",
      "Epoch [144/300], Loss: 23.4087\n",
      "Epoch [145/300], Loss: 23.3743\n",
      "Epoch [146/300], Loss: 23.3406\n",
      "Epoch [147/300], Loss: 23.3069\n",
      "Epoch [148/300], Loss: 23.2735\n",
      "Epoch [149/300], Loss: 23.2399\n",
      "Epoch [150/300], Loss: 23.2068\n",
      "Epoch [151/300], Loss: 23.1739\n",
      "Epoch [152/300], Loss: 23.1411\n",
      "Epoch [153/300], Loss: 23.1084\n",
      "Epoch [154/300], Loss: 23.0757\n",
      "Epoch [155/300], Loss: 23.0433\n",
      "Epoch [156/300], Loss: 23.0111\n",
      "Epoch [157/300], Loss: 22.9790\n",
      "Epoch [158/300], Loss: 22.9468\n",
      "Epoch [159/300], Loss: 22.9151\n",
      "Epoch [160/300], Loss: 22.8834\n",
      "Epoch [161/300], Loss: 22.8518\n",
      "Epoch [162/300], Loss: 22.8205\n",
      "Epoch [163/300], Loss: 22.7889\n",
      "Epoch [164/300], Loss: 22.7579\n",
      "Epoch [165/300], Loss: 22.7267\n",
      "Epoch [166/300], Loss: 22.6956\n",
      "Epoch [167/300], Loss: 22.6649\n",
      "Epoch [168/300], Loss: 22.6340\n",
      "Epoch [169/300], Loss: 22.6032\n",
      "Epoch [170/300], Loss: 22.5728\n",
      "Epoch [171/300], Loss: 22.5425\n",
      "Epoch [172/300], Loss: 22.5123\n",
      "Epoch [173/300], Loss: 22.4822\n",
      "Epoch [174/300], Loss: 22.4522\n",
      "Epoch [175/300], Loss: 22.4224\n",
      "Epoch [176/300], Loss: 22.3926\n",
      "Epoch [177/300], Loss: 22.3628\n",
      "Epoch [178/300], Loss: 22.3333\n",
      "Epoch [179/300], Loss: 22.3038\n",
      "Epoch [180/300], Loss: 22.2748\n",
      "Epoch [181/300], Loss: 22.2458\n",
      "Epoch [182/300], Loss: 22.2169\n",
      "Epoch [183/300], Loss: 22.1879\n",
      "Epoch [184/300], Loss: 22.1596\n",
      "Epoch [185/300], Loss: 22.1309\n",
      "Epoch [186/300], Loss: 22.1026\n",
      "Epoch [187/300], Loss: 22.0745\n",
      "Epoch [188/300], Loss: 22.0467\n",
      "Epoch [189/300], Loss: 22.0188\n",
      "Epoch [190/300], Loss: 21.9912\n",
      "Epoch [191/300], Loss: 21.9638\n",
      "Epoch [192/300], Loss: 21.9365\n",
      "Epoch [193/300], Loss: 21.9092\n",
      "Epoch [194/300], Loss: 21.8824\n",
      "Epoch [195/300], Loss: 21.8556\n",
      "Epoch [196/300], Loss: 21.8289\n",
      "Epoch [197/300], Loss: 21.8027\n",
      "Epoch [198/300], Loss: 21.7764\n",
      "Epoch [199/300], Loss: 21.7502\n",
      "Epoch [200/300], Loss: 21.7245\n",
      "Epoch [201/300], Loss: 21.6985\n",
      "Validation Loss: 0.0263\n",
      "Epoch [202/300], Loss: 21.6732\n",
      "Epoch [203/300], Loss: 21.6479\n",
      "Epoch [204/300], Loss: 21.6225\n",
      "Epoch [205/300], Loss: 21.5976\n",
      "Epoch [206/300], Loss: 21.5725\n",
      "Epoch [207/300], Loss: 21.5478\n",
      "Epoch [208/300], Loss: 21.5232\n",
      "Epoch [209/300], Loss: 21.4988\n",
      "Epoch [210/300], Loss: 21.4744\n",
      "Epoch [211/300], Loss: 21.4502\n",
      "Epoch [212/300], Loss: 21.4262\n",
      "Epoch [213/300], Loss: 21.4022\n",
      "Epoch [214/300], Loss: 21.3784\n",
      "Epoch [215/300], Loss: 21.3551\n",
      "Epoch [216/300], Loss: 21.3312\n",
      "Epoch [217/300], Loss: 21.3079\n",
      "Epoch [218/300], Loss: 21.2847\n",
      "Epoch [219/300], Loss: 21.2615\n",
      "Epoch [220/300], Loss: 21.2383\n",
      "Epoch [221/300], Loss: 21.2151\n",
      "Epoch [222/300], Loss: 21.1926\n",
      "Epoch [223/300], Loss: 21.1697\n",
      "Epoch [224/300], Loss: 21.1471\n",
      "Epoch [225/300], Loss: 21.1247\n",
      "Epoch [226/300], Loss: 21.1018\n",
      "Epoch [227/300], Loss: 21.0797\n",
      "Epoch [228/300], Loss: 21.0572\n",
      "Epoch [229/300], Loss: 21.0350\n",
      "Epoch [230/300], Loss: 21.0131\n",
      "Epoch [231/300], Loss: 20.9911\n",
      "Epoch [232/300], Loss: 20.9693\n",
      "Epoch [233/300], Loss: 20.9474\n",
      "Epoch [234/300], Loss: 20.9259\n",
      "Epoch [235/300], Loss: 20.9039\n",
      "Epoch [236/300], Loss: 20.8823\n",
      "Epoch [237/300], Loss: 20.8610\n",
      "Epoch [238/300], Loss: 20.8396\n",
      "Epoch [239/300], Loss: 20.8181\n",
      "Epoch [240/300], Loss: 20.7972\n",
      "Epoch [241/300], Loss: 20.7759\n",
      "Epoch [242/300], Loss: 20.7548\n",
      "Epoch [243/300], Loss: 20.7338\n",
      "Epoch [244/300], Loss: 20.7134\n",
      "Epoch [245/300], Loss: 20.6923\n",
      "Epoch [246/300], Loss: 20.6714\n",
      "Epoch [247/300], Loss: 20.6513\n",
      "Epoch [248/300], Loss: 20.6307\n",
      "Epoch [249/300], Loss: 20.6101\n",
      "Epoch [250/300], Loss: 20.5898\n",
      "Epoch [251/300], Loss: 20.5695\n",
      "Epoch [252/300], Loss: 20.5495\n",
      "Epoch [253/300], Loss: 20.5296\n",
      "Epoch [254/300], Loss: 20.5097\n",
      "Epoch [255/300], Loss: 20.4899\n",
      "Epoch [256/300], Loss: 20.4699\n",
      "Epoch [257/300], Loss: 20.4502\n",
      "Epoch [258/300], Loss: 20.4307\n",
      "Epoch [259/300], Loss: 20.4112\n",
      "Epoch [260/300], Loss: 20.3919\n",
      "Epoch [261/300], Loss: 20.3729\n",
      "Epoch [262/300], Loss: 20.3531\n",
      "Epoch [263/300], Loss: 20.3344\n",
      "Epoch [264/300], Loss: 20.3152\n",
      "Epoch [265/300], Loss: 20.2965\n",
      "Epoch [266/300], Loss: 20.2775\n",
      "Epoch [267/300], Loss: 20.2585\n",
      "Epoch [268/300], Loss: 20.2402\n",
      "Epoch [269/300], Loss: 20.2213\n",
      "Epoch [270/300], Loss: 20.2027\n",
      "Epoch [271/300], Loss: 20.1844\n",
      "Epoch [272/300], Loss: 20.1661\n",
      "Epoch [273/300], Loss: 20.1477\n",
      "Epoch [274/300], Loss: 20.1294\n",
      "Epoch [275/300], Loss: 20.1115\n",
      "Epoch [276/300], Loss: 20.0933\n",
      "Epoch [277/300], Loss: 20.0752\n",
      "Epoch [278/300], Loss: 20.0572\n",
      "Epoch [279/300], Loss: 20.0395\n",
      "Epoch [280/300], Loss: 20.0217\n",
      "Epoch [281/300], Loss: 20.0043\n",
      "Epoch [282/300], Loss: 19.9870\n",
      "Epoch [283/300], Loss: 19.9691\n",
      "Epoch [284/300], Loss: 19.9516\n",
      "Epoch [285/300], Loss: 19.9344\n",
      "Epoch [286/300], Loss: 19.9169\n",
      "Epoch [287/300], Loss: 19.8997\n",
      "Epoch [288/300], Loss: 19.8827\n",
      "Epoch [289/300], Loss: 19.8658\n",
      "Epoch [290/300], Loss: 19.8486\n",
      "Epoch [291/300], Loss: 19.8318\n",
      "Epoch [292/300], Loss: 19.8150\n",
      "Epoch [293/300], Loss: 19.7985\n",
      "Epoch [294/300], Loss: 19.7819\n",
      "Epoch [295/300], Loss: 19.7651\n",
      "Epoch [296/300], Loss: 19.7486\n",
      "Epoch [297/300], Loss: 19.7321\n",
      "Epoch [298/300], Loss: 19.7157\n",
      "Epoch [299/300], Loss: 19.6993\n",
      "Epoch [300/300], Loss: 19.6830\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.02341228 Test: 0.023887461\n",
      "Epoch [1/300], Loss: 195.6399\n",
      "Validation Loss: 0.1461\n",
      "Epoch [2/300], Loss: 93.7675\n",
      "Epoch [3/300], Loss: 72.4863\n",
      "Epoch [4/300], Loss: 61.5417\n",
      "Epoch [5/300], Loss: 55.7570\n",
      "Epoch [6/300], Loss: 52.2421\n",
      "Epoch [7/300], Loss: 49.5286\n",
      "Epoch [8/300], Loss: 47.1947\n",
      "Epoch [9/300], Loss: 45.1141\n",
      "Epoch [10/300], Loss: 43.2387\n",
      "Epoch [11/300], Loss: 41.5497\n",
      "Epoch [12/300], Loss: 40.0440\n",
      "Epoch [13/300], Loss: 38.7255\n",
      "Epoch [14/300], Loss: 37.5965\n",
      "Epoch [15/300], Loss: 36.6506\n",
      "Epoch [16/300], Loss: 35.8702\n",
      "Epoch [17/300], Loss: 35.2289\n",
      "Epoch [18/300], Loss: 34.6960\n",
      "Epoch [19/300], Loss: 34.2430\n",
      "Epoch [20/300], Loss: 33.8472\n",
      "Epoch [21/300], Loss: 33.4918\n",
      "Epoch [22/300], Loss: 33.1661\n",
      "Epoch [23/300], Loss: 32.8628\n",
      "Epoch [24/300], Loss: 32.5772\n",
      "Epoch [25/300], Loss: 32.3064\n",
      "Epoch [26/300], Loss: 32.0484\n",
      "Epoch [27/300], Loss: 31.8021\n",
      "Epoch [28/300], Loss: 31.5662\n",
      "Epoch [29/300], Loss: 31.3403\n",
      "Epoch [30/300], Loss: 31.1237\n",
      "Epoch [31/300], Loss: 30.9159\n",
      "Epoch [32/300], Loss: 30.7165\n",
      "Epoch [33/300], Loss: 30.5251\n",
      "Epoch [34/300], Loss: 30.3414\n",
      "Epoch [35/300], Loss: 30.1651\n",
      "Epoch [36/300], Loss: 29.9962\n",
      "Epoch [37/300], Loss: 29.8340\n",
      "Epoch [38/300], Loss: 29.6788\n",
      "Epoch [39/300], Loss: 29.5302\n",
      "Epoch [40/300], Loss: 29.3883\n",
      "Epoch [41/300], Loss: 29.2528\n",
      "Epoch [42/300], Loss: 29.1236\n",
      "Epoch [43/300], Loss: 29.0005\n",
      "Epoch [44/300], Loss: 28.8834\n",
      "Epoch [45/300], Loss: 28.7722\n",
      "Epoch [46/300], Loss: 28.6663\n",
      "Epoch [47/300], Loss: 28.5658\n",
      "Epoch [48/300], Loss: 28.4704\n",
      "Epoch [49/300], Loss: 28.3796\n",
      "Epoch [50/300], Loss: 28.2932\n",
      "Epoch [51/300], Loss: 28.2110\n",
      "Epoch [52/300], Loss: 28.1326\n",
      "Epoch [53/300], Loss: 28.0576\n",
      "Epoch [54/300], Loss: 27.9860\n",
      "Epoch [55/300], Loss: 27.9173\n",
      "Epoch [56/300], Loss: 27.8515\n",
      "Epoch [57/300], Loss: 27.7880\n",
      "Epoch [58/300], Loss: 27.7271\n",
      "Epoch [59/300], Loss: 27.6679\n",
      "Epoch [60/300], Loss: 27.6108\n",
      "Epoch [61/300], Loss: 27.5554\n",
      "Epoch [62/300], Loss: 27.5017\n",
      "Epoch [63/300], Loss: 27.4495\n",
      "Epoch [64/300], Loss: 27.3987\n",
      "Epoch [65/300], Loss: 27.3489\n",
      "Epoch [66/300], Loss: 27.3005\n",
      "Epoch [67/300], Loss: 27.2531\n",
      "Epoch [68/300], Loss: 27.2067\n",
      "Epoch [69/300], Loss: 27.1615\n",
      "Epoch [70/300], Loss: 27.1169\n",
      "Epoch [71/300], Loss: 27.0731\n",
      "Epoch [72/300], Loss: 27.0300\n",
      "Epoch [73/300], Loss: 26.9877\n",
      "Epoch [74/300], Loss: 26.9463\n",
      "Epoch [75/300], Loss: 26.9055\n",
      "Epoch [76/300], Loss: 26.8648\n",
      "Epoch [77/300], Loss: 26.8251\n",
      "Epoch [78/300], Loss: 26.7860\n",
      "Epoch [79/300], Loss: 26.7474\n",
      "Epoch [80/300], Loss: 26.7089\n",
      "Epoch [81/300], Loss: 26.6711\n",
      "Epoch [82/300], Loss: 26.6337\n",
      "Epoch [83/300], Loss: 26.5967\n",
      "Epoch [84/300], Loss: 26.5602\n",
      "Epoch [85/300], Loss: 26.5241\n",
      "Epoch [86/300], Loss: 26.4882\n",
      "Epoch [87/300], Loss: 26.4527\n",
      "Epoch [88/300], Loss: 26.4174\n",
      "Epoch [89/300], Loss: 26.3826\n",
      "Epoch [90/300], Loss: 26.3480\n",
      "Epoch [91/300], Loss: 26.3136\n",
      "Epoch [92/300], Loss: 26.2796\n",
      "Epoch [93/300], Loss: 26.2456\n",
      "Epoch [94/300], Loss: 26.2121\n",
      "Epoch [95/300], Loss: 26.1786\n",
      "Epoch [96/300], Loss: 26.1453\n",
      "Epoch [97/300], Loss: 26.1124\n",
      "Epoch [98/300], Loss: 26.0795\n",
      "Epoch [99/300], Loss: 26.0468\n",
      "Epoch [100/300], Loss: 26.0141\n",
      "Epoch [101/300], Loss: 25.9819\n",
      "Validation Loss: 0.0314\n",
      "Epoch [102/300], Loss: 25.9495\n",
      "Epoch [103/300], Loss: 25.9174\n",
      "Epoch [104/300], Loss: 25.8854\n",
      "Epoch [105/300], Loss: 25.8533\n",
      "Epoch [106/300], Loss: 25.8213\n",
      "Epoch [107/300], Loss: 25.7897\n",
      "Epoch [108/300], Loss: 25.7579\n",
      "Epoch [109/300], Loss: 25.7261\n",
      "Epoch [110/300], Loss: 25.6944\n",
      "Epoch [111/300], Loss: 25.6629\n",
      "Epoch [112/300], Loss: 25.6314\n",
      "Epoch [113/300], Loss: 25.5998\n",
      "Epoch [114/300], Loss: 25.5683\n",
      "Epoch [115/300], Loss: 25.5367\n",
      "Epoch [116/300], Loss: 25.5051\n",
      "Epoch [117/300], Loss: 25.4736\n",
      "Epoch [118/300], Loss: 25.4422\n",
      "Epoch [119/300], Loss: 25.4104\n",
      "Epoch [120/300], Loss: 25.3785\n",
      "Epoch [121/300], Loss: 25.3470\n",
      "Epoch [122/300], Loss: 25.3151\n",
      "Epoch [123/300], Loss: 25.2833\n",
      "Epoch [124/300], Loss: 25.2515\n",
      "Epoch [125/300], Loss: 25.2194\n",
      "Epoch [126/300], Loss: 25.1874\n",
      "Epoch [127/300], Loss: 25.1551\n",
      "Epoch [128/300], Loss: 25.1228\n",
      "Epoch [129/300], Loss: 25.0906\n",
      "Epoch [130/300], Loss: 25.0580\n",
      "Epoch [131/300], Loss: 25.0256\n",
      "Epoch [132/300], Loss: 24.9929\n",
      "Epoch [133/300], Loss: 24.9599\n",
      "Epoch [134/300], Loss: 24.9269\n",
      "Epoch [135/300], Loss: 24.8940\n",
      "Epoch [136/300], Loss: 24.8607\n",
      "Epoch [137/300], Loss: 24.8274\n",
      "Epoch [138/300], Loss: 24.7940\n",
      "Epoch [139/300], Loss: 24.7602\n",
      "Epoch [140/300], Loss: 24.7265\n",
      "Epoch [141/300], Loss: 24.6931\n",
      "Epoch [142/300], Loss: 24.6588\n",
      "Epoch [143/300], Loss: 24.6248\n",
      "Epoch [144/300], Loss: 24.5901\n",
      "Epoch [145/300], Loss: 24.5560\n",
      "Epoch [146/300], Loss: 24.5213\n",
      "Epoch [147/300], Loss: 24.4867\n",
      "Epoch [148/300], Loss: 24.4518\n",
      "Epoch [149/300], Loss: 24.4169\n",
      "Epoch [150/300], Loss: 24.3817\n",
      "Epoch [151/300], Loss: 24.3467\n",
      "Epoch [152/300], Loss: 24.3113\n",
      "Epoch [153/300], Loss: 24.2759\n",
      "Epoch [154/300], Loss: 24.2402\n",
      "Epoch [155/300], Loss: 24.2048\n",
      "Epoch [156/300], Loss: 24.1690\n",
      "Epoch [157/300], Loss: 24.1332\n",
      "Epoch [158/300], Loss: 24.0971\n",
      "Epoch [159/300], Loss: 24.0613\n",
      "Epoch [160/300], Loss: 24.0251\n",
      "Epoch [161/300], Loss: 23.9887\n",
      "Epoch [162/300], Loss: 23.9527\n",
      "Epoch [163/300], Loss: 23.9165\n",
      "Epoch [164/300], Loss: 23.8801\n",
      "Epoch [165/300], Loss: 23.8437\n",
      "Epoch [166/300], Loss: 23.8075\n",
      "Epoch [167/300], Loss: 23.7710\n",
      "Epoch [168/300], Loss: 23.7345\n",
      "Epoch [169/300], Loss: 23.6980\n",
      "Epoch [170/300], Loss: 23.6615\n",
      "Epoch [171/300], Loss: 23.6252\n",
      "Epoch [172/300], Loss: 23.5887\n",
      "Epoch [173/300], Loss: 23.5524\n",
      "Epoch [174/300], Loss: 23.5158\n",
      "Epoch [175/300], Loss: 23.4794\n",
      "Epoch [176/300], Loss: 23.4431\n",
      "Epoch [177/300], Loss: 23.4069\n",
      "Epoch [178/300], Loss: 23.3704\n",
      "Epoch [179/300], Loss: 23.3344\n",
      "Epoch [180/300], Loss: 23.2982\n",
      "Epoch [181/300], Loss: 23.2623\n",
      "Epoch [182/300], Loss: 23.2258\n",
      "Epoch [183/300], Loss: 23.1899\n",
      "Epoch [184/300], Loss: 23.1543\n",
      "Epoch [185/300], Loss: 23.1183\n",
      "Epoch [186/300], Loss: 23.0825\n",
      "Epoch [187/300], Loss: 23.0469\n",
      "Epoch [188/300], Loss: 23.0113\n",
      "Epoch [189/300], Loss: 22.9758\n",
      "Epoch [190/300], Loss: 22.9404\n",
      "Epoch [191/300], Loss: 22.9052\n",
      "Epoch [192/300], Loss: 22.8698\n",
      "Epoch [193/300], Loss: 22.8348\n",
      "Epoch [194/300], Loss: 22.7999\n",
      "Epoch [195/300], Loss: 22.7650\n",
      "Epoch [196/300], Loss: 22.7303\n",
      "Epoch [197/300], Loss: 22.6954\n",
      "Epoch [198/300], Loss: 22.6611\n",
      "Epoch [199/300], Loss: 22.6267\n",
      "Epoch [200/300], Loss: 22.5922\n",
      "Epoch [201/300], Loss: 22.5585\n",
      "Validation Loss: 0.0273\n",
      "Epoch [202/300], Loss: 22.5237\n",
      "Epoch [203/300], Loss: 22.4900\n",
      "Epoch [204/300], Loss: 22.4563\n",
      "Epoch [205/300], Loss: 22.4222\n",
      "Epoch [206/300], Loss: 22.3888\n",
      "Epoch [207/300], Loss: 22.3553\n",
      "Epoch [208/300], Loss: 22.3220\n",
      "Epoch [209/300], Loss: 22.2885\n",
      "Epoch [210/300], Loss: 22.2554\n",
      "Epoch [211/300], Loss: 22.2227\n",
      "Epoch [212/300], Loss: 22.1899\n",
      "Epoch [213/300], Loss: 22.1570\n",
      "Epoch [214/300], Loss: 22.1245\n",
      "Epoch [215/300], Loss: 22.0920\n",
      "Epoch [216/300], Loss: 22.0595\n",
      "Epoch [217/300], Loss: 22.0276\n",
      "Epoch [218/300], Loss: 21.9953\n",
      "Epoch [219/300], Loss: 21.9635\n",
      "Epoch [220/300], Loss: 21.9314\n",
      "Epoch [221/300], Loss: 21.8998\n",
      "Epoch [222/300], Loss: 21.8684\n",
      "Epoch [223/300], Loss: 21.8370\n",
      "Epoch [224/300], Loss: 21.8056\n",
      "Epoch [225/300], Loss: 21.7745\n",
      "Epoch [226/300], Loss: 21.7437\n",
      "Epoch [227/300], Loss: 21.7128\n",
      "Epoch [228/300], Loss: 21.6821\n",
      "Epoch [229/300], Loss: 21.6516\n",
      "Epoch [230/300], Loss: 21.6212\n",
      "Epoch [231/300], Loss: 21.5911\n",
      "Epoch [232/300], Loss: 21.5607\n",
      "Epoch [233/300], Loss: 21.5308\n",
      "Epoch [234/300], Loss: 21.5011\n",
      "Epoch [235/300], Loss: 21.4710\n",
      "Epoch [236/300], Loss: 21.4414\n",
      "Epoch [237/300], Loss: 21.4118\n",
      "Epoch [238/300], Loss: 21.3827\n",
      "Epoch [239/300], Loss: 21.3537\n",
      "Epoch [240/300], Loss: 21.3248\n",
      "Epoch [241/300], Loss: 21.2957\n",
      "Epoch [242/300], Loss: 21.2668\n",
      "Epoch [243/300], Loss: 21.2383\n",
      "Epoch [244/300], Loss: 21.2102\n",
      "Epoch [245/300], Loss: 21.1814\n",
      "Epoch [246/300], Loss: 21.1535\n",
      "Epoch [247/300], Loss: 21.1255\n",
      "Epoch [248/300], Loss: 21.0971\n",
      "Epoch [249/300], Loss: 21.0696\n",
      "Epoch [250/300], Loss: 21.0420\n",
      "Epoch [251/300], Loss: 21.0143\n",
      "Epoch [252/300], Loss: 20.9870\n",
      "Epoch [253/300], Loss: 20.9595\n",
      "Epoch [254/300], Loss: 20.9323\n",
      "Epoch [255/300], Loss: 20.9054\n",
      "Epoch [256/300], Loss: 20.8786\n",
      "Epoch [257/300], Loss: 20.8513\n",
      "Epoch [258/300], Loss: 20.8247\n",
      "Epoch [259/300], Loss: 20.7981\n",
      "Epoch [260/300], Loss: 20.7715\n",
      "Epoch [261/300], Loss: 20.7451\n",
      "Epoch [262/300], Loss: 20.7191\n",
      "Epoch [263/300], Loss: 20.6927\n",
      "Epoch [264/300], Loss: 20.6670\n",
      "Epoch [265/300], Loss: 20.6409\n",
      "Epoch [266/300], Loss: 20.6148\n",
      "Epoch [267/300], Loss: 20.5893\n",
      "Epoch [268/300], Loss: 20.5636\n",
      "Epoch [269/300], Loss: 20.5379\n",
      "Epoch [270/300], Loss: 20.5126\n",
      "Epoch [271/300], Loss: 20.4873\n",
      "Epoch [272/300], Loss: 20.4621\n",
      "Epoch [273/300], Loss: 20.4369\n",
      "Epoch [274/300], Loss: 20.4121\n",
      "Epoch [275/300], Loss: 20.3869\n",
      "Epoch [276/300], Loss: 20.3622\n",
      "Epoch [277/300], Loss: 20.3371\n",
      "Epoch [278/300], Loss: 20.3128\n",
      "Epoch [279/300], Loss: 20.2881\n",
      "Epoch [280/300], Loss: 20.2638\n",
      "Epoch [281/300], Loss: 20.2397\n",
      "Epoch [282/300], Loss: 20.2152\n",
      "Epoch [283/300], Loss: 20.1910\n",
      "Epoch [284/300], Loss: 20.1675\n",
      "Epoch [285/300], Loss: 20.1431\n",
      "Epoch [286/300], Loss: 20.1191\n",
      "Epoch [287/300], Loss: 20.0960\n",
      "Epoch [288/300], Loss: 20.0721\n",
      "Epoch [289/300], Loss: 20.0484\n",
      "Epoch [290/300], Loss: 20.0246\n",
      "Epoch [291/300], Loss: 20.0019\n",
      "Epoch [292/300], Loss: 19.9787\n",
      "Epoch [293/300], Loss: 19.9554\n",
      "Epoch [294/300], Loss: 19.9321\n",
      "Epoch [295/300], Loss: 19.9094\n",
      "Epoch [296/300], Loss: 19.8866\n",
      "Epoch [297/300], Loss: 19.8639\n",
      "Epoch [298/300], Loss: 19.8415\n",
      "Epoch [299/300], Loss: 19.8188\n",
      "Epoch [300/300], Loss: 19.7962\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.023524344 Test: 0.024015106\n",
      "Epoch [1/300], Loss: 191.6456\n",
      "Validation Loss: 0.1459\n",
      "Epoch [2/300], Loss: 94.6645\n",
      "Epoch [3/300], Loss: 73.3352\n",
      "Epoch [4/300], Loss: 61.7311\n",
      "Epoch [5/300], Loss: 55.6095\n",
      "Epoch [6/300], Loss: 51.9520\n",
      "Epoch [7/300], Loss: 49.1064\n",
      "Epoch [8/300], Loss: 46.6385\n",
      "Epoch [9/300], Loss: 44.4469\n",
      "Epoch [10/300], Loss: 42.5083\n",
      "Epoch [11/300], Loss: 40.8202\n",
      "Epoch [12/300], Loss: 39.3816\n",
      "Epoch [13/300], Loss: 38.1828\n",
      "Epoch [14/300], Loss: 37.2013\n",
      "Epoch [15/300], Loss: 36.4034\n",
      "Epoch [16/300], Loss: 35.7517\n",
      "Epoch [17/300], Loss: 35.2105\n",
      "Epoch [18/300], Loss: 34.7513\n",
      "Epoch [19/300], Loss: 34.3527\n",
      "Epoch [20/300], Loss: 33.9992\n",
      "Epoch [21/300], Loss: 33.6798\n",
      "Epoch [22/300], Loss: 33.3873\n",
      "Epoch [23/300], Loss: 33.1159\n",
      "Epoch [24/300], Loss: 32.8619\n",
      "Epoch [25/300], Loss: 32.6220\n",
      "Epoch [26/300], Loss: 32.3936\n",
      "Epoch [27/300], Loss: 32.1748\n",
      "Epoch [28/300], Loss: 31.9638\n",
      "Epoch [29/300], Loss: 31.7594\n",
      "Epoch [30/300], Loss: 31.5607\n",
      "Epoch [31/300], Loss: 31.3672\n",
      "Epoch [32/300], Loss: 31.1782\n",
      "Epoch [33/300], Loss: 30.9937\n",
      "Epoch [34/300], Loss: 30.8138\n",
      "Epoch [35/300], Loss: 30.6384\n",
      "Epoch [36/300], Loss: 30.4681\n",
      "Epoch [37/300], Loss: 30.3026\n",
      "Epoch [38/300], Loss: 30.1426\n",
      "Epoch [39/300], Loss: 29.9881\n",
      "Epoch [40/300], Loss: 29.8392\n",
      "Epoch [41/300], Loss: 29.6962\n",
      "Epoch [42/300], Loss: 29.5592\n",
      "Epoch [43/300], Loss: 29.4280\n",
      "Epoch [44/300], Loss: 29.3028\n",
      "Epoch [45/300], Loss: 29.1831\n",
      "Epoch [46/300], Loss: 29.0695\n",
      "Epoch [47/300], Loss: 28.9612\n",
      "Epoch [48/300], Loss: 28.8581\n",
      "Epoch [49/300], Loss: 28.7603\n",
      "Epoch [50/300], Loss: 28.6673\n",
      "Epoch [51/300], Loss: 28.5791\n",
      "Epoch [52/300], Loss: 28.4948\n",
      "Epoch [53/300], Loss: 28.4146\n",
      "Epoch [54/300], Loss: 28.3381\n",
      "Epoch [55/300], Loss: 28.2649\n",
      "Epoch [56/300], Loss: 28.1947\n",
      "Epoch [57/300], Loss: 28.1273\n",
      "Epoch [58/300], Loss: 28.0626\n",
      "Epoch [59/300], Loss: 28.0001\n",
      "Epoch [60/300], Loss: 27.9396\n",
      "Epoch [61/300], Loss: 27.8810\n"
     ]
    }
   ],
   "source": [
    "T=[]\n",
    "for i in range(1,50,2):\n",
    "    t=[]\n",
    "    for j in range(5):\n",
    "        t.append(runLSTM(t=i))\n",
    "    T.append(t)\n",
    "    np.save(\"/its/home/drs25/GonkRobot/Data/experimentdata/LSTM_T\",np.array(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=[]\n",
    "for i in range(1,50,2):\n",
    "    T.append(runRF(t=i))\n",
    "    np.save(\"/its/home/drs25/GonkRobot/Data/experimentdata/RF_T\",np.array(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAHHCAYAAAC7soLdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB7tElEQVR4nO3dd3hT1f8H8HdW00EHtNAySgd7T1vZKGUPWcoUZMpGEJAhS0BEFPixERniF2SpIIhIAdkFlC17tAwpm+6VcX9/HJI2tIWGJk2bvl/Pc58mNyf3fnJ623567hkySZIkEBEREdkxua0DICIiIrI2JjxERERk95jwEBERkd1jwkNERER2jwkPERER2T0mPERERGT3mPAQERGR3WPCQ0RERHaPCQ8RERHZPSY8RJQta9euhUwmQ0RExBu9//r162jWrBnc3d0hk8mwbdu2bB+TiOhlTHiIrMjwh/uff/7J8PXGjRujcuXKORxVqmnTpkEmk712a9y4sdVi6N27Ny5cuIBZs2bhxx9/RO3ata12LiLKv5S2DoCIbKdjx44oXbq08XlcXBwGDx6MDh06oGPHjsb93t7emR7jww8/RNeuXaFWq80+f2JiIsLCwjBp0iQMGzbM7PcTEWUVEx6ifKxq1aqoWrWq8fmTJ08wePBgVK1aFT179szSMRQKBRQKxRud//HjxwAADw+PN3o/ZY1er0dKSgocHR3TvRYfHw8XF5dsHT8hIQHOzs7ZOgaRtfGWFlEuo9VqMWPGDJQqVQpqtRr+/v6YOHEikpOTjWVGjx4NT09PSJJk3Dd8+HDIZDIsXLjQuO/hw4eQyWRYtmyZ1eLNqL+Nv78/2rRpgyNHjiAoKAiOjo4IDAzEunXrjGWmTZsGPz8/AMDYsWMhk8ng7++f6XlkMhmmTZuWbr+/vz8++ugjk31RUVH45JNP4OvrC7VajdKlS2POnDnQ6/XGMhEREZDJZPjmm2/w3XffGev7rbfewt9//53uPFeuXMEHH3yAwoULw8nJCeXKlcOkSZNMyvz333/o27cvvL29oVarUalSJaxevTrdsRYtWoRKlSrB2dkZBQsWRO3atbFhw4ZMP7tBcnIypk6ditKlS0OtVsPX1xfjxo0zuTYMdTVs2DCsX78elSpVglqtxu7du43fq4MHD2LIkCEoUqQISpQoYXzf0qVLjeWLFSuGoUOHIioqyuTYhtuwp06dQsOGDeHs7IyJEye+NnYiW2MLD1EOiI6OxpMnT9Lt12g06fb1798fP/zwAzp37oxPP/0UJ06cwOzZs3H58mX8+uuvAIAGDRpg/vz5uHjxorEP0OHDhyGXy3H48GGMGDHCuA8AGjZsaK2PlqkbN26gc+fO6NevH3r37o3Vq1fjo48+Qq1atVCpUiV07NgRHh4eGDVqFLp164ZWrVqhQIEC2T5vQkICGjVqhP/++w8ff/wxSpYsiWPHjmHChAmIjIzEggULTMpv2LABsbGx+PjjjyGTyfD111+jY8eOuHXrFlQqFQDg/PnzaNCgAVQqFQYOHAh/f3/cvHkTO3bswKxZswCI5PLtt982JhuFCxfGH3/8gX79+iEmJgaffPIJAGDlypUYMWIEOnfujJEjRyIpKQnnz5/HiRMn0L1790w/l16vR7t27XDkyBEMHDgQFSpUwIULFzB//nxcu3YN27ZtMym/f/9+bN68GcOGDYOXlxf8/f1x9uxZAMCQIUNQuHBhTJkyBfHx8QBEAjp9+nSEhIRg8ODBuHr1KpYtW4a///4bR48eNdYFADx9+hQtW7ZE165d0bNnz1fe8iTKNSQispo1a9ZIAF65VapUyVj+7NmzEgCpf//+JscZM2aMBEDav3+/JEmS9OjRIwmAtHTpUkmSJCkqKkqSy+XS+++/L3l7exvfN2LECKlQoUKSXq/PUryPHz+WAEhTp041+zOGh4cb9/n5+UkApEOHDhn3PXr0SFKr1dKnn35q3BceHi4BkObOnfvaY2YWl5+fn9S7d2/j8xkzZkguLi7StWvXTMqNHz9eUigU0p07d0zO7enpKT179sxYbvv27RIAaceOHcZ9DRs2lFxdXaXbt2+bHDNtvfbr108qWrSo9OTJE5MyXbt2ldzd3aWEhARJkiTpvffeM/meZ9WPP/4oyeVy6fDhwyb7ly9fLgGQjh49atwHQJLL5dLFixdNyhrqtX79+pJWqzXuf/TokeTg4CA1a9ZM0ul0xv2LFy+WAEirV6827mvUqJEEQFq+fLnZn4HIlnhLiygHLFmyBKGhoem2tP1nAGDXrl0AxC2rtD799FMAwO+//w4AKFy4MMqXL49Dhw4BAI4ePQqFQoGxY8fi4cOHuH79OgDRwlO/fn3IZDKrfr6MVKxYEQ0aNDA+L1y4MMqVK4dbt25Z9bxbtmxBgwYNULBgQTx58sS4hYSEQKfTGevMoEuXLihYsKDxuSFmQ5yPHz/GoUOH0LdvX5QsWdLkvYZ6lSQJP//8M9q2bQtJkkzO27x5c0RHR+P06dMARH+le/fuZXjb7HWfq0KFCihfvrzJ8d99910AwF9//WVSvlGjRqhYsWKGxxowYIBJv6u9e/ciJSUFn3zyCeRyuUk5Nzc343VnoFar0adPH7PiJ7I13tIiygFBQUEZDrc2/FE2uH37NuRyucnIKQDw8fGBh4cHbt++bdzXoEEDY4J0+PBh1K5dG7Vr10ahQoVw+PBheHt749y5c6+8TWJNLycHgPi8z58/t+p5r1+/jvPnz6Nw4cIZvv7o0SOT5y/HaUh+DHEaEp9XTR/w+PFjREVF4bvvvsN33333yvN+9tln2Lt3L4KCglC6dGk0a9YM3bt3R7169V77uS5fvpzlzxUQEJDpsV5+zXBdlStXzmS/g4MDAgMDTa47AChevDgcHBxeGS9RbsOEhygXykqLTP369bFy5UrcunULhw8fRoMGDSCTyVC/fn0cPnwYxYoVg16vN2llyUmZjdyS0nS0tgSdTmfyXK/Xo2nTphg3blyG5cuWLWvy3BJxGjpD9+zZE717986wjKE1r0KFCrh69Sp27tyJ3bt34+eff8bSpUsxZcoUTJ8+/ZXnqFKlCubNm5fh676+vibPnZycMj3Wq17Liuy+n8gWmPAQ5SJ+fn7Q6/W4fv06KlSoYNz/8OFDREVFGUc1Aam3XkJDQ/H3339j/PjxAEQH5WXLlqFYsWJwcXFBrVq1cvZDWEnBggXTjRhKSUlBZGSkyb5SpUohLi4OISEhFjlvYGAgAODff//NtEzhwoXh6uoKnU6XpfO6uLigS5cu6NKlC1JSUtCxY0fMmjULEyZMyHDoOCA+17lz59CkSROL36I0XFdXr141fl5A1G94eLjF6pLIltiHhygXadWqFQCkG0lk+K++devWxn0BAQEoXrw45s+fD41GY7wl0qBBA9y8eRNbt27F22+/DaXSPv6vKVWqVLr+N9999126Fp4PPvgAYWFh+PPPP9MdIyoqClqt1qzzFi5cGA0bNsTq1atx584dk9cMrUAKhQKdOnXCzz//nGFiZJhvCBAjnNJycHBAxYoVIUlShqP20n6u//77DytXrkz3WmJionG01ZsICQmBg4MDFi5caNKytWrVKkRHR5tcd0R5lX38JiSyE9WqVUPv3r3x3XffISoqCo0aNcLJkyfxww8/oH379njnnXdMyjdo0AAbN25ElSpVjH1PatasCRcXF1y7ds1m/XesoX///hg0aBA6deqEpk2b4ty5c/jzzz/h5eVlUm7s2LH47bff0KZNG+Mw+Pj4eFy4cAFbt25FREREuve8zsKFC1G/fn3UrFkTAwcOREBAACIiIvD7778bh3p/9dVX+OuvvxAcHIwBAwagYsWKePbsGU6fPo29e/fi2bNnAIBmzZrBx8cH9erVg7e3Ny5fvozFixejdevWcHV1zTSGDz/8EJs3b8agQYPw119/oV69etDpdLhy5Qo2b96MP//8842X5ShcuDAmTJiA6dOno0WLFmjXrh2uXr2KpUuX4q233sryJJREuRkTHqJc5vvvv0dgYCDWrl2LX3/9FT4+PpgwYQKmTp2arqwh4alfv75xn1KpRJ06dbB3716b9d+xhgEDBiA8PByrVq3C7t270aBBA4SGhqJJkyYm5ZydnXHw4EF8+eWX2LJlC9atWwc3NzeULVsW06dPh7u7u9nnrlatGo4fP47Jkydj2bJlSEpKgp+fHz744ANjGW9vb5w8eRJffPEFfvnlFyxduhSenp6oVKkS5syZYyz38ccfY/369Zg3bx7i4uJQokQJjBgxAp9//vkrY5DL5di2bRvmz5+PdevW4ddff4WzszMCAwMxcuTIdH2TzDVt2jQULlwYixcvxqhRo1CoUCEMHDgQX375pckcPER5lUyydA9CIiIiolyGfXiIiIjI7jHhISIiIrvHhIeIiIjsHhMeIiIisntMeIiIiMjuMeEhIiIiu5fv5uHR6/W4f/8+XF1dbbKCNBEREZlPkiTExsaiWLFikMvNb6/JdwnP/fv30y2yR0RERHnD3bt3UaJECbPfl+8SHsPU7Xfv3oWbm5tFj63RaLBnzx40a9aMM5PmINa7bbDebYP1bhusd9tIW++JiYnw9fV95RIsr5LvEh7DbSw3NzerJDzOzs5wc3PjD0QOYr3bBuvdNljvtsF6t42M6v1Nu6Ow0zIRERHZPSY8REREZPeY8BAREZHdy3d9eIiIyDx6vR4pKSm2DsOmNBoNlEolkpKSoNPpbB2O3XJwcHijIedZwYSHiIgylZKSgvDwcOj1eluHYlOSJMHHxwd3797lHG5WJJfLERAQAAcHB4sfmwkPERFlSJIkREZGQqFQwNfX12r/eecFer0ecXFxKFCgQL6uB2syTAwcGRmJkiVLWjyxZMJDREQZ0mq1SEhIQLFixeDs7GzrcGzKcFvP0dGRCY8VFS5cGPfv34dWq7X48H9+14iIKEOGvirWuL1AlBHDtWaNflJMeIiI6JXYZ4VyijWvNSY8REREZPeY8BARUb4TEREBmUyGs2fPZlrmwIEDkMlkiIqKyrG4XsXf3x8LFiyw+nkyqpujR4+iSpUqUKlUaN++fa6rm6xgwkNERHblo48+gkwmg0wmg0qlQkBAAMaNG4ekpCRjGV9fX0RGRqJy5co2jDRVTEwMJk2ahPLly8PR0RE+Pj4ICQnBL7/8AkmScjSWjOpm9OjRqF69OsLDw7F27VrUrVsXkZGRcHd3z9HYsoOjtCwpIgKud+7YOgoionyvRYsWWLNmDTQaDU6dOoXevXtDJpNhzpw5AACFQgEfHx8bRylERUWhfv36iI6OxsyZM/HWW29BqVTi4MGDGDduHN599114eHjkWDwZ1c3NmzcxaNAglChRwrgvu/WXkpKSox3i2cJjKT//DGWVKqi+ZAmQw9k4ERGZUqvV8PHxga+vL9q3b4+QkBCEhoYaX8/ots2uXbtQtmxZODk54Z133kFERES6465cuRK+vr5wdnZGhw4dMG/evHTJyPbt21GzZk04OjoiMDAQ06dPh1arzTTWiRMnIiIiAidOnEDv3r1RsWJFlC1bFgMGDMDZs2dRoECBDN83b948VKlSBS4uLvD19cWQIUMQFxdnfP327dto27YtChYsCBcXF1SqVAm7du0CADx//hw9evRA4cKF4eTkhDJlymDNmjXp6sbw+OnTp+jbty9kMhnWrl2b4S2tI0eOoEGDBnBycoKvry9GjBiB+Ph44+v+/v6YMWMGevXqBTc3NwwcODDTOrEGmyY8hw4dQtu2bVGsWDHIZDJs27btte85cOAAatasCbVajdKlS2Pt2rVWjzNL6tQBlEoUunoVss2bbR0NEZHlSRIQH2+bLRv/SP777784duzYK1sT7t69i44dO6Jt27Y4e/Ys+vfvj/Hjx5uUOX78OIYMGYKRI0fi7NmzaNq0KWbNmmVS5vDhw+jVqxdGjhyJS5cuYcWKFVi7dm26cgZ6vR4bN25Ejx49UKxYsXSvFyhQAEplxjdj5HI5Fi5ciIsXL+KHH37A/v37MW7cOOPrQ4cORXJyMg4dOoQLFy5gzpw5xuRp8uTJuHTpEv744w9cvnwZy5Ytg5eXV7pzGG5vubm5YcGCBYiMjESXLl3Slbt58yZatGiBTp064fz589i0aROOHDmCYcOGmZT75ptvUK1aNZw5cwaTJ0/O8HNZjWRDu3btkiZNmiT98ssvEgDp119/fWX5W7duSc7OztLo0aOlS5cuSYsWLZIUCoW0e/fuLJ8zOjpaAiBFR0dnM/r0tFOnShIg6UuWlKSEBIsfnzKWkpIibdu2TUpJSbF1KPkK6902crLeExMTpUuXLkmJiYliR1ycJInUI+e3uLgsx927d29JoVBILi4uklqtlgBIcrlc2rp1q7FMeHi4BEA6c+aMJEmSNGHCBKlixYomx/nss88kANLz588lnU4ndejQQWrVqpVJmR49ekju7u7G502aNJG+/PJLkzI//vijVLRo0QxjffjwoQRAmjdv3ms/l5+fnzR//vxMX9+yZYvk6elpfF6lShVp2rRpGZZt27at1KdPnwxfe7luJEmS3N3dpTVr1hif//XXX8a6kSRJ6tevnzRw4ECT4xw+fFiSy+XG68fPz09q3779Kz5h+msu7fWe3b/fNu3D07JlS7Rs2TLL5ZcvX46AgAB8++23AIAKFSrgyJEjmD9/Ppo3b26tMLNMP2oUUhYvhtOdO8D8+cDEibYOiYgoX3rnnXewbNkyxMfHY/78+VAqlejUqVOm5S9fvozg4GCTfXXq1DF5fuPGjXTHCAoKws6dO43Pz507h6NHj5q06Oh0OiQlJSEhISHdjNVSNlqu9u7di9mzZ+PKlSuIiYmBVqs1Oc+IESMwePBg7NmzByEhIejUqROqVq0KABg8eDA6deqE06dPo1mzZmjfvj3q1q37xrGcO3cO58+fx/r1600+m16vR3h4OCpUqAAAqF279hufI7vyVKflsLAwhISEmOxr3rw5Pvnkk0zfk5ycjOTkZOPzmJgYAGLlW41GY9H4NCoVLvfqhVrz50OaPRvaDz8EckmnOHtm+D5a+vtJr8Z6t42crHeNRmP8o6XX6wFHR+DF79Ac5+gIZHEBU0mS4OzsjMDAQADA999/jxo1amDlypXo168fABgXQzV8NkmSjJ/VIG0ZQ2LychnDfsO+uLg4TJs2DR06dEgXl4ODQ7pFWD09PeHh4YHLly9naYFWw/kjIiLQpk0bDBo0CDNmzEChQoVw5MgRDBgwAElJSXB0dETfvn3RtGlT/P777wgNDcXs2bPxzTffYNiwYWjevDnCw8Oxa9cu7N27F02aNMGQIUMwd+7cdHWTtj7SvpZ2X1xcHAYOHIjhw4eni7lkyZLG8s7Ozq/8nIa61mg0UCgUJtd7dq/5PJXwPHjwAN7e3ib7vL29ERMTg8TERDg5OaV7z+zZszF9+vR0+/fs2WOdtWEaNEDgzp0oeP06/uvXD+eGDrX8OShDaTskUs5hvdtGTtS7UqmEj48P4uLikJKSYvXzvVJsbJaLajQaaLVa4z+4ADBy5Eh8/vnnaNOmDZycnIyde+Pj4xETE4PAwED88ccfJu85dOjQi1PHQi6Xo3Tp0jh+/LhJmWPHjkGSJOO+qlWr4t9//8XHH3+cLq60HYrT6tChAzZs2IBRo0ahaNGi6d7j6OgIpVIJvV6PpKQkxMTE4MiRI9Dr9ZgyZYpxbS9DJ2tDvADg7u6O7t27o3v37pg+fTpWrFiBXr16ARAduzt06IAOHTqgdu3amDp1KiZPnpyubgCRaBnODQAJCQkm56pcuTIuXLiAIkWKpPt8SUlJSEpKMok/MykpKUhMTMShQ4dMOnqHhoYaz/mm8lTC8yYmTJiA0aNHG5/HxMTA19cXzZo1g5ubm0XPpdFoEBoaCqdly4CQEPjt3YviX34JVKtm0fOQKUO9N23a1OKLzVHmWO+2kZP1npSUhLt376JAgQJwdHS06rksSaVSQalUmvyO79WrF6ZNm4b//e9/+PTTT42dd11cXODm5oYRI0ZgyZIlmDlzJvr164dTp05h48aNAABXV1e4urpi4MCBaN26NVatWoU2bdpg//792LdvH+RyufFc06ZNQ7t27VCqVCl06tQJcrkc586dw8WLFzFjxowM4/36668RFhaGZs2aYcaMGahduzZUKhUOHz6MOXPm4MSJE3Bzc4NcLoejoyPc3NxQpUoVaDQarFu3Dm3atMHRo0eNg3hcXV3h5uaGUaNGoUWLFihbtiyeP3+OsLAwVKpUCW5ubpg6dSpq1qyJSpUqITk5Gfv27UOFChXg5uaWrm4AseSD4dwAjA0GhnNNmjQJdevWxaRJk9CvXz+4uLjg0qVL2Lt3LxYtWgQAJvFnJikpCU5OTmjYsCEcHR1NrvfExMQ3uh4M8lTC4+Pjg4cPH5rse/jwIdzc3DJs3QFEBqtWq9PtV6lUVvtloWjYEHj/fci2bIFq/HggNBTgWjRWZ83vKWWO9W4bOVHvOp0OMpkMcrk8T60Qbph0MG3MDg4OGDZsGObOnYshQ4YYXzN8Nn9/f/z8888YNWoUFi9ejKCgIHz55Zfo27cv5HI5ZDIZ3n77bSxduhQzZszA5MmT0bx5c2N5w/FatmyJnTt34osvvsDXX38NlUqF8uXLo3///pnWoZeXF44fP46vvvoKX375JW7fvo2CBQuiSpUqmDt3LgoWLGhcY8rwuWrUqIF58+bh66+/xsSJE9GwYUPMnj0bvXr1Mn4mvV6P4cOH4969e3Bzc0OLFi0wf/58yOVyqNVqTJo0CREREXByckKDBg2wceNGk+/1y9/3l19Lu6969eo4ePAgJk2ahEaNGkGSJJQqVQpdunQxOcbL35eXGer65etbpVK9cmh/Vsik7PSYsiCZTIZff/0V7du3z7TMZ599hl27duHChQvGfd27d8ezZ8+we/fuLJ0nJiYG7u7uiI6OtkoLz65du9CqVSuo7t0DypcHUlKAHTuANm0sei5KZVLv/MObY1jvtpGT9Z6UlITw8HAEBATkqRYea9Dr9YiJiTG2tBgMGDAAV65cweHDh20Ynf14+ZpLe70nJiZm6++3TVP2uLg4nD171jjxU3h4OM6ePYs7L2YrnjBhgvFeIwAMGjQIt27dwrhx43DlyhUsXboUmzdvxqhRo2wR/qsFBACGuMaMAdixk4goz/v2229x7tw53LhxA4sWLcIPP/yA3r172zosygKbJjz//PMPatSogRo1agAQa3XUqFEDU6ZMAQBERkYakx8ACAgIMPY2r1atGr799lt8//33uWJIeoYmTgQKFwauXgWWLbN1NERElE0nT55E06ZNUaVKFSxfvhwLFy5E//79bR0WZYFN+/A0btz4lXMQZDSLcuPGjXHmzBkrRmVBbm7AjBnAoEHAtGlAz55AoUK2joqIiN7Qpk2b8lR/JkrF75q19esHVKoEPH8ukh8iIiLKcUx4rE2pBObNE48XLwauXbNtPERERPkQE56c0KwZ0KoVoNUCaRZ2IyIiopzBhMfSMuuT9M03gEIBbN8O/PVXzsZERESUzzHhsaTnz9F41CjINm5Mn/hUqCA6LwPA6NGATpfz8REREeVTTHgsSL5wIdwjIqDs1Qto2jR9f51p0wB3d+DsWeCHH2wRIhERUb7EhMeC9OPH43KPHpAcHYF9+4AqVYApUwDD+h9eXsDkyeLxpElAJgvJERERkWUx4bEktRrX3n8f2rNngZYtxbISM2aIxMew9MWwYUCpUsCDB8CcOTYNl4jInoWFhUGhUKB169Ym+yMiIiCTyVCkSBHEvrQKe/Xq1TFt2jTj88aNG0MmkxkXEjVYsGAB/P39rRU6WQETHmsIDAR+/x3YuhUoXhy4eRMw/LCo1cDXX4vH33wD3L1ruziJiOzYqlWrMHz4cBw6dAj3799P93psbCy++eab1x7H0dERU6ZMgYZLBOVpTHisRSYDOnUCLl8Gxo8H5s5Nfa1xY6BBAyApCZgwwWYhEhHZq7i4OGzatAmDBw9G69atM5y5f/jw4Zg3bx4ePXr0ymN169YNUVFR+IF9L/M0JjzW5uoKzJ4t1tQy+Ogj4OFD8Xj9euDkSZuERkRkDkmSoIvX2WR71TJEGdm8eTPKly+PcuXKoWfPnli9enW6Y3Tr1g2lS5fGF1988cpjubm5YeLEiZg7dy7i4+PNrjfKHWy6lla+dPcucOSIWGrCYMQIICxMtAoREeVS+gQ9Dhc4bJNzN4hrAIWLIsvlV61ahZ49ewIAWrRogejoaBw8eBCNGzc2lpHJZPjqq6/Qtm1bjBo1CqVKlcr0eIMHD8aCBQswf/584wLXlLewhSen+foCV64AvXun7jtxQnRmNvM/GCIiSu/q1as4efIkunXrBgBQKpXo0qULVq1ala5s8+bNUb9+fUw2jKDNhFqtxsSJE/Htt9/iyZMnVombrIstPLZQpAiwdi3Qpw/QuTPw5AmwdClw4YLo3FysmK0jJCJKR+4sR4O4BjY7d1atWrUKWq0WxdL8LpUkCWq1GosXL05X/quvvkKdOnUwduzYVx73gw8+wNKlSzFz5kyO0MqDmPDYUqNGYnJCf38gJga4eBEoWNDWURERZUgmk5l1W8kWtFot1q1bh2+//RbNmjUzea19+/b46aef0KJFC5P9QUFB6NixI8aPH//KY8vlcsyaNQudO3fG4MGDLR47WRcTHlsrWBBYtEjc4kpKAmJjAScn8dqzZ0ChQraNj4goD9m5cyeeP3+Ofv36wd3d3eS1Tp06YdWqVekSHgCYNWsWKlWqBKXy1X8WW7dujeDgYKxYsQLe3t4WjZ2si314coOePYFatYCEBDEzsyQBM2cCZcsCN27YOjoiojxj1apVCAkJSZfsACLh+eeffxATE5PutbJly6Jv375ISkp67TnmzJmTpXKUu7CFJzeQy4H584GGDYGVK4GBA4EdO4CnT4G2bcUILg8PW0dJRJTr7dixI9PXgoKCjEPTMxrmvmLFCqxYscJk34EDBwAAer3euK9OnTpmD5Mn22MLT27RoIGYqFCvF5MR/vorUKKEGNHVpQug1do6QiIiojyLCU9uMmcO4OAA7NkjVlT/7TfA2Vk8//RTW0dHRESUZzHhyU1KlRKTEAIiwalcGfjxR/F84ULgpaZWIiIiyhomPLnNpEmAl5e4lfXNN0DHjsCsWeK1YcPYiZmIiOgNsNNybuPhIUZoDRoETJwo9o0fLxKdevWA0qVtGh4REVFexIQnNxo4EPjvP2DGDJH0PH8OrFrFtbaIiIjeEG9p5UYyGfDFF8C8eeL53LkiCdLpxPMnT0SrD0duERERZQlbeHKzUaMAd3dgwADg+++B6GixBleTJsD580B8vJilmYiIiF6JLTy5Xd++wJYtYrj6li1Ahw5inh4AWLxYLDpKREREr8SEJy/o2BHYuTN1Tp5Fi8QSFIAYxr53r23jIyIiyuWY8OQVTZuKxMbDAzh2TMzE3Lmz6Nfz/vvA1au2jpCIKFcJCwuDQqFA69atTfZHRERAJpOhSJEiiI2NNXmtevXqmDZtmvF548aNIZPJsHHjRpNyCxYsgL+/v7VCJytgwpOX1KkDHDoE+PgAFy4AZ86IRUejosSaW8+e2TpCIqJcY9WqVRg+fDgOHTqE+/fvp3s9NjYW33zzzWuP4+joiClTpkCj0VgjTMohTHjymipVgMOHAX9/4OZNMXy9aFGxBhcTHiIiAEBcXBw2bdqEwYMHo3Xr1li7dm26MsOHD8e8efPw6NGjVx6rW7duiIqKwg8//GClaCknMOHJi0qXBo4cASpWBB48ABITgeXLOSkhEeUIXbwu8y1Jl/WyiVkr+yY2b96M8uXLo1y5cujZsydWr16dboXzbt26oXTp0vjiiy9eeSw3NzdMnDgRc+fORXx8/BvFQ7bHYel5VfHi4vZWy5bA33+Ljs2//QY0bgxERopWHyIiKzhc4HCmrxVqVQhVf69qfH60yFHoE/QZlnVv5I4aB2oYnx/3Pw7Nk/S3jRpLjc2OcdWqVejZsycAoEWLFoiOjsbBgwfRuHHqsWQyGb766iu0bdsWo0aNQqlSpTI93uDBg7FgwQLMnz8fUwyDRihPYQtPXubpCezbB7zzDhAbC7RoAQwdCgQEiNFcRET50NWrV3Hy5El069YNAKBUKtGlSxesWrUqXdnmzZujfv36mDx58iuPqVarMXHiRHz77bd48uSJVeIm62ILT17n6grs2gV07Qps3w4sWwZIEvDBB8Dx40D58raOkIjsTIO4Bpm/qDB9Wu9RvczLvvQv99sRb795UGmsWrUKWq0WxYoVM+6TJAlqtRqLFy9OV/6rr75CnTp1MHbs2Fce94MPPsDSpUsxc+ZMjtDKg9jCYw8cHYGtW4EPPxTJDiBmZW7TBnj61LaxEZHdUbgoMt8cFVkv65S1subQarVYt24dvv32W5w9e9a4nTt3DsWKFcNPP/2U7j1BQUHo2LEjxo8f/8pjy+VyzJo1C8uWLUNERIRZcZHtsYXHXiiVYtkJD4/U5SZu3hRz9fz5p5ipmYjIzu3cuRPPnz9Hv3794O7ubvJap06dsGrVKrRo0SLd+2bNmoVKlSpBqXz1n8XWrVsjODgYK1asgLe3t0VjJ+tiC489kcuB//u/1FmYAeDAAdGv56XRCURE9mjVqlUICQlJl+wAIuH5559/EBMTk+61smXLom/fvkhKSnrtOebMmZOlcpS7sIXH3shkwPTpQMGCYvFRQCw8+t574hYXEZEd27FjR6avBQUFGYemvzxEHQBWrFiBFStWmOw7cOAAAECvTx1pVqdOnQzfT7kbW3js1SefAKtXiwQIAGbNAm7csGlIREREtsKEx5716SPW3HJzEyO2qlcHVq7k7S0iIsp3mPDYu/feE+tuNW4MxMcDAweKyQofPrR1ZERERDmGCU9+ULKkmKDQMCfPn38ClSuLmZmJiIjyASY8+YVcDqxfL+bsAYAnT0TrT//+YpZmIqJMsIMu5RRrXmtMePKTmjWBl1f7XbVK9O05dswmIRFR7qVQiEn/UlJSbBwJ5ReGa81w7VkSh6XnNx98IPr0zJwJqFRAoULArVtAgwbA+PHA1KmcpJCIAIg1qJydnfH48WOoVCrI5fn3f2S9Xo+UlBQkJSXl63qwJr1ej8ePH8PZ2fm1E0C+CSY8+dH06cC//wLbtonnnToBP/8MfPklsHs38L//ARUq2DREIrI9mUyGokWLIjw8HLdv37Z1ODYlSRISExPh5OQEmWG6D7I4uVyOkiVLWqWOmfDkR3I58OOPQN26oi/PpEli8dGPPwZOnxa3vubMAYYNE2WJKN9ycHBAmTJl8v1tLY1Gg0OHDqFhw4ZQqVS2DsduOTg4WK0FjQlPflWgALBjh1iDq3hxoEYNoF49oG9f0cozciSwcyewZo14nYjyLblcDkfDgId8SqFQQKvVwtHRkQlPHsV/3/MzPz/TZEapBHbtApYsAZycgNBQoEoVYNMm28VIRERkAUx4SNiyBQgIEHPzDBkCnDkD1K4NPH8ubnf16AFERdk6SiIiojfChIeEQ4fETMw9e4pRXOXKiaHqU6YACgWwYYNo7dm/39aREhERmY0JDwnz5gHvvgvExQHt2onOzCqVGNF15AhQujRw7x4QEiJGc3EiMiIiykOY8JCgUgGbNwOlSgEREUDnzoBhVMbbbwNnz4pZmSVJjOrq0kW0CBEREeUBTHgolacnsH074OoKHDwoRmoZuLiIldZXrBDJ0ZYtYlh7eLjt4iUiIsoiJjxkqlIl0V9HJgOWLxcJUFoDBwJ//QV4ewPnzwNvvcV+PURElOvZPOFZsmQJ/P394ejoiODgYJw8efKV5RcsWIBy5crByckJvr6+GDVqFJKSknIo2nyiTRtg9mzg00/F45fVqwf8848YxfX0KdCsGbBwoXn9ep48AbRay8VMRET0CjZNeDZt2oTRo0dj6tSpOH36NKpVq4bmzZvj0aNHGZbfsGEDxo8fj6lTp+Ly5ctYtWoVNm3ahIkTJ+Zw5PnAZ58B33wjRmhlpEQJMbLrww8BnU7c/urbF8go+Xz6FPj7b9N9wcGAr2/6/URERFZg04Rn3rx5GDBgAPr06YOKFSti+fLlcHZ2xurVqzMsf+zYMdSrVw/du3eHv78/mjVrhm7dur22VYiyKTkZmDYNiIkx3e/kJFZfnzdPLEGxdi1Qvz6wdatIlrp0EZ2gvbyApk0BvT71vcWKAQ8eAI0bA3/8kYMfhoiI8iObLS2RkpKCU6dOYcKECcZ9crkcISEhCAsLy/A9devWxf/+9z+cPHkSQUFBuHXrFnbt2oUPP/ww0/MkJycjOTnZ+DzmxR9tjUYDjUZjoU8D4zHTfrUXip49Id+6Ffp//oFu69bUVp+kJMDRERg2DLIKFaB47z3ITp0C3n8/3TGkwoWhvX9f9P0BgG3boOjWDfLQUEht20K3YgWkXr3eKD57rffcjvVuG6x322C920baes9u3cskyTYTqty/fx/FixfHsWPHUKdOHeP+cePG4eDBgzhx4kSG71u4cCHGjBkDSZKg1WoxaNAgLFu2LNPzTJs2DdOnT0+3f8OGDXB2ds7+B8kHPK5dQ/1Jk6DQaHC3YUPoHRzgceMGXO/cwZ9r1iDF3R0AUGX5cgTu3g0AkGQyRAYHI6JlS0SVKgVNgQLpjivTaFBjyRL4HjgAALjcoweude4sOkwTERGlkZCQgO7duyM6Ohpubm5mvz9PJTwHDhxA165dMXPmTAQHB+PGjRsYOXIkBgwYgMmTJ2d4noxaeHx9ffHkyZM3qrBX0Wg0CA0NRdOmTe1ucTnZhg1QfvRRuv3anTshNWsmnjx8CMTHQzF+POTbtgEAdEOGQD93rhjKnhFJgvzzz6GYO1eUHzsW+lmzzIrNnus9N2O92wbr3TZY77aRtt4TExPh5eX1xgmPzW5peXl5QaFQ4OHDhyb7Hz58CB8fnwzfM3nyZHz44Yfo378/AKBKlSqIj4/HwIEDMWnSpAyXlFer1VCr1en2q1Qqq1201jy2zfTuDURHA3/+CVSrJoaj164NZYkSqS0yJUqIrz//DMyaBUyZAsXSpVBcvCjm7SlcOONjf/01ULIkMH48FB06QPGGdWeX9Z4HsN5tg/VuG6x321CpVNBmc2SvzTotOzg4oFatWti3b59xn16vx759+0xafNJKSEhIl9QoXvQnsVFDVf4yYgTw++9iaYkOHcQoq4xuP8nlwOTJppMY1q4tZmvOzLBhwK1bQNrvPb+nRERkITYdpTV69GisXLkSP/zwAy5fvozBgwcjPj4effr0AQD06tXLpFNz27ZtsWzZMmzcuBHh4eEIDQ3F5MmT0bZtW2PiQ7lIu3bAiRNAmTLAnTtiZuaNGzMvX6RI6uMzZ0Tyc/u29eMkIiK7Z7NbWgDQpUsXPH78GFOmTMGDBw9QvXp17N69G94vRvLcuXPHpEXn888/h0wmw+eff47//vsPhQsXRtu2bTHLzD4flIMqVABOngS6dQN27xZfz54Vt7wyS1IlCfj4YzFHT926Yth61ao5GjYREdkXmyY8ADBs2DAMGzYsw9cOvBi9Y6BUKjF16lRMnTo1ByIji/HwAHbuFIuOzpkjtvPnxRIWHh7py8tkoh9Qy5bAxYtAgwbAtm3AO+/kcOBERGQvbL60BOUTCgXw1VciyXFyEq025cuL5SvOnUtf3tcXOHIEaNhQTHjYogWwaVPOx01ERHbhjRIejUaDu3fv4urVq3j27JmlYyJ71q0bcPQoEBAghrHPmwdUry5uWX3zDXD/fmpZDw8xKqxzZyAlBejaFZg/31aRExFRHpblhCc2NhbLli1Do0aN4ObmBn9/f1SoUAGFCxeGn58fBgwYgL+5LhJlRY0awJUrYhRX586AgwNw4QIwdqxo2WneHFi/HoiPFzM5b9wIDB8u3vvnn2LtLiIiIjNkKeGZN28e/P39sWbNGoSEhGDbtm04e/Ysrl27hrCwMEydOhVarRbNmjVDixYtcP36dWvHTXmdg4MYxbVli1hTa8UKsQq7Xg/s2QP07An4+AAffQQcOCBaglavFut0cUQeERGZKUudlv/++28cOnQIlSpVyvD1oKAg9O3bF8uXL8eaNWtw+PBhlClTxqKBkh0rWBAYOFBsN28C//sfsG6dmJfnhx/EVqIE0KOHGN5esaIYybV0qVit3cnJ1p+AiIhyuSy18Pz000+ZJjtpqdVqDBo0CH379s12YJRPlSoFTJ0K3Lgh+vp8/LHoy3PvnhjdVamSmMTwvffEZIUNG5r2+yEiIsqAxUZpSZKER48eWepwlN/JZGIOnuXLgchIcSurXTtAqQROnQJ27BDlzp2DsnZtuPM2KhERvUKWEx5nZ2c8fvzY+Lx169aIjIw0Pn/06BGKFi1q2eiIANFxuVMn0cn5/n1g0SIgKMj4suzJEzQaOxaKypWB336zYaBERJRbZTnhSUpKMlmv6tChQ0hMTDQpw/WsyOoKFxa3sk6cAC5fBkaNguTkBBkA+bVr4lZXqVJilFdSkq2jJSKiXMKiEw/KMlpIkshaypcH5s2D9tkznBswAJKhhfHWLTHKq3hxYORIMQSeiIjyNc60THmfQoGI1q2hvX1brNv1xRdiPp9nz4CFC8V6XtWrs9WHiCgfy3LCI5PJTFpwXn5OlCu89RYweTIQHi4mLDQsPnvunGj1KVIEGDWKrT5ERPlMlhMeSZJQtmxZFCpUCIUKFUJcXBxq1KhhfF6+fHlrxklkHoUC6NIFuH5dLGdhmKwwNhZYsEC0+jRsKNb2YqsPEZHdy/Jq6WvWrLFmHETWERgokpqvvhLbypWAViteO3xYbJ6eQO/eYuLDcuVsGy8REVlFlhOe3r17WzMOIusqWVLMzPz558DcuSIJ6ttX9Ou5e1csXTFvHtCoETB0KNChg5jzh4iI7EK2Oi0nJSXhhx9+wNKlS7l+FuUNxYqJFdfv3AFmzxZ9fXbsELM5A8DBg8AHH4iWoa+/Fh2fiYgoz8tywjN69GgMN6xYDSAlJQV16tTBgAEDMHHiRNSoUQNhYWFWCZLI4tRq8VWhEMtV6PWpr8lkotXns8/E0PZBg4BLl2wTJxERWUSWE549e/agadOmxufr16/H7du3cf36dTx//hzvv/8+Zs6caZUgiawqIACIiACmTRMTG6adQDMpSazkXqkS0Lw5sGuXaXJERER5QpYTnjt37qBixYrG53v27EHnzp3h5+cHmUyGkSNH4syZM1YJksjqChYUi5bevw/89ZeYzblYMfHa22+LVp89e4DWrUWrzzffAHFxto2ZiIiyLMsJj1wuN1k64vjx43j77beNzz08PPD8+XPLRkeU05RKoHFjsV7X3bvAsWMiAbp5Exg9GnBwAB48AMaOFUlS8+bA6dO2jpqIiF4jywlPhQoVsOPFCtUXL17EnTt38M477xhfv337Nry9vS0fIZGtyOVAnTpi8dKAAODbb4EvvwQM17lWK1p9atUSExqOHg28tL4cERHlDllOeMaNG4cJEyagSZMmaNKkCVq1aoWAgADj67t27UJQmhWsiezSp5+KFp4LF8TMza6uYv/jx2L019tvA2vXir4/UVGm/YGIiMhmspzwdOjQAbt27ULVqlUxatQobNq0yeR1Z2dnDBkyxOIBEuVKlSsDP/4IxMQAf/wBBAeL22HnzwN9+oi1vEqWBLy8gGbNgHHjgJ9+Eiu863S2jp6IKN8xa2Y1Q+tORqZOnWqRgIjynBYtxPbsmZjJefFi4N691NdDQ8Vm4OQkZnVesCB1X1KSuHVGRERWkeWE586dO1kqV7JkyTcOhihPK1RIzN3z6afAr7+Kjs+HD6e+rlKJr4mJpsnN48dA0aJiWYsaNcTK7tWri8eenjn5Ccyj1fKWHRHlGVlOeNL21zGM1kq7WrokSZDJZNCxuZ7yO6USeP99sV27Bnz/vejX8/hxapljx4CtW4F27UR/IJ1OTG546ZJY7sKgRAlg0iQx+WFuotdD8c47aHP6NOR+fqJTt59f6ubvL5I2Qx8nIiIby3LCI5PJUKJECXz00Udo27YtlFxniOj1ypYVS1TMnAls3y5ueYWGpi5cWqSIWLj08GHRH+jMGeDsWfH15k1xayw3/qzJ5dAtWQJlUBBkN24AN26kL3PoENCggXj8228iwfP3N02MSpZMnfWaiMiKsvyb9N69e/jhhx+wZs0aLF++HD179kS/fv1QoUIFa8ZHZB8cHFJbfW7dAlatAlavFiO+5s4V2zvvAAMGAGPGiCQgJgY4dw4oUyb1OBs2iFtj77+f85/h33+Bv/8WnbIBoEIF7F22DO8EBkL5339iturbt1M3f//U9x47Jjp5Z6RoUTGDdfXqVv4ARJSfZTnh8fHxwWeffYbPPvsMR44cwZo1axAcHIyKFSuiX79+6NevH+TybK1FSpQ/BAYCs2aJpSx+/120+vzxh5jg8K+/RL+dXr1E8mNoIQFEK8qAAUBCAvDxx2IYvJNTzsS8bp24rZaSApQuLeJSqZDg7Q2pUaPU/kmZaddOLNB6+7ZpYpSQAERGiv5PRERW9EYZSv369bFq1Spcv34dzs7OGDRoEKKioiwcGpGdU6mA9u1F0hMRIZa2KFECePpUJDMVK4rEYt060dHZzw8YOVIsc7FiBRAUZP1FTRMTRZLVu7d43KQJUL68+cepWxcYPx5Ytkwkd5cuiaU5Hj8WrUbFi6eWjY+3XPxERC+8UcJz7Ngx9O/fH2XLlkVcXByWLFkCDw8PC4dGlI+ULClafCIigJ07gffeEyu5Hzkikg0PDzGp4cOHwODBokXk33+B2rXFrTFrjJa6cUMkKt9/L5Ks6dPFrafChS1zfJlMzFNUu7b4rJIEfP65mNOIy9QQkYVl+ZZWZGQk1q1bhzVr1uD58+fo0aMHjh49isqVK1szPqL8RaEQC5S2bi0WMl2zRiQcERFiza6X1+1KTAT69QP+7/+AGTPEUPYSJUQykR2//gp89JHoR1S4sBg51rRp9o75Ok+eiM97/764BbZnT87dsiMiu5flhKdkyZIoXrw4evfujXbt2kGlUkGv1+P8+fMm5apWrWrxIInypWLFxJD0iROB8HAxeivtdvduatnz50WrECBafwxz+Ri28uVf388mrXv3RLJTrx6waZPpLSdrKVwY2L1b3MY7cgTo3l2M7FIorH9uIrJ7WU54dDod7ty5gxkzZmDmzJkAYLJ6OgDOw0NkDTKZ6OgcGAh07Ji6/+lTMYpr507g0SOR9Fy+LGZ83r9fbAYODmI5jGrVxFIXnTunH+4uSaktQ8OGidtoXbualyhlV5UqYvh+8+bAtm3A0KGi3092W6yIKN/LcsITHh5uzTiIyFyensC774rNICZG9PVRKkVfmGvXRGtQTEzqLbE1a0TL0Wefif5BajXw55+iD9Hu3YC7u0gwPvzQNp+rUSNxC+3990Xn7GLFgClTbBMLEdmNLCc8fn5+1oyDiCzhwgUxz09ysmgBWr9eJBARESLxOXFCzAF065YY2j5tmmj52btXtPDMng189ZWNPwSATp3EmmRDh4rRa+3bAzl9uzwmRgzD9/LK2fMSkVVkaZRWVtfRMvjvv//eKBgiyqZ69YCTJ0Wfnfv3RevPtGli9fYOHUQyExEhFi718RFz4ISGimSndm0x7D23GDJEJDvr1uVssiNJIiksWVJsmzbl3LmJyGqylPC89dZb+Pjjj/H3339nWiY6OhorV65E5cqV8fPPP1ssQCIyU9WqwD//AH37ij/eX3wh5s8xrODu4gLUrJnaL8bw9Z9/xAKm48eL4e+5wbRpOXtr7cED0cepf38gOlqMguvaVQyX1+tzLg4isrgsJTyXLl2Ci4sLmjZtCh8fH7Ru3RoDBgzA8OHD0bNnT9SsWRNFihTB6tWr8fXXX2PEiBHWjpuIXsXFRbRSrF8PFCgg1rUaMEC8tnWrWMYiMhKoUEHc6vrpJ9FhODYWmDNHLAsxbJiYDTm3ePBAJG7WnGzR1VWsYeboKJb7GDtW7J81S3T05urwRHlWlhIeT09PzJs3D5GRkVi8eDHKlCmDJ0+e4Pr16wCAHj164NSpUwgLC0OrVq2sGjARmaF7d9FR+Z13gCVLxL5GjcTtrB49xO2vqlVFK8a5c2KRz+BgIClJlC9dWqyddeWKbT8HAIwaJUaetWiR2lplCRERqa03Li7Axo1ixNuYMWLh13XrRMfuunU5WowoDzNrGWYnJyd07twZnTt3tlY8RGRpZcqYDlEvXFjcvvL2Nv0DLpMBbdsCbdoABw4AX34pOjOvXQv88IPoSDxxopjc0BYWLxatUVeuiKTn8GGgYME3P55OJ5bwmDwZ+PZb0WcIEEt2pPXhh2LkW+nSqfu02ty5ij0RZYqrfRLlRz4+mbdWyGSiRSg0VIzqat9e3MrZulX0/WnVSkwMmNM8PcWw+WLFgIsXxUSLiYlvdqyLF0WLzdixojXLMEotM2XKpNZXTIzo4L1s2Zudm4hsggkPEWUuKEgsM3HhgrgFJpeLxT8bNIDi3Xfh9+ef4rWcmnDUzy91rqDDh0VM5pxbowFmzhStVCdPiuN8/z3w889Zv121Zo24/TdkiFjXTKN5s89CRDmKCQ8RvV7lysD//icmMvz4Y8DBAfIjR1B92TKoatUSt5aaNhW3h3btErM9W4thNma1WiRjkydn7X0XLgBvvSXKazTi1t3Fi2ItMnP65owYIYb3y2TA8uXicz958mafhYhyDBMeIsq6UqXEH/lbt6D7/HM8rlIFUoECYnTX3r2i9aR1a3H7qVw5sQDpihWiE7AlW4EMszFXrCgSsKxISRErzHt6ivf+9tubrREmk4lZqn/7TYzqOnhQJFIvrStIRLmLWQmPRqNB3759ucwEUX5XvDj0U6bg2IwZ0D5+LDoTL18ulqooW1aUuXZNdHYeNEis4eXhIYaVf/458Pvv2W8V6dRJnLdECdGv5sEDMaQ87cSnDx6kPq5VSyQ6Fy+K0WvZHXHVpg1w/LhIAiMiRJ+gtJ3DiShXMWuYgUqlws8//4zJWW1CJiL7p1CIhKZatdTWlqdPRTJw/DgQFiY6P8fFpV/UtEwZoE4dkYwAQEICEB8vtqw+Tk5OH1O9eqJ/zv79oq9OlSpif5culv3sFSuK43/wgVi4tUIFyx6fiCzG7HGV7du3x7Zt2zBq1ChrxENE9sDTU9zaat1aPNfpRMtKWFhqEnT1KnD9utjWrbPcuWUy4OjR1OdDh4qOxqVKWe4caRUqJDpS37kDFC2aup9D14lyFbN/GsuUKYMvvvgCR48eRa1ateDi4mLyOmdZJqJ0FAoxwWHVqqatQCdOiATowgVApRIT/xk2Z+fMn7/8moODuE21c2fq8HKFQiRahw+LOXSaNhW319q2FeeyJKUSCAxMfb5pk5i0cNs2sY4ZEdmc2QnPqlWr4OHhgVOnTuHUqVMmr8lkMiY8RJQ1np5iTh9Lzc6+aZNYB+voUWDgQDFx4pEjom/Rn3+KeYVCQ0UrTP/+YitZ0jLnTis5WXRqvn1bzNfz66+ifw8R2ZTZCQ87LBNRruTsLGaIfvw49dbSe++JLTwcWLlSrC8WGQnMmCHWx2rdWrT6NG8uWoQsQa0WI7fee0/M19O4sUi6+va1zPGJ6I1ka1i6JEmQuJgeEeUWSqVpPxqDgADR4nP3rmgJeucdsX7Wjh0i6QkMFAlQ2lFd2eHnJ1qXOnUSc/706wf07Clu3/F3JpFNvFHCs27dOlSpUgVOTk5wcnJC1apV8eOPP1o6NiIiy3JwECOq9u8Xo6pGjRKTJt65I4bL+/oC778P7NuXuqDomypQANi8GZg2TTxfvx4ICRGjy4gox5l9S2vevHmYPHkyhg0bhnr16gEAjhw5gkGDBuHJkyccvUVEeUP58sC8eaJlZ+tWcdvp2DHxeOtWMWS+fXsxCsvdHXBzM93S7nNwyPgccjkwdaq4ZbZsmZiLqEAB8Zokib4+LVqI215yzgNLZE1mJzyLFi3CsmXL0KtXL+O+du3aoVKlSpg2bRoTHiLKW5ycxIroH34oZktesQL48UcxXH7u3KwdQ61OnwS9nBzVqQNUqiSW3ShUSKxYP3eu2AIDxW2vjz4Si6MSkcWZnfBERkaibgYjDurWrYvIyEizA1iyZAnmzp2LBw8eoFq1ali0aBGCgoIyLR8VFYVJkybhl19+wbNnz+Dn54cFCxaglaVGehBR/lW1KrBkCTBnjrgdde6cmMU57RYdnfrYcHsqOVl0ln78OGvn8fER/YqqVBGJ1a1bwKRJwJQpYtTagAFAy5acx4fIgsz+aSpdujQ2b96MiRMnmuzftGkTypQpY9axNm3ahNGjR2P58uUIDg7GggUL0Lx5c1y9ehVFihRJVz4lJQVNmzZFkSJFsHXrVhQvXhy3b9+Gh4eHuR+DiChzBQpkbVSVVitmkE6bBGWUGEVFiWUvLl4Uw9UfPMi4g7ROJzpS79gBfPIJ0LWrmM3Z1dXSn5Ao3zE74Zk+fTq6dOmCQ4cOGfvwHD16FPv27cPmzZvNOta8efMwYMAA9OnTBwCwfPly/P7771i9ejXGjx+frvzq1avx7NkzHDt2DKoXE4f5+/ub+xGIiCxDqRT9csz5pys2Frh0SSQ/hu3ff03XAAOABQvEBog5i3x9gXfegaxyZbgmJGS/UzVRPmN2wtOpUyecPHkS8+bNw7Zt2wAAFSpUwMmTJ1GjRo0sHyclJQWnTp3ChAkTjPvkcjlCQkIQFhaW4Xt+++031KlTB0OHDsX27dtRuHBhdO/eHZ999hkUlppDg4jImlxdgeBgsaUVFZWaCF24kPr4wQMxK/XTp8DZs1ACeBeANH68WIOsUSMxsWFwsBhxRkQZMivh0Wg0+PjjjzF58mT873//y9aJnzx5Ap1OB29vb5P93t7euHLlSobvuXXrFvbv348ePXpg165duHHjBoYMGQKNRoOpU6dm+J7k5GQkp1lcMCYmxvhZNBpNtj7DywzHs/Rx6dVY77bBercwFxfgrbfEltbdu1BMmwbZjh2QRUUZd8vi44FDh8T2glSuHKBUQl+hAqQXSZXk5wd4e3MUWDbxereNtPWe3bqXSWbOHOju7o6zZ88iICAgWye+f/8+ihcvjmPHjqFOnTrG/ePGjcPBgwdx4sSJdO8pW7YskpKSEB4ebmzRmTdvHubOnZtph+lp06Zh+vTp6fZv2LABzs7O2foMREQ5RqdDkbNnUfzIEbg8eACXBw+gjopCXLFikOl0KPCKSRMluRzJrq64+847uPTRR2KnXo+ix48DAGR6PeQ6HWRarfFxbIkSeFq5MgBAkZSEUtu3p5Z7sRkePy9TBnebNLF2DVA+l5CQgO7duyM6Ohpubm5mv99mq6V7eXlBoVDg4cOHJvsfPnwIHx+fDN9TtGhRqFQqk9tXFSpUwIMHD5CSkgKHDObCmDBhAkaPHm18HhMTA19fXzRr1uyNKuxVNBoNQkND0bRpU2MfI7I+1rttsN5toG1bk3rXyuVwTE4GnJ2hefwYsj//hHzpUshu3waePoXsRT8fmV4Px+holNm2DaUvXoT09tuQKlSA4uuvMz2VbsAA6MeNE0+ePoWqa9dMy/p5eqJK2pGyjx8DhQtb5CPnCpIE7e3b2PPvv2javDmv9xyU9npPTEzM1rFstlq6g4MDatWqhX379qF9+/YAAL1ej3379mHYsGEZvqdevXrYsGED9Ho95C+aZ69du4aiRYtmmOwAgFqthlqtTrdfpVJZ7aK15rEpc6x322C924ax3h0dxY5ixYA+fcQGiBFk//4rFlPdvx84eRK4dw+y69chu3499UBKpej74+UFFCki5g1SKqGoWRMKw/fVzU0syKpUik2lSn2sVEJerRrkhrIHD4qJFj/+GBg/PuOlPvKSK1eAUaOg2r0bjQID4VCoEJSNGtk6qnxHpVJBq9Vm6xg2XS199OjR6N27N2rXro2goCAsWLAA8fHxxlFbvXr1QvHixTF79mwAwODBg7F48WKMHDkSw4cPx/Xr1/Hll19yhXYiopcplUD16mIbOlTse/pUrOd1/DgQFia+xsenziF0+bKYHygkREyO+OSJSIScncWEjFmxbZuYl2jhQuC774AhQ4Bx40Q/orwkPl4sN7J4sUgeAXjcuiVmxe7eXUwYyUki8xbJDHq9XoqIiJASEhLMedsrLVq0SCpZsqTk4OAgBQUFScePHze+1qhRI6l3794m5Y8dOyYFBwdLarVaCgwMlGbNmiVptdosny86OloCIEVHR1vqIxilpKRI27Ztk1JSUix+bMoc6902WO+2YdF6T06WpMOHJWnqVEmqX1+SlEpJEotepG7Vq0vSmDGStHu3JMXFvf6Yer0khYZKUp06qcdwdpakceMk6fHj7MecU5KSJKl0aRF/u3aS5vBhKSIkRNLLZKKeLl+2dYT5QtrrPbt/v83qtKzX6+Ho6IiLFy+aPclgbhETEwN3d/c37vT0KhqNBrt27UKrVq3YxJ+DWO+2wXq3DavWe2ysGPW1d69YQPXCBdPXVSoxBD4kRGy1a2c+G7QkAX/+KdYSO3lS7KtWDThzBpDJLBu3pYSFic9kqNf9+8Vq982bp9a7jw9Up08Dgwenvu/cOTFLd279XHlY2us9MTExW3+/zRqnKJfLUaZMGTx9+tTsExERUS7n6gq0bg3Mny/WFYuMFKu89+kjJj7UaEQfncmTxdpgnp7Ae+8BixaJP/ppO5XKZGJh1OPHgZ07gZo1xezRhqQgJQV4/twmHzOd27eBLl1EMrd8eer+d98V/ZHSqlnTNNk5exaoUQNo2lTMm0S5ltkTM3z11VcYO3Ys/v33X2vEQ0REuYWPj+ivsnq1SAquXQOWLgU6dRIdnWNigN9+A0aMEH2FnJ1FYvTuu6LT8ty5wPbtgL8/cPgwkGbRaaxdK/oLffGFWIbDFhISRAtU+fJi7TS5PP2M169z9izg4CBaxKpVE3WRWxI5MmF2p+VevXohISEB1apVg4ODA5ycnExef/bsmcWCIyKiXEImA8qUEdvgwWLdrzNnxB/60FCx+nt0NHDvntj++iv9+0uUEO8vXVrcOouOFgnHggXAmDHA8OE5s26YJAGbNgFjx4pYATFj9YIFInEzx0cfAQ0bivh//VW0dm3YAMycKRaB5SoAuYbZCc8Cw9ouRESUfykUor9L7drAZ5+JJOLpU7H6+/XrwI0bpl+jo4G7d8W2f7/psZ4/F6vFT5smbhl16iQSkOrVReuJpX36qbhtBwB+fsA334hzvmkfnMBA4JdfRPI3cqS4tTV4sGg12rePfXtyCbMTnt69e1sjDiIiystkMjGE3ctL9O9JS5LEEHdD8pM2Ebp2TXSWBkQfoRMnxAaIZKdKFbF6/dtvA61aiX42mXWUzqqPPgK+/1608IwZA7x0p+KNNWkibnEtWwZMmQJ07MhkJxd5o6vm5s2bWLNmDW7evIn/+7//Q5EiRfDHH3+gZMmSqFSpkqVjJCKivEwmEzMvFy6ceTJ0+bLoIL11q+gHdO+eaDEyzPd28CAwZ454XKCAKFOlimhJadw483OnpIi5dKKiRH8hQIyoundPTKhoaUqluDXXrRvg4ZG6f+dOcdtv3DjR14lynNkJz8GDB9GyZUvUq1cPhw4dwqxZs1CkSBGcO3cOq1atwtatW60RJxER2aO0yVDDhmKUlF4vOhDfugX8+KPYIiNTR4HFxYkE6fJlcduoYkWxWryXF/D77yKhKV9ezBz9f/8HXL0qEpEPPxR9iADrJDtpeXmlPk5OFre6bt0SHcC/+gpo2zZn+iuRkdkJz/jx4zFz5kyMHj0armm+We+++y4WL15s0eCIiCifkclSO/qWKiX69UybJp7HxwN79ojt5ElxWywmBrh0SWwGaR8DIvGZPVsczxYcHESSM2YMcOcO0KOH+JwVKgBvvQW0by82siqzh6VfuHABHTp0SLe/SJEiePLkiUWCIiIiSsfFBejQQfSROXVKdIR++FAMjZ80CWjQIOP+OCqV6C909WrOxwyI5Ob990WL1LRp4nacJInE7IcfxIg1g9hYcZtu1SoxF1I214+iVGa38Hh4eCAyMhIBAQEm+8+cOYPixYtbLDAiIqLXKlJE3B5q21Y81+vFgp8nToi5f379Vcyt89VXYqtdW9za6tYt51d0d3YWw/CnThWJ2t9/i+3dd1PLnDplOvmhs7OY2PCtt8TWuHH21vCSJDH/UGysuDUYGyu2SpXERJIAEBEhFp3V6Uw3vV58bdYMKFtWlL16Ffj5Z9NypUqJjuG5jNkJT9euXfHZZ59hy5YtkMlk0Ov1OHr0KMaMGYNeaSeVIiIiymlyuejTU7GimCF66VLRYfjHH4Fdu0TH4X/+EUPTW7YUyU/btqmrzucUb2+gTRuxpeXjI0aP/f23SH5iY0XycfSoeH3JErEgKyASuV27xK0+Q+Ji2OLigOnTxWSIgJjoceRIsV+vTx/P9u1Au3bi8ZEjol4y87//pSY8ly+L1rW0QkLsI+H58ssvMXToUPj6+kKn06FixYrQ6XTo3r07Pv/8c2vESERE9GYcHYHOncX2+DGwcaNIfv7+G9ixQ2zu7sAHH4iZoOvVs+1Q8vLlga+/Fo/1ejFs39AS9PffonO2wR9/AAMHZn6s/v1TEx65XPR3SqtAAdFx2tU1df0wQLQgNWki+lJltPn6ppb19wf69TN9vVy5bFWBtZid8Dg4OGDlypWYMmUKLly4gLi4ONSoUSPPLiZKRET5ROHCYsj48OGiZeLHH0Vrxd27wMqVYgsMBHr2FC0cpUvbNl65XCRA5ctn3OLi6CjWPjMkLa6upklM5cqpZdu1E7efDGVcXMTxM/Luu6a32V6lenUxp1Ee8MazN/n6+sI3bZZHRESUV1SoAHz5pVgC4uBBYN06MQfQrVtivp4vvhCTHH74oWj9yY1DyHv2FFtWeHiYzguUD2VzukoiIqI8TC4H3nlHbEuWANu2ieQnNBQ4dkxsI0dC0bIlKqhUkF+5IubYKVgQKFRIfDVsBQpwZuVcjAkPERERIEZEde8utshIsQjounXA+fOQb9+OsoBoBcqMUilaUQwJ0MsJkWHz8hKdfkuVMu07Q1bFhIeIiOhlRYuKkVyffgqcOwfdtm24ffIk/N3dIY+OFguePn8OPHsmvmo0Ys6cJ0/ElhVKpUh6DP100m75/PaTNTDhISIiepVq1aCvWBEXdu2Cb6tWkL/cKmOY28aQBKXdDAlR2u3BAzH6Kj5edCS+elUMC0/L2zvjRKhkycw7G9MrZSnhOX/+fJYPWLVq1TcOhoiIKM+RycSoJxcXoESJrL1HksQ8OleupN/++09MTPjwoehQnZaTk7gdZkiAKlYEatYULUXsP/RKWUp4qlevDplMBkmSIHtNhep0OosERkREZLdkMpEclSghJupLKyZGtABduSKGzxsSoevXxQKq586JLS13d5H41KwJ1KolttKl2RqURpYSnvDwcOPjM2fOYMyYMRg7dizq1KkDAAgLC8O3336Lrw2TJREREdGbcXMTS2DUrm26X6sFwsNF8nP1qkiGzp8XW3Q08NdfYjNwdRXLUhgSoJo1ReuQYXHWfCZLCY+fn5/x8fvvv4+FCxeiVatWxn1Vq1aFr68vJk+ejPZc8ZWIiMjylEqgTBmxGdYOA0SH6YsXxVIUp0+Lr+fOiSUmDh0yXZzUxSU1CTK0BpUvny+SILM7LV+4cCHdwqEAEBAQgEuXLlkkKCIiIsoilUrMeFy9uljmARBJ0OXLqQnQqVPA2bOio/SRI2IzcHYW761aVSwLYdj8/OwqETI74alQoQJmz56N77//Hg4ODgCAlJQUzJ49GxUqVLB4gERERGQmlUokMFWrpi7kqdWKW2GGBOj0aeDMGZEEGSZZTEutFv2ADAlQ+fKpj/PgsHmzE57ly5ejbdu2KFGihHFE1vnz5yGTybBjxw6LB0hEREQWoFQClSqJrVcvsU+nEx2kT50CLl1KHSZ//TqQnCxulV28mP5YRYqYtgYZEqKAAHGeXMjsqIKCgnDr1i2sX78eV65cAQB06dIF3bt3h4uLi8UDJCIiIitRKMS6Yi/fodHpgNu3UxOgtNv9+8CjR2I7fNj0fSqVGCJfv75YjDUXeaM0zMXFBQNftSQ9ERER5V0KhVg5PjAQaNnS9LXYWNEqZEiADKPGrl0Tw+avXAF8fGwT9yu8UcLz448/YsWKFbh16xbCwsLg5+eH+fPnIzAwEO+9956lYyQiIqLcwtU1dah7Wno9cO+eSH5y4W0ts2ckWrZsGUaPHo2WLVvi+fPnxokGCxYsiAULFlg6PiIiIsoL5HKx9EXTpmL1+VzG7IRn0aJFWLlyJSZNmgRlmgyudu3auHDhgkWDIyIiIrIEsxOe8PBw1KhRI91+tVqN+Ph4iwRFREREZElmJzwBAQE4e/Zsuv27d+/mPDxERESUK5ndq2j06NEYOnQokpKSIEkSTp48iZ9++sk4GSERERFRbmN2wtO/f384OTnh888/R0JCArp3745ixYrh//7v/9C1a1drxEhERESULWYlPFqtFhs2bEDz5s3Ro0cPJCQkIC4uDkWKFLFWfERERETZZlYfHqVSiUGDBiEpKQkA4OzszGSHiIiIcj2zOy0HBQXhzJkz1oiFiIiIyCrM7sMzZMgQfPrpp7h37x5q1aqVbv0sw4KiRERERLmF2QmPoWPyiBEjjPtkMhkkSYJMJjPOvExERESUW5id8ISHh1sjDiIiIiKrMTvh8fPzs0YcRERERFbzxsuZXrp0CXfu3EFKSorJ/nbt2mU7KCIiIiJLMjvhuXXrFjp06IALFy4Y++4Aoh8PAPbhISIiolzH7GHpI0eOREBAAB49egRnZ2dcvHgRhw4dQu3atXHgwAErhEhERESUPWa38ISFhWH//v3w8vKCXC6HXC5H/fr1MXv2bIwYMYJz9BAREVGuY3YLj06ng6urKwDAy8sL9+/fByA6M1+9etWy0RERERFZgNktPJUrV8a5c+cQEBCA4OBgfP3113BwcMB3332HwMBAa8RIRERElC1mJzyff/454uPjAQBffPEF2rRpgwYNGsDT0xObNm2yeIBERERE2WV2wtO8eXPj49KlS+PKlSt49uwZChYsaBypRURERJSbvPE8PGkVKlTIEochIiIisgqzE5533nnnlS05+/fvz1ZARERERJZmdsJTvXp1k+cajQZnz57Fv//+i969e1sqLiIiIiKLMTvhmT9/fob7p02bhri4uGwHRERERGRpZs/Dk5mePXti9erVljocERERkcVYLOEJCwuDo6OjpQ5HREREZDFm39Lq2LGjyXNJkhAZGYl//vkHkydPtlhgRERERJZidguPu7u7yVaoUCE0btwYu3btwtSpU98oiCVLlsDf3x+Ojo4IDg7GyZMns/S+jRs3QiaToX379m90XiIiIsofzG7hWbNmjUUD2LRpE0aPHo3ly5cjODgYCxYsQPPmzXH16lUUKVIk0/dFRERgzJgxaNCggUXjISIiIvtjsT48b2revHkYMGAA+vTpg4oVK2L58uVwdnZ+ZQdonU6HHj16YPr06Vy/i4iIiF7L7BYec5aQePbs2StfT0lJwalTpzBhwgTjPrlcjpCQEISFhWX6vi+++AJFihRBv379cPjw4VeeIzk5GcnJycbnMTExAMT8QRqNJisfI8sMx7P0cenVWO+2wXq3Dda7bbDebSNtvWe37s1OeCZPnoyZM2eiefPmqFOnDgAxQuvPP//E5MmTzVpm4smTJ9DpdPD29jbZ7+3tjStXrmT4niNHjmDVqlU4e/Zsls4xe/ZsTJ8+Pd3+PXv2wNnZOcuxmiM0NNQqx6VXY73bBuvdNljvtsF6t43Q0FAkJCRk6xhmJzxHjx7FF198gWHDhhn3jRgxAosXL8bevXuxbdu2bAX0KrGxsfjwww+xcuVKeHl5Zek9EyZMwOjRo43PY2Ji4Ovri2bNmsHNzc2i8Wk0GoSGhqJp06ZQqVQWPTZljvVuG6x322C92wbr3TbS1ntiYmK2jmV2wvPnn39izpw56fa3aNEC48ePN+tYXl5eUCgUePjwocn+hw8fwsfHJ135mzdvIiIiAm3btjXu0+v1AAClUomrV6+iVKlSJu9Rq9VQq9XpjqVSqax20Vrz2JQ51rttsN5tg/VuG6x321CpVNBqtdk6htmdlj09PbF9+/Z0+7dv3w5PT0+zjuXg4IBatWph3759xn16vR779u0z3i5Lq3z58rhw4QLOnj1r3Nq1a4d33nkHZ8+eha+vr7kfh4iIiPIBs1t4pk+fjv79++PAgQMIDg4GAJw4cQK7d+/GypUrzQ5g9OjR6N27N2rXro2goCAsWLAA8fHx6NOnDwCgV69eKF68OGbPng1HR0dUrlzZ5P0eHh4AkG4/ERERkYHZCc9HH32EChUqYOHChfjll18AABUqVMCRI0eMCZA5unTpgsePH2PKlCl48OABqlevjt27dxs7Mt+5cwdyuc1HzxMREVEeZnbCAwDBwcFYv369xYIYNmyYSSfotA4cOPDK965du9ZicRAREZF9Mrvp5PTp07hw4YLx+fbt29G+fXtMnDgRKSkpFg2OiIiIyBLMTng+/vhjXLt2DQBw69YtdOnSBc7OztiyZQvGjRtn8QCJiIiIssvshOfatWuoXr06AGDLli1o1KgRNmzYgLVr1+Lnn3+2dHxERERE2WZ2wiNJknHum71796JVq1YAAF9fXzx58sSy0RERERFZgNkJT+3atTFz5kz8+OOPOHjwIFq3bg0ACA8PT7dEBBEREVFuYHbCs2DBApw+fRrDhg3DpEmTULp0aQDA1q1bUbduXYsHSERERJRdZg9Lr1q1qskoLYO5c+dCoVBYJCgiIiIiS3qjeXgy4ujoaKlDEREREVkUpzAmIiIiu8eEh4iIiOweEx4iIiKye0x4iIiIyO6Z3WlZp9Nh7dq12LdvHx49emSchNBg//79FguOiIiIyBLMTnhGjhyJtWvXonXr1qhcuTJkMpk14iIiIiKyGLMTno0bN2Lz5s3GJSWIiIiIcjuz+/A4ODgYZ1cmIiIiygvMTng+/fRT/N///R8kSbJGPEREREQWZ/YtrSNHjuCvv/7CH3/8gUqVKkGlUpm8/ssvv1gsOCIiIiJLMDvh8fDwQIcOHawRCxEREZFVmJ3wrFmzxhpxEBEREVkNJx4kIiIiu/dGq6Vv3boVmzdvxp07d5CSkmLy2unTpy0SGBEREZGlmN3Cs3DhQvTp0wfe3t44c+YMgoKC4OnpiVu3bqFly5bWiJGIiIgoW8xOeJYuXYrvvvsOixYtgoODA8aNG4fQ0FCMGDEC0dHR1oiRiIiIKFvMTnju3LmDunXrAgCcnJwQGxsLAPjwww/x008/WTY6IiIiIgswO+Hx8fHBs2fPAAAlS5bE8ePHAQDh4eGcjJCIiIhyJbMTnnfffRe//fYbAKBPnz4YNWoUmjZtii5dunB+HiIiIsqVzB6l9d1330Gv1wMAhg4dCk9PTxw7dgzt2rXDxx9/bPEAiYiIiLLL7IRHLpdDLk9tGOratSu6du1q0aCIiIiILOmNJh48fPgwevbsiTp16uC///4DAPz44484cuSIRYMjIiIisgSzE56ff/4ZzZs3h5OTE86cOYPk5GQAQHR0NL788kuLB0hERESUXWYnPDNnzsTy5cuxcuVKk5XS69Wrx1mWiYiIKFcyO+G5evUqGjZsmG6/u7s7oqKiLBETERERkUW90Tw8N27cSLf/yJEjCAwMtEhQRERERJZkdsIzYMAAjBw5EidOnIBMJsP9+/exfv16jBkzBoMHD7ZGjERERETZYvaw9PHjx0Ov16NJkyZISEhAw4YNoVarMWbMGAwfPtwaMRIRERFli9kJj0wmw6RJkzB27FjcuHEDcXFxqFixIgoUKGCN+IiIiIiyzeyEx8DBwQEVK1a0ZCxEREREVpHlhKdv375ZKrd69eo3DoaIiIjIGrKc8KxduxZ+fn6oUaMGV0UnIiKiPCXLCc/gwYPx008/ITw8HH369EHPnj1RqFAha8ZGREREZBFZHpa+ZMkSREZGYty4cdixYwd8fX3xwQcf4M8//2SLDxEREeVqZs3Do1ar0a1bN4SGhuLSpUuoVKkShgwZAn9/f8TFxVkrRiIiIqJseaPV0gFALpdDJpNBkiTodDpLxkRERERkUWYlPMnJyfjpp5/QtGlTlC1bFhcuXMDixYtx584dzsNDREREuVaWOy0PGTIEGzduhK+vL/r27YuffvoJXl5e1oyNiIiIyCKynPAsX74cJUuWRGBgIA4ePIiDBw9mWO6XX36xWHBERERElpDlhKdXr16QyWTWjIWIiIjIKsyaeJCIiIgoL3rjUVpEREREeQUTHiIiIrJ7THiIiIjI7jHhISIiIrvHhIeIiIjsHhMeIiIisntMeIiIiMju5YqEZ8mSJfD394ejoyOCg4Nx8uTJTMuuXLkSDRo0QMGCBVGwYEGEhIS8sjwRERGRzROeTZs2YfTo0Zg6dSpOnz6NatWqoXnz5nj06FGG5Q8cOIBu3brhr7/+QlhYGHx9fdGsWTP8999/ORw5ERER5RU2T3jmzZuHAQMGoE+fPqhYsSKWL18OZ2dnrF69OsPy69evx5AhQ1C9enWUL18e33//PfR6Pfbt25fDkRMREVFekeWlJawhJSUFp06dwoQJE4z75HI5QkJCEBYWlqVjJCQkQKPRoFChQhm+npycjOTkZOPzmJgYAIBGo4FGo8lG9OkZjmfp49Krsd5tg/VuG6x322C920baes9u3ds04Xny5Al0Oh28vb1N9nt7e+PKlStZOsZnn32GYsWKISQkJMPXZ8+ejenTp6fbv2fPHjg7O5sfdBaEhoZa5bj0aqx322C92wbr3TZY77YRGhqKhISEbB3DpglPdn311VfYuHEjDhw4AEdHxwzLTJgwAaNHjzY+j4mJMfb7cXNzs2g8Go0GoaGhaNq0KVQqlUWPTZljvdsG6902WO+2wXq3jbT1npiYmK1j2TTh8fLygkKhwMOHD032P3z4ED4+Pq987zfffIOvvvoKe/fuRdWqVTMtp1aroVar0+1XqVRWu2iteWzKHOvdNljvtsF6tw3Wu22oVCpotdpsHcOmnZYdHBxQq1Ytkw7Hhg7IderUyfR9X3/9NWbMmIHdu3ejdu3aOREqERER5WE2v6U1evRo9O7dG7Vr10ZQUBAWLFiA+Ph49OnTBwDQq1cvFC9eHLNnzwYAzJkzB1OmTMGGDRvg7++PBw8eAAAKFCiAAgUK2OxzEBERUe5l84SnS5cuePz4MaZMmYIHDx6gevXq2L17t7Ej8507dyCXpzZELVu2DCkpKejcubPJcaZOnYpp06blZOhERESUR9g84QGAYcOGYdiwYRm+duDAAZPnERER1g+IiIiI7IrNJx4kIiIisjYmPERERGT3mPAQERGR3WPCQ0RERHaPCQ8RERHZPSY8REREZPeY8BAREZHdY8JDREREdo8JDxEREdk9JjxERERk95jwEBERkd1jwkNERER2jwkPERER2T0mPERERGT3mPAQERGR3WPCQ0RERHaPCQ8RERHZPSY8REREZPeY8BAREZHdY8JDREREdo8JDxEREdk9JjxERERk95jwEBERkd1jwkNERER2jwkPERER2T0mPERERGT3mPAQERGR3WPCQ0RERHaPCQ8RERHZPSY8REREZPeY8BAREZHdY8JDREREdo8JDxEREdk9JjxERERk95jwEBERkd1jwkNERER2jwkPERER2T0mPERERGT3mPAQERGR3WPCQ0RERHaPCQ8RERHZPSY8REREZPeY8BAREZHdY8JDREREdo8JDxEREdk9JjxERERk95jwEBERkd1jwkNERER2jwkPERER2T0mPERERGT3mPAQERGR3WPCQ0RERHZPaesAiIiI6PUkSYLmsQZJd5KQFJGEhEsJ0MZo4VTaCQXfKQjncs62DjFXY8JDRPSSmJMxiD4WDUkrwSnACY6BjnAMcITKQ2Xr0MiO6TV6JP+XjOQ7yUi6nQSXKi6QKWVICk/C0x1P8WDNA0haKcP3yl3lcCnnAnVJNWRKGZ7veQ6VlwoOxR3g6O8Ip1JOUJdQQ11MDZeqLlAXVefwp7M9JjwWknAjAedbnIfrI1f8U+gfqAqpxMXm4wCHYg5Ql1TDvY47XGu4AhCZOgDIZDJbhk2Ur0iSBM1TDZJuJiHxZiISbyYi6VYSEq4noNQ3paCL0SHpdhIiv49E7MnYdO9XuCngVMoJFX+qaPxvOvm/ZOiT9FCXVEOusnwvAV2iDtpnWmieaaB9pkXS0yQoLyihe0cHlUokYJJegkzO3yVvQpIkSFoJkkaCPkUPSSMeK92VULgoAACaKA1ir8RCcU2B2MKxUCqUxt/hkADHAEeofUQCoY3WIu5CHCDBuKUt61TKCY4lHQEAMadjcPuL20i8mYiU+ynQPteK95hDDkAP6GP1iP0nFrH/pF632igtEm8kIvpgtMlbCtQqgIJNCkLtq4aUIuHB2gdQl1TDoYgDZA4yyNVyyFQyyB3k8GrvBbcgNwBA0j2ReMkdxOsyB1HG8NW5vLPxs+VGuSLhWbJkCebOnYsHDx6gWrVqWLRoEYKCgjItv2XLFkyePBkREREoU6YM5syZg1atWuVgxOkl30tG0s0kyCFHcmwykm8nZ1hO4aaAQ1EHKAsqEXsiFnK1HHIXOZRuSigLKaEqLJIkrzZeKNypMABA80yDewvuATIAshdJkgyAXDx2re2KQs0LAQB08Tr8t+y/1DIyQCaXQe4oh9xRDqeyTnCv4w5A/JKMORljfM24qVO/5uVfopIkiV8EmtRfYpJG/HKDDFAXS/0PJ/5iPLSxWvGaXtSN4avcQQ6Phh7GslGHo6B5qklXDnpAppShyAdFjGWf/vEUyf8lA3qk/uJ78RhyoPjg4sayT7Y/QeKNRHEswy9Kw2MAvuN8IVeKP6jP/nyGhGsJkClkkClk4lowPFYARd4vArlalI09GyuuR4W4FmRK0821lquxbMrDFGijteJYyvSbwlVhvCYkSQKSgZRHKdAma6GL05lsBZsVhLKA0hjvs93PjK9pY7XQxeigjdZCF6tDqW9Kwbm8MxQuCjz/6zkeb30MpbsSSjclFAUUULimbp6tPI3fO80zDTRPNVC4KqB0VULuLIekk5B8NxlJt5Lg9rYb5M5yaJ9pEfFFBCK/j4Q+QZ/h9XKmzpnXXlO6GB3izsThVPApOAU4Qe2nRvLdZMSdjgNkgIOPA5xKOcGpnJP4GuiEQq0KQekq6iH5v2Qk/5cM7fPUBEbzXHwGzWMNAmYGQBcrEpzbs28jan8UJE36v4AucMH1C9fhVNIJyoJKPP/rOWKOxBjrSumhhMpTBZWnCspCSgR8EQAHbwcAQNyFOCTfSYbSQwmFqwKSVoI+WQ8pRXx1b+AOhZP4Qx/zdwzizsYZX9OnvCiXooeULMF3jC8cvB0gSRIe//wYj7c+Fj9DOkCv1QM6QNJJkHQSig8tDgcfB0gpEp7ve46nu56Kn50Xr0MnrndJL8HrPS+oS4jvcfyFeET9FSV+HiCZJgQS4NXRC06lnETZf+Px7PdnJgmG8Rx6CUW6FUGBagUgk8sQeyoW91fcB3QZf6+LDSkGj8YegAyIOx2HO7PvoAAK4DzOpyvr+Z4n3N5yg16jR+KNRDxa/yjTa8i5ojMcijogKULclsrs/AbKgko4BjjC0c8RKk8VnKs6w6Wsi2h1LOkImYMMmicaJN9NFtf9nSQk3khEwuUEJN1NguahBtoo00Qq7lQc4k7FmZwn/kJ8hue/t+AeFC4KyJQySFoJmieaTGM1xClTyuBcyRllFpR59YfLYTLJeGXYxqZNm9CrVy8sX74cwcHBWLBgAbZs2YKrV6+iSJEi6cofO3YMDRs2xOzZs9GmTRts2LABc+bMwenTp1G5cuXXni8mJgbu7u6Ijo6Gm5ubxT6HJkqDJ3ue4Pz35+Gj8oH2qRbaZ1poo7TQxeugT9KLP3RZpQKUbkrI5DJIegnap9pMiyrcFXDwcRBldRISryVmWtahuAPcgtwgd5QDcrzyB7NArQIo8UkJkfioZbg24Jr4g5r2D/GLx661XOE7xtf4R/1S10vG5OHl8i5VXeA/1d+YjF3qcQn6eL1JgmZ47FLRBWWWlRF/qBUyXPzgoviBk8MkqUtOTIZbOTeUW17O+Mv7UrdLSIlMybh6vVQo8UkJ8Us8WY/IVZGZ1rHcWfyXY4j/WeizTMvKVDIUal7I+HljTsZk/r2TAQVDChqfxp2Je+UvE/fG7sa6if83HpqHryj7rruxtSHhYgKS72WcgANAkZ5FoCqkglwtR9SBKMT+nb5lw6DM0jJwDHCE3EGO+2vu4/H/Hmda1rOtJ2QKGbQxWiRcSUDK/Yy/F+ZyqeoCdTE15M5yJN9NNo33xfVg+FlTB6iheaQR19dryF3kcPQXv7ANGxQw3l5IvJWI5NvJ0MW+5i/US4p0KwLnCs6ADHj4v4dIvJr5z6e1qLxFi7OqoArJ90UymBnP9zwhd5BDn6xH3Pk4JEdkfu3IC8gBLaBP1pvfMkGCAlAVUcHJzwlOZZ3gXMkZzqWcLXobVdJJSI5MNiZFhsQo+W6yaOW8nQRdtHnX9eu41XVDzaM1s30cjUaDXbt2oVWrVkhMTMzW32+bJzzBwcF46623sHjxYgCAXq+Hr68vhg8fjvHjx6cr36VLF8THx2Pnzp3GfW+//TaqV6+O5cuXv/Z81kp4ANNvjKGpOS1trBYp91OMF17iDXGhpfyXguQHydA8Ef/xSSn8zUH5hBxQOCsgLyAXtxTi9eKfAytQFlbCwdsBzmVe/DHxE/8hq/3UcPRzhNJDmaVbzNoYrfhjcVskQoYt8UYikiKSXvnPyWvJYWyVUbgoxD8zhR1EwlJIbDIXGc7/cx7li5eHPkYP7XMtkh8lQ/NY/P7QReugjdFm2pKVY+SixVOmkAFKQFlACbmTHHIHuWjJSZFEq6MqtQVRrpIDCkDto4bcSSTsulidaKHAixaelxiSdUDc/tNFpf7hliBac2WQQZJJ4lpTvTi/VoKULKWWkUQZ4MXjNK2xkiRaoZ7HPoentycUaoXxlo9MJcv8sYP4TC8/lju9SK4DHKEups4VLen6ZD10cTpRLxbYlIWU8Gzpme24LJnw2PSWVkpKCk6dOoUJEyYY98nlcoSEhCAsLCzD94SFhWH06NEm+5o3b45t27ZZM1SLULoqoSynfG1Pem2sFikPU0RTdtrbJlL62y0mLSiZlJW04peLPklvbNHQJ4kmaeO+F191iaI1ShevA1LEfwb6JD10STpon2tFTGlup+HFz6lM+eK+74t9+iR9huUgF5vCQSFi1AG6JJ1o0n7R9Jy2CVomifdK+hfN5Cl6UUZK0xQuAXrooXRWQuGkELfk1KIFy/BYppaZ3rJTy433qk0eO4j3pW1lys5jY/0Y/pAa6uGlr5m+/vJj6aWvaR6bNONnUt7YX8FwiyLNrQyT2xbJpo91yeK6kFJe9HV4ca0kaBPgXtwdDu4OULgpjLeWFK4KKNxe/1juJE+XZEh6CboEHfQJ4jrUJ+ihS9ClPk7zVZeggz5eD238i9bUGB2gh7il5O8EdUmRzKh91cbbNNmldFOiQOUCKFC5QIav61P0xv+gk26/SIzuJEEml0FZ8MWt60IqKAu++JrmucJV8dqkS6PRIMUrBSValcjwHysDSSdBG6OF9rnYNM81xsfa56K+tLFa8UdYnfqzYPx5Mfe5Q+rPkkwps7v+iYY/vI1aNXplvedVxt+bdsymCc+TJ0+g0+ng7e1tst/b2xtXrlzJ8D0PHjzIsPyDBw8yLJ+cnIzk5NQm2ZiYGADi4tVoMr8l8CYMx8v2cR0BlZ/9/UBZi0ajQWhoKJo2bWqXv4hyK0O912la543qXQ+96OeRETUgU4sEwRL00EOvyaEWDxmgLKlEgZIFUKB+xklRRiRI0Gpf3zpk1u+ZAqJlRemrhCOs35lUb7iXmI1GrtzKYr/fySxp6z27dZ8rOi1b0+zZszF9+vR0+/fs2QNnZ+vMWRAaGmqV49Krsd5tg/VuG6x322C920ZoaCgSEhKydQybJjxeXl5QKBR4+PChyf6HDx/Cx8cnw/f4+PiYVX7ChAkmt8BiYmLg6+uLZs2aWaUPD1sach7r3TZY77bBercN1rttpK33xMTsdfi3acLj4OCAWrVqYd++fWjfvj0A0Wl53759GDZsWIbvqVOnDvbt24dPPvnEuC80NBR16tTJsLxarYZanX6CJZVKZbWL1prHpsyx3m2D9W4brHfbYL3bhkqlytIt31ex+S2t0aNHo3fv3qhduzaCgoKwYMECxMfHo0+fPgCAXr16oXjx4pg9ezYAYOTIkWjUqBG+/fZbtG7dGhs3bsQ///yD7777zpYfg4iIiHIxmyc8Xbp0wePHjzFlyhQ8ePAA1atXx+7du40dk+/cuQO5PLXneN26dbFhwwZ8/vnnmDhxIsqUKYNt27ZlaQ4eIiIiyp9snvAAwLBhwzK9hXXgwIF0+95//328//77Vo6KiIiI7IV9D7onIiIiAhMeIiIiygeY8BAREZHdY8JDREREdo8JDxEREdk9JjxERERk95jwEBERkd1jwkNERER2L1dMPJiTJEkCIBYRtTSNRoOEhATExMRwrZUcxHq3Dda7bbDebYP1bhtp692weKjh77i58l3CExsbCwDw9fW1cSRERERkrtjYWLi7u5v9Ppn0pqlSHqXX63H//n24urpCJpNZ9NgxMTHw9fXF3bt34ebmZtFjU+ZY77bBercN1rttsN5tI229u7q6IjY2FsWKFTNZYzOr8l0Lj1wuR4kSJax6Djc3N/5A2ADr3TZY77bBercN1rttGOr9TVp2DNhpmYiIiOweEx4iIiKye0x4LEitVmPq1KlQq9W2DiVfYb3bBuvdNljvtsF6tw1L1nu+67RMRERE+Q9beIiIiMjuMeEhIiIiu8eEh4iIiOweEx4iIiKye0x4LGTJkiXw9/eHo6MjgoODcfLkSVuHZNemTZsGmUxmspUvX97WYdmdQ4cOoW3btihWrBhkMhm2bdtm8rokSZgyZQqKFi0KJycnhISE4Pr167YJ1o68rt4/+uijdNd/ixYtbBOsHZk9ezbeeustuLq6okiRImjfvj2uXr1qUiYpKQlDhw6Fp6cnChQogE6dOuHhw4c2itg+ZKXeGzdunO6aHzRokFnnYcJjAZs2bcLo0aMxdepUnD59GtWqVUPz5s3x6NEjW4dm1ypVqoTIyEjjduTIEVuHZHfi4+NRrVo1LFmyJMPXv/76ayxcuBDLly/HiRMn4OLigubNmyMpKSmHI7Uvr6t3AGjRooXJ9f/TTz/lYIT26eDBgxg6dCiOHz+O0NBQaDQaNGvWDPHx8cYyo0aNwo4dO7BlyxYcPHgQ9+/fR8eOHW0Ydd6XlXoHgAEDBphc819//bV5J5Io24KCgqShQ4can+t0OqlYsWLS7NmzbRiVfZs6dapUrVo1W4eRrwCQfv31V+NzvV4v+fj4SHPnzjXui4qKktRqtfTTTz/ZIEL79HK9S5Ik9e7dW3rvvfdsEk9+8ujRIwmAdPDgQUmSxPWtUqmkLVu2GMtcvnxZAiCFhYXZKky783K9S5IkNWrUSBo5cmS2jssWnmxKSUnBqVOnEBISYtwnl8sREhKCsLAwG0Zm/65fv45ixYohMDAQPXr0wJ07d2wdUr4SHh6OBw8emFz77u7uCA4O5rWfAw4cOIAiRYqgXLlyGDx4MJ4+fWrrkOxOdHQ0AKBQoUIAgFOnTkGj0Zhc8+XLl0fJkiV5zVvQy/VusH79enh5eaFy5cqYMGECEhISzDpuvls81NKePHkCnU4Hb29vk/3e3t64cuWKjaKyf8HBwVi7di3KlSuHyMhITJ8+HQ0aNMC///4LV1dXW4eXLzx48AAAMrz2Da+RdbRo0QIdO3ZEQEAAbt68iYkTJ6Jly5YICwuDQqGwdXh2Qa/X45NPPkG9evVQuXJlAOKad3BwgIeHh0lZXvOWk1G9A0D37t3h5+eHYsWK4fz58/jss89w9epV/PLLL1k+NhMeypNatmxpfFy1alUEBwfDz88PmzdvRr9+/WwYGZH1de3a1fi4SpUqqFq1KkqVKoUDBw6gSZMmNozMfgwdOhT//vsv+wbmsMzqfeDAgcbHVapUQdGiRdGkSRPcvHkTpUqVytKxeUsrm7y8vKBQKNL10n/48CF8fHxsFFX+4+HhgbJly+LGjRu2DiXfMFzfvPZtLzAwEF5eXrz+LWTYsGHYuXMn/vrrL5QoUcK438fHBykpKYiKijIpz2veMjKr94wEBwcDgFnXPBOebHJwcECtWrWwb98+4z69Xo99+/ahTp06Nowsf4mLi8PNmzdRtGhRW4eSbwQEBMDHx8fk2o+JicGJEyd47eewe/fu4enTp7z+s0mSJAwbNgy//vor9u/fj4CAAJPXa9WqBZVKZXLNX716FXfu3OE1nw2vq/eMnD17FgDMuuZ5S8sCRo8ejd69e6N27doICgrCggULEB8fjz59+tg6NLs1ZswYtG3bFn5+frh//z6mTp0KhUKBbt262To0uxIXF2fyH1R4eDjOnj2LQoUKoWTJkvjkk08wc+ZMlClTBgEBAZg8eTKKFSuG9u3b2y5oO/Cqei9UqBCmT5+OTp06wcfHBzdv3sS4ceNQunRpNG/e3IZR531Dhw7Fhg0bsH37dri6uhr75bi7u8PJyQnu7u7o168fRo8ejUKFCsHNzQ3Dhw9HnTp18Pbbb9s4+rzrdfV+8+ZNbNiwAa1atYKnpyfOnz+PUaNGoWHDhqhatWrWT5StMV5ktGjRIqlkyZKSg4ODFBQUJB0/ftzWIdm1Ll26SEWLFpUcHByk4sWLS126dJFu3Lhh67Dszl9//SUBSLf17t1bkiQxNH3y5MmSt7e3pFarpSZNmkhXr161bdB24FX1npCQIDVr1kwqXLiwpFKpJD8/P2nAgAHSgwcPbB12npdRnQOQ1qxZYyyTmJgoDRkyRCpYsKDk7OwsdejQQYqMjLRd0HbgdfV+584dqWHDhlKhQoUktVotlS5dWho7dqwUHR1t1nlkL05GREREZLfYh4eIiIjsHhMeIiIisntMeIiIiMjuMeEhIiIiu8eEh4iIiOweEx4iIiKye0x4iIiIyO4x4SEiu9O4cWN88skntg6DiHIRTjxIRDYlk8le+frUqVMxbdo0s4757NkzqFQquLq6ZiMyIrInTHiIyKYM6+YAwKZNmzBlyhRcvXrVuK9AgQIoUKCALUIjIjvCW1pEZFM+Pj7Gzd3dHTKZzGRfZsnO0qVLUaZMGTg6OsLb2xudO3c2vpb2ltaBAwcgk8nSbR999JGx/Pbt21GzZk04OjoiMDAQ06dPh1artebHJqIcxtXSiSjP+eeffzBixAj8+OOPqFu3Lp49e4bDhw9nWLZu3bqIjIw0Pr98+TJatWqFhg0bAgAOHz6MXr16YeHChWjQoAFu3ryJgQMHAhC304jIPjDhIaI8586dO3BxcUGbNm3g6uoKPz8/1KhRI8OyDg4O8PHxAQA8ffoU/fv3R9++fdG3b18AwPTp0zF+/Hj07t0bABAYGIgZM2Zg3LhxTHiI7AgTHiLKc5o2bQo/Pz8EBgaiRYsWaNGiBTp06ABnZ+dM36PRaNCpUyf4+fnh//7v/4z7z507h6NHj2LWrFnGfTqdDklJSUhISHjlMYko72DCQ0R5jqurK06fPo0DBw5gz549mDJlCqZNm4a///4bHh4eGb5n8ODBuHv3Lk6ePAmlMvVXX1xcHKZPn46OHTume4+jo6O1PgIR5TAmPESUJymVSoSEhCAkJARTp06Fh4cH9u/fn2HiMm/ePGzevBnHjh2Dp6enyWs1a9bE1atXUbp06ZwKnYhsgAkPEeU5O3fuxK1bt9CwYUMULFgQu3btgl6vR7ly5dKV3bt3L8aNG4clS5bAy8vLOAzeyckJ7u7umDJlCtq0aYOSJUuic+fOkMvlOHfuHP7991/MnDkzpz8aEVkJh6UTUZ7j4eGBX375Be+++y4qVKiA5cuX46effkKlSpXSlT1y5Ah0Oh0GDRqEokWLGreRI0cCAJo3b46dO3diz549eOutt/D2229j/vz58PPzy+mPRURWxIkHiYiIyO6xhYeIiIjsHhMeIiIisntMeIiIiMjuMeEhIiIiu8eEh4iIiOweEx4iIiKye0x4iIiIyO4x4SEiIiK7x4SHiIiI7B4THiIiIrJ7THiIiIjI7jHhISIiIrv3/04r7jdVqqe2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ridge_t=np.load(\"/its/home/drs25/GonkRobot/Data/experimentdata/Ridge_T.npy\")\n",
    "#rf_t=np.load(\"/its/home/drs25/GonkRobot/Data/experimentdata/RF_T.npy\")\n",
    "ann_t=np.average(np.load(\"/its/home/drs25/GonkRobot/Data/experimentdata/FNN_T.npy\"),axis=1)\n",
    "lstm_t=np.average(np.load(\"/its/home/drs25/GonkRobot/Data/experimentdata/LSTM_T.npy\"),axis=1)\n",
    "\n",
    "plt.plot(ridge_t[:,0],c=\"r\",label=\"Ridge Classifier\")\n",
    "plt.plot(ridge_t[:,1],\"--\",c=\"r\")\n",
    "#plt.plot(rf_t[:,0],c=\"b\",label=\"Random Forest Classifier\")\n",
    "#plt.plot(rf_t[:,1],\"--\",c=\"b\")\n",
    "plt.plot(ann_t[:,0],c=\"m\",label=\"ANN\")\n",
    "plt.plot(ann_t[:,1],\"--\",c=\"m\",label=\"ANN\")\n",
    "#plt.plot(lstm_t,label=\"LSTM\")\n",
    "plt.grid()\n",
    "plt.ylabel(\"Mean squared error (MSE)\")\n",
    "plt.xlabel(\"T size\")\n",
    "plt.title(\"How T influences error\")\n",
    "plt.savefig(\"/its/home/drs25/GonkRobot/assets/Tsize.pdf\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify edges\n",
    "\n",
    "This section we experiment with a regression model to classify simply where an edge is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn import svm\n",
    "#filepath=\"/its/home/drs25/Documents/GitHub/TactileSensor/Code/Data collection/edges/\"\n",
    "if sys.platform.startswith('win'): filepath=\"C:/Users/dexte/Documents/GitHub/TactileSensor/Code/Data collection/edges/\"\n",
    "def load_edges(datapath=\"/its/home/drs25/GonkRobot/Data/\"):\n",
    "    names = [\n",
    "        \"east\", \"north-east\", \"north-west\", \"north\",\n",
    "        \"south-east\", \"south-west\", \"south\", \"west\"\n",
    "    ]\n",
    "\n",
    "    X_all = []\n",
    "    y_all = []\n",
    "\n",
    "    for i, name in enumerate(names):\n",
    "        X = np.load(datapath + name + \".npy\")\n",
    "\n",
    "        # keep frames that contain strong edges\n",
    "        mask = np.max(X, axis=(1)) > 6900\n",
    "\n",
    "        X = X[mask]\n",
    "\n",
    "        y = np.full(len(X), i)\n",
    "\n",
    "        X_all.append(X)\n",
    "        y_all.append(y)\n",
    "\n",
    "    X = np.concatenate(X_all, axis=0)\n",
    "    y = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2672, 10) (2672,)\n",
      "Train: 66.58867571361722 Test: 66.9158878504673\n"
     ]
    }
   ],
   "source": [
    "X,y=load_edges()\n",
    "x_train,x_test,y_train,y_test=train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "alpha = 1.0  # Ridge regularization parameter (adjust as needed)\n",
    "ridge_model = RidgeClassifier(alpha=alpha)\n",
    "#ridge_model = LinearRegression()\n",
    "# Fit the model to the training data\n",
    "ridge_model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "Y_pred = ridge_model.predict(x_train)\n",
    "train=accuracy_score(y_train,Y_pred)\n",
    "Y_pred = ridge_model.predict(x_test)\n",
    "test=accuracy_score(y_test,Y_pred)\n",
    "print(\"Train:\",train*100,\"Test:\",test*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 82.73280299485259 Test: 83.3644859813084\n"
     ]
    }
   ],
   "source": [
    "X,y=load_edges()\n",
    "x_train,x_test,y_train,y_test=train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm_ = svm.SVC()\n",
    "#ridge_model = LinearRegression()\n",
    "# Fit the model to the training data\n",
    "svm_.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "Y_pred = svm_.predict(x_train)\n",
    "train=accuracy_score(y_train,Y_pred)\n",
    "Y_pred = svm_.predict(x_test)\n",
    "test=accuracy_score(y_test,Y_pred)\n",
    "print(\"Train:\",train*100,\"Test:\",test*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 100.0 Test: 95.88785046728971\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X,y=load_edges()\n",
    "x_train,x_test,y_train,y_test=train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "alpha = 1.0  # Ridge regularization parameter (adjust as needed)\n",
    "forest_model = RandomForestClassifier(n_estimators=25, random_state=42)\n",
    "#ridge_model = LinearRegression()\n",
    "# Fit the model to the training data\n",
    "forest_model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "Y_pred = forest_model.predict(x_train)\n",
    "train=accuracy_score(y_train,Y_pred)\n",
    "Y_pred = forest_model.predict(x_test)\n",
    "test=accuracy_score(y_test,Y_pred)\n",
    "print(\"Train:\",train*100,\"Test:\",test*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nueral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=np.load(filepath+\"xdata_soft.npy\")\n",
    "y=np.load(filepath+\"ydata1_soft.npy\")\n",
    "\n",
    "#mean_values = np.mean(x, axis=1, keepdims=True)\n",
    "#std_dev_values = np.std(x, axis=1, keepdims=True)\n",
    "#x = (x - mean_values) / std_dev_values\n",
    "\n",
    "n=np.zeros_like(x[0])\n",
    "s=np.zeros_like(x[0])\n",
    "w=np.zeros_like(x[0])\n",
    "e=np.zeros_like(x[0])\n",
    "for i in range(len(x)):\n",
    "    if list(y[i])==[1,1,0,0]: \n",
    "        sig=x[i]\n",
    "        #sig[sig!=np.max(sig)]=0\n",
    "        n+=sig\n",
    "    elif list(y[i])==[0,1,1,0]: \n",
    "        sig=x[i]\n",
    "        #sig[sig!=np.max(sig)]=0\n",
    "        e+=sig\n",
    "    elif list(y[i])==[0,0,1,1]: \n",
    "        sig=x[i]\n",
    "        #sig[sig!=np.max(sig)]=0\n",
    "        s+=sig\n",
    "    elif list(y[i])==[1,0,0,1]: \n",
    "        sig=x[i]\n",
    "        #sig[sig!=np.max(sig)]=0\n",
    "        w+=sig\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 6))\n",
    "\n",
    "axs[0][0].imshow(n,cmap='inferno')\n",
    "axs[0][0].set_title(\"North\")\n",
    "axs[0][0].xaxis.set_visible(False)\n",
    "axs[0][0].yaxis.set_visible(False)\n",
    "\n",
    "axs[0][1].imshow(e,cmap='inferno')\n",
    "axs[0][1].set_title(\"East\")\n",
    "axs[0][1].xaxis.set_visible(False)\n",
    "axs[0][1].yaxis.set_visible(False)\n",
    "\n",
    "axs[1][0].imshow(s,cmap='inferno')\n",
    "axs[1][0].set_title(\"South\")\n",
    "axs[1][0].xaxis.set_visible(False)\n",
    "axs[1][0].yaxis.set_visible(False)\n",
    "\n",
    "axs[1][1].imshow(w,cmap='inferno')\n",
    "axs[1][1].set_title(\"West\")\n",
    "axs[1][1].xaxis.set_visible(False)\n",
    "axs[1][1].yaxis.set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X data: (4950, 30) /ny data: (4950, 3)\n",
      "X data: (4950, 30) /ny data: (4950, 3)\n",
      "X data: (4950, 30) /ny data: (4950, 3)\n",
      "X data: (4950, 30) /ny data: (4950, 3)\n",
      "X data: (4950, 30) /ny data: (4950, 3)\n",
      "X data: (4950, 30) /ny data: (4950, 3)\n",
      "X data: (4950, 30) /ny data: (4950, 3)\n",
      "X data: (4950, 30) /ny data: (4950, 3)\n",
      "X data: (4950, 30) /ny data: (4950, 3)\n",
      "X data: (4950, 30) /ny data: (4950, 3)\n",
      "(49500, 15)\n",
      "1.1363636363636365 %\n",
      "2.272727272727273 %\n",
      "3.4090909090909087 %\n",
      "4.545454545454546 %\n",
      "5.681818181818182 %\n",
      "6.8181818181818175 %\n",
      "7.954545454545454 %\n",
      "9.090909090909092 %\n",
      "10.227272727272728 %\n",
      "11.363636363636363 %\n",
      "12.5 %\n",
      "13.636363636363635 %\n",
      "14.772727272727273 %\n",
      "15.909090909090908 %\n",
      "17.045454545454543 %\n",
      "18.181818181818183 %\n",
      "19.318181818181817 %\n",
      "20.454545454545457 %\n",
      "21.59090909090909 %\n",
      "22.727272727272727 %\n",
      "23.863636363636363 %\n",
      "25.0 %\n",
      "26.136363636363637 %\n",
      "27.27272727272727 %\n",
      "28.40909090909091 %\n",
      "29.545454545454547 %\n",
      "30.681818181818183 %\n",
      "31.818181818181817 %\n",
      "32.95454545454545 %\n",
      "34.090909090909086 %\n",
      "35.22727272727273 %\n",
      "36.36363636363637 %\n",
      "37.5 %\n",
      "38.63636363636363 %\n",
      "39.77272727272727 %\n",
      "40.909090909090914 %\n",
      "42.04545454545455 %\n",
      "43.18181818181818 %\n",
      "44.31818181818182 %\n",
      "45.45454545454545 %\n",
      "46.590909090909086 %\n",
      "47.72727272727273 %\n",
      "48.86363636363637 %\n",
      "50.0 %\n",
      "51.13636363636363 %\n",
      "52.27272727272727 %\n",
      "53.40909090909091 %\n",
      "54.54545454545454 %\n",
      "55.68181818181818 %\n",
      "56.81818181818182 %\n",
      "57.95454545454546 %\n",
      "59.09090909090909 %\n",
      "60.22727272727273 %\n",
      "61.36363636363637 %\n",
      "62.5 %\n",
      "63.63636363636363 %\n",
      "64.77272727272727 %\n",
      "65.9090909090909 %\n",
      "67.04545454545455 %\n",
      "68.18181818181817 %\n",
      "69.31818181818183 %\n",
      "70.45454545454545 %\n",
      "71.5909090909091 %\n",
      "72.72727272727273 %\n",
      "73.86363636363636 %\n",
      "75.0 %\n",
      "76.13636363636364 %\n",
      "77.27272727272727 %\n",
      "78.4090909090909 %\n",
      "79.54545454545455 %\n",
      "80.68181818181817 %\n",
      "81.81818181818183 %\n",
      "82.95454545454545 %\n",
      "84.0909090909091 %\n",
      "85.22727272727273 %\n",
      "86.36363636363636 %\n",
      "87.5 %\n",
      "88.63636363636364 %\n",
      "89.77272727272727 %\n",
      "90.9090909090909 %\n",
      "92.04545454545455 %\n",
      "93.18181818181817 %\n",
      "94.31818181818183 %\n",
      "95.45454545454545 %\n",
      "96.5909090909091 %\n",
      "97.72727272727273 %\n",
      "98.86363636363636 %\n",
      "100.0 %\n",
      "101.13636363636364 %\n",
      "102.27272727272727 %\n",
      "103.40909090909092 %\n",
      "104.54545454545455 %\n",
      "105.68181818181819 %\n",
      "106.81818181818181 %\n",
      "107.95454545454545 %\n",
      "109.09090909090908 %\n",
      "110.22727272727273 %\n",
      "111.36363636363636 %\n",
      "112.5 %\n",
      "113.63636363636364 %\n",
      "114.77272727272727 %\n",
      "115.90909090909092 %\n",
      "117.04545454545455 %\n",
      "118.18181818181819 %\n",
      "119.31818181818181 %\n",
      "120.45454545454545 %\n",
      "121.59090909090908 %\n",
      "122.72727272727273 %\n",
      "123.86363636363636 %\n",
      "125.0 %\n",
      "126.13636363636364 %\n",
      "127.27272727272727 %\n",
      "128.4090909090909 %\n",
      "129.54545454545453 %\n",
      "130.6818181818182 %\n",
      "131.8181818181818 %\n",
      "132.95454545454547 %\n",
      "134.0909090909091 %\n",
      "135.22727272727272 %\n",
      "136.36363636363635 %\n",
      "137.5 %\n",
      "138.63636363636365 %\n",
      "139.77272727272728 %\n",
      "140.9090909090909 %\n",
      "142.04545454545453 %\n",
      "143.1818181818182 %\n",
      "144.3181818181818 %\n",
      "145.45454545454547 %\n",
      "146.5909090909091 %\n",
      "147.72727272727272 %\n",
      "148.86363636363635 %\n",
      "150.0 %\n",
      "151.13636363636365 %\n",
      "152.27272727272728 %\n",
      "153.4090909090909 %\n",
      "154.54545454545453 %\n",
      "155.6818181818182 %\n",
      "156.8181818181818 %\n",
      "157.95454545454547 %\n",
      "159.0909090909091 %\n",
      "160.22727272727272 %\n",
      "161.36363636363635 %\n",
      "162.5 %\n",
      "163.63636363636365 %\n",
      "164.77272727272728 %\n",
      "165.9090909090909 %\n",
      "167.04545454545453 %\n",
      "168.1818181818182 %\n",
      "169.3181818181818 %\n",
      "170.45454545454547 %\n",
      "171.5909090909091 %\n",
      "172.72727272727272 %\n",
      "173.86363636363635 %\n",
      "175.0 %\n",
      "176.13636363636365 %\n",
      "177.27272727272728 %\n",
      "178.4090909090909 %\n",
      "179.54545454545453 %\n",
      "180.6818181818182 %\n",
      "181.8181818181818 %\n",
      "182.95454545454547 %\n",
      "184.0909090909091 %\n",
      "185.22727272727272 %\n",
      "186.36363636363635 %\n",
      "187.5 %\n",
      "188.63636363636365 %\n",
      "189.77272727272728 %\n",
      "190.9090909090909 %\n",
      "192.04545454545453 %\n",
      "193.1818181818182 %\n",
      "194.3181818181818 %\n",
      "195.45454545454547 %\n",
      "196.5909090909091 %\n",
      "197.72727272727272 %\n",
      "198.86363636363635 %\n",
      "200.0 %\n",
      "201.13636363636363 %\n",
      "202.27272727272728 %\n",
      "203.4090909090909 %\n",
      "204.54545454545453 %\n",
      "205.68181818181816 %\n",
      "206.81818181818184 %\n",
      "207.95454545454547 %\n",
      "209.0909090909091 %\n",
      "210.22727272727272 %\n",
      "211.36363636363637 %\n",
      "212.5 %\n",
      "213.63636363636363 %\n",
      "214.77272727272728 %\n",
      "215.9090909090909 %\n",
      "217.04545454545453 %\n",
      "218.18181818181816 %\n",
      "219.31818181818184 %\n",
      "220.45454545454547 %\n",
      "221.5909090909091 %\n",
      "222.72727272727272 %\n",
      "223.86363636363637 %\n",
      "225.0 %\n",
      "226.13636363636363 %\n",
      "227.27272727272728 %\n",
      "228.4090909090909 %\n",
      "229.54545454545453 %\n",
      "230.68181818181816 %\n",
      "231.81818181818184 %\n",
      "232.95454545454547 %\n",
      "234.0909090909091 %\n",
      "235.22727272727272 %\n",
      "236.36363636363637 %\n",
      "237.5 %\n",
      "238.63636363636363 %\n",
      "239.77272727272728 %\n",
      "240.9090909090909 %\n",
      "242.04545454545453 %\n",
      "243.18181818181816 %\n",
      "244.31818181818184 %\n",
      "245.45454545454547 %\n",
      "246.5909090909091 %\n",
      "247.72727272727272 %\n",
      "248.86363636363637 %\n",
      "250.0 %\n",
      "251.13636363636363 %\n",
      "252.27272727272728 %\n",
      "253.4090909090909 %\n",
      "254.54545454545453 %\n",
      "255.68181818181816 %\n",
      "256.8181818181818 %\n",
      "257.95454545454544 %\n",
      "259.09090909090907 %\n",
      "260.2272727272727 %\n",
      "261.3636363636364 %\n",
      "262.5 %\n",
      "263.6363636363636 %\n",
      "264.7727272727273 %\n",
      "265.90909090909093 %\n",
      "267.04545454545456 %\n",
      "268.1818181818182 %\n",
      "269.3181818181818 %\n",
      "270.45454545454544 %\n",
      "271.59090909090907 %\n",
      "272.7272727272727 %\n",
      "273.8636363636364 %\n",
      "275.0 %\n",
      "276.1363636363636 %\n",
      "277.2727272727273 %\n",
      "278.40909090909093 %\n",
      "279.54545454545456 %\n",
      "280.6818181818182 %\n",
      "281.8181818181818 %\n",
      "282.95454545454544 %\n",
      "284.09090909090907 %\n",
      "285.2272727272727 %\n",
      "286.3636363636364 %\n",
      "287.5 %\n",
      "288.6363636363636 %\n",
      "289.7727272727273 %\n",
      "290.90909090909093 %\n",
      "292.04545454545456 %\n",
      "293.1818181818182 %\n",
      "294.3181818181818 %\n",
      "295.45454545454544 %\n",
      "296.59090909090907 %\n",
      "297.7272727272727 %\n",
      "298.8636363636364 %\n",
      "300.0 %\n",
      "301.1363636363636 %\n",
      "302.2727272727273 %\n",
      "303.40909090909093 %\n",
      "304.54545454545456 %\n",
      "305.6818181818182 %\n",
      "306.8181818181818 %\n",
      "307.95454545454544 %\n",
      "309.09090909090907 %\n",
      "310.2272727272727 %\n",
      "311.3636363636364 %\n",
      "312.5 %\n",
      "313.6363636363636 %\n",
      "314.7727272727273 %\n",
      "315.90909090909093 %\n",
      "317.04545454545456 %\n",
      "318.1818181818182 %\n",
      "319.3181818181818 %\n",
      "320.45454545454544 %\n",
      "321.59090909090907 %\n",
      "322.7272727272727 %\n",
      "323.8636363636364 %\n",
      "325.0 %\n",
      "326.1363636363636 %\n",
      "327.2727272727273 %\n",
      "328.40909090909093 %\n",
      "329.54545454545456 %\n",
      "330.6818181818182 %\n",
      "331.8181818181818 %\n",
      "332.95454545454544 %\n",
      "334.09090909090907 %\n",
      "335.2272727272727 %\n",
      "336.3636363636364 %\n",
      "337.5 %\n",
      "338.6363636363636 %\n",
      "339.7727272727273 %\n",
      "340.90909090909093 %\n",
      "342.04545454545456 %\n",
      "343.1818181818182 %\n",
      "344.3181818181818 %\n",
      "345.45454545454544 %\n",
      "346.59090909090907 %\n",
      "347.7272727272727 %\n",
      "348.8636363636364 %\n",
      "350.0 %\n",
      "351.1363636363636 %\n",
      "352.2727272727273 %\n",
      "353.40909090909093 %\n",
      "354.54545454545456 %\n",
      "355.6818181818182 %\n",
      "356.8181818181818 %\n",
      "357.95454545454544 %\n",
      "359.09090909090907 %\n",
      "360.2272727272727 %\n",
      "361.3636363636364 %\n",
      "362.5 %\n",
      "363.6363636363636 %\n",
      "364.7727272727273 %\n",
      "365.90909090909093 %\n",
      "367.04545454545456 %\n",
      "368.1818181818182 %\n",
      "369.3181818181818 %\n",
      "370.45454545454544 %\n",
      "371.59090909090907 %\n",
      "372.7272727272727 %\n",
      "373.8636363636364 %\n",
      "375.0 %\n",
      "376.1363636363636 %\n",
      "377.2727272727273 %\n",
      "378.40909090909093 %\n",
      "379.54545454545456 %\n",
      "380.6818181818182 %\n",
      "381.8181818181818 %\n",
      "382.95454545454544 %\n",
      "384.09090909090907 %\n",
      "385.2272727272727 %\n",
      "386.3636363636364 %\n",
      "387.5 %\n",
      "388.6363636363636 %\n",
      "389.7727272727273 %\n",
      "390.90909090909093 %\n",
      "392.04545454545456 %\n",
      "393.1818181818182 %\n",
      "394.3181818181818 %\n",
      "395.45454545454544 %\n",
      "396.59090909090907 %\n",
      "397.7272727272727 %\n",
      "398.8636363636364 %\n",
      "400.0 %\n",
      "401.1363636363637 %\n",
      "402.27272727272725 %\n",
      "403.40909090909093 %\n",
      "404.54545454545456 %\n",
      "405.6818181818182 %\n",
      "406.8181818181818 %\n",
      "407.95454545454544 %\n",
      "409.09090909090907 %\n",
      "410.22727272727275 %\n",
      "411.3636363636363 %\n",
      "412.5 %\n",
      "413.6363636363637 %\n",
      "414.77272727272725 %\n",
      "415.90909090909093 %\n",
      "417.04545454545456 %\n",
      "418.1818181818182 %\n",
      "419.3181818181818 %\n",
      "420.45454545454544 %\n",
      "421.59090909090907 %\n",
      "422.72727272727275 %\n",
      "423.8636363636363 %\n",
      "425.0 %\n",
      "426.1363636363637 %\n",
      "427.27272727272725 %\n",
      "428.40909090909093 %\n",
      "429.54545454545456 %\n",
      "430.6818181818182 %\n",
      "431.8181818181818 %\n",
      "432.95454545454544 %\n",
      "434.09090909090907 %\n",
      "435.22727272727275 %\n",
      "436.3636363636363 %\n",
      "437.5 %\n",
      "438.6363636363637 %\n",
      "439.77272727272725 %\n",
      "440.90909090909093 %\n",
      "442.04545454545456 %\n",
      "443.1818181818182 %\n",
      "444.3181818181818 %\n",
      "445.45454545454544 %\n",
      "446.59090909090907 %\n",
      "447.72727272727275 %\n",
      "448.8636363636363 %\n",
      "450.0 %\n",
      "451.1363636363637 %\n",
      "452.27272727272725 %\n",
      "453.40909090909093 %\n",
      "454.54545454545456 %\n",
      "455.6818181818182 %\n",
      "456.8181818181818 %\n",
      "457.95454545454544 %\n",
      "459.09090909090907 %\n",
      "460.22727272727275 %\n",
      "461.3636363636363 %\n",
      "462.5 %\n",
      "463.6363636363637 %\n",
      "464.77272727272725 %\n",
      "465.90909090909093 %\n",
      "467.04545454545456 %\n",
      "468.1818181818182 %\n",
      "469.3181818181818 %\n",
      "470.45454545454544 %\n",
      "471.59090909090907 %\n",
      "472.72727272727275 %\n",
      "473.8636363636363 %\n",
      "475.0 %\n",
      "476.1363636363637 %\n",
      "477.27272727272725 %\n",
      "478.40909090909093 %\n",
      "479.54545454545456 %\n",
      "480.6818181818182 %\n",
      "481.8181818181818 %\n",
      "482.95454545454544 %\n",
      "484.09090909090907 %\n",
      "485.22727272727275 %\n",
      "486.3636363636363 %\n",
      "487.5 %\n",
      "488.6363636363637 %\n",
      "489.77272727272725 %\n",
      "490.90909090909093 %\n",
      "492.04545454545456 %\n",
      "493.1818181818182 %\n",
      "494.3181818181818 %\n",
      "495.45454545454544 %\n",
      "496.59090909090907 %\n",
      "497.72727272727275 %\n",
      "498.8636363636363 %\n",
      "500.0 %\n",
      "501.1363636363637 %\n",
      "502.27272727272725 %\n",
      "503.40909090909093 %\n",
      "504.54545454545456 %\n",
      "505.6818181818182 %\n",
      "506.8181818181818 %\n",
      "507.95454545454544 %\n",
      "509.09090909090907 %\n",
      "510.22727272727275 %\n",
      "511.3636363636363 %\n",
      "512.5 %\n",
      "513.6363636363636 %\n",
      "514.7727272727273 %\n",
      "515.9090909090909 %\n",
      "517.0454545454546 %\n",
      "518.1818181818181 %\n",
      "519.3181818181819 %\n",
      "520.4545454545454 %\n",
      "521.5909090909091 %\n",
      "522.7272727272727 %\n",
      "523.8636363636364 %\n",
      "525.0 %\n",
      "526.1363636363636 %\n",
      "527.2727272727273 %\n",
      "528.4090909090909 %\n",
      "529.5454545454546 %\n",
      "530.6818181818181 %\n",
      "531.8181818181819 %\n",
      "532.9545454545454 %\n",
      "534.0909090909091 %\n",
      "535.2272727272727 %\n",
      "536.3636363636364 %\n",
      "537.5 %\n",
      "538.6363636363636 %\n",
      "539.7727272727273 %\n",
      "540.9090909090909 %\n",
      "542.0454545454546 %\n",
      "543.1818181818181 %\n",
      "544.3181818181819 %\n",
      "545.4545454545454 %\n",
      "546.5909090909091 %\n",
      "547.7272727272727 %\n",
      "548.8636363636364 %\n",
      "550.0 %\n",
      "551.1363636363636 %\n",
      "552.2727272727273 %\n",
      "553.4090909090909 %\n",
      "554.5454545454546 %\n",
      "555.6818181818181 %\n",
      "556.8181818181819 %\n",
      "557.9545454545454 %\n",
      "559.0909090909091 %\n",
      "560.2272727272727 %\n",
      "561.3636363636364 %\n",
      "562.5 %\n",
      "563.6363636363636 %\n",
      "564.7727272727273 %\n",
      "565.9090909090909 %\n",
      "567.0454545454546 %\n",
      "568.1818181818181 %\n",
      "569.3181818181819 %\n",
      "570.4545454545454 %\n",
      "571.5909090909091 %\n",
      "572.7272727272727 %\n",
      "573.8636363636364 %\n",
      "575.0 %\n",
      "576.1363636363636 %\n",
      "577.2727272727273 %\n",
      "578.4090909090909 %\n",
      "579.5454545454546 %\n",
      "580.6818181818181 %\n",
      "581.8181818181819 %\n",
      "582.9545454545454 %\n",
      "584.0909090909091 %\n",
      "585.2272727272727 %\n",
      "586.3636363636364 %\n",
      "587.5 %\n",
      "588.6363636363636 %\n",
      "589.7727272727273 %\n",
      "590.9090909090909 %\n",
      "592.0454545454546 %\n",
      "593.1818181818181 %\n",
      "594.3181818181819 %\n",
      "595.4545454545454 %\n",
      "596.5909090909091 %\n",
      "597.7272727272727 %\n",
      "598.8636363636364 %\n",
      "600.0 %\n",
      "601.1363636363636 %\n",
      "602.2727272727273 %\n",
      "603.4090909090909 %\n",
      "604.5454545454546 %\n",
      "605.6818181818181 %\n",
      "606.8181818181819 %\n",
      "607.9545454545454 %\n",
      "609.0909090909091 %\n",
      "610.2272727272727 %\n",
      "611.3636363636364 %\n",
      "612.5 %\n",
      "613.6363636363636 %\n",
      "614.7727272727273 %\n",
      "615.9090909090909 %\n",
      "617.0454545454546 %\n",
      "618.1818181818181 %\n",
      "619.3181818181819 %\n",
      "620.4545454545454 %\n",
      "621.5909090909091 %\n",
      "622.7272727272727 %\n",
      "623.8636363636364 %\n",
      "625.0 %\n",
      "626.1363636363636 %\n",
      "627.2727272727273 %\n",
      "628.4090909090909 %\n",
      "629.5454545454546 %\n",
      "630.6818181818181 %\n",
      "631.8181818181819 %\n",
      "632.9545454545454 %\n",
      "634.0909090909091 %\n",
      "635.2272727272727 %\n",
      "636.3636363636364 %\n",
      "637.5 %\n",
      "638.6363636363636 %\n",
      "639.7727272727273 %\n",
      "640.9090909090909 %\n",
      "642.0454545454546 %\n",
      "643.1818181818181 %\n",
      "644.3181818181819 %\n",
      "645.4545454545454 %\n",
      "646.5909090909091 %\n",
      "647.7272727272727 %\n",
      "648.8636363636364 %\n",
      "650.0 %\n",
      "651.1363636363636 %\n",
      "652.2727272727273 %\n",
      "653.4090909090909 %\n",
      "654.5454545454546 %\n",
      "655.6818181818181 %\n",
      "656.8181818181819 %\n",
      "657.9545454545454 %\n",
      "659.0909090909091 %\n",
      "660.2272727272727 %\n",
      "661.3636363636364 %\n",
      "662.5 %\n",
      "663.6363636363636 %\n",
      "664.7727272727273 %\n",
      "665.9090909090909 %\n",
      "667.0454545454546 %\n",
      "668.1818181818181 %\n",
      "669.3181818181819 %\n",
      "670.4545454545454 %\n",
      "671.5909090909091 %\n",
      "672.7272727272727 %\n",
      "673.8636363636364 %\n",
      "675.0 %\n",
      "676.1363636363636 %\n",
      "677.2727272727273 %\n",
      "678.4090909090909 %\n",
      "679.5454545454546 %\n",
      "680.6818181818181 %\n",
      "681.8181818181819 %\n",
      "682.9545454545454 %\n",
      "684.0909090909091 %\n",
      "685.2272727272727 %\n",
      "686.3636363636364 %\n",
      "687.5 %\n",
      "688.6363636363636 %\n",
      "689.7727272727273 %\n",
      "690.9090909090909 %\n",
      "692.0454545454546 %\n",
      "693.1818181818181 %\n",
      "694.3181818181819 %\n",
      "695.4545454545454 %\n",
      "696.5909090909091 %\n",
      "697.7272727272727 %\n",
      "698.8636363636364 %\n",
      "700.0 %\n",
      "701.1363636363636 %\n",
      "702.2727272727273 %\n",
      "703.4090909090909 %\n",
      "704.5454545454546 %\n",
      "705.6818181818181 %\n",
      "706.8181818181819 %\n",
      "707.9545454545454 %\n",
      "709.0909090909091 %\n",
      "710.2272727272727 %\n",
      "711.3636363636364 %\n",
      "712.5 %\n",
      "713.6363636363636 %\n",
      "714.7727272727273 %\n",
      "715.9090909090909 %\n",
      "717.0454545454546 %\n",
      "718.1818181818181 %\n",
      "719.3181818181819 %\n",
      "720.4545454545454 %\n",
      "721.5909090909091 %\n",
      "722.7272727272727 %\n",
      "723.8636363636364 %\n",
      "725.0 %\n",
      "726.1363636363636 %\n",
      "727.2727272727273 %\n",
      "728.4090909090909 %\n",
      "729.5454545454546 %\n",
      "730.6818181818181 %\n",
      "731.8181818181819 %\n",
      "732.9545454545454 %\n",
      "734.0909090909091 %\n",
      "735.2272727272727 %\n",
      "736.3636363636364 %\n",
      "737.5 %\n",
      "738.6363636363636 %\n",
      "739.7727272727273 %\n",
      "740.9090909090909 %\n",
      "742.0454545454546 %\n",
      "743.1818181818181 %\n",
      "744.3181818181819 %\n",
      "745.4545454545454 %\n",
      "746.5909090909091 %\n",
      "747.7272727272727 %\n",
      "748.8636363636364 %\n",
      "750.0 %\n",
      "751.1363636363636 %\n",
      "752.2727272727273 %\n",
      "753.4090909090909 %\n",
      "754.5454545454546 %\n",
      "755.6818181818181 %\n",
      "756.8181818181819 %\n",
      "757.9545454545454 %\n",
      "759.0909090909091 %\n",
      "760.2272727272727 %\n",
      "761.3636363636364 %\n",
      "762.5 %\n",
      "763.6363636363636 %\n",
      "764.7727272727273 %\n",
      "765.9090909090909 %\n",
      "767.0454545454546 %\n",
      "768.1818181818181 %\n",
      "769.3181818181819 %\n",
      "770.4545454545454 %\n",
      "771.5909090909091 %\n",
      "772.7272727272727 %\n",
      "773.8636363636364 %\n",
      "775.0 %\n",
      "776.1363636363636 %\n",
      "777.2727272727273 %\n",
      "778.4090909090909 %\n",
      "779.5454545454546 %\n",
      "780.6818181818181 %\n",
      "781.8181818181819 %\n",
      "782.9545454545454 %\n",
      "784.0909090909091 %\n",
      "785.2272727272727 %\n",
      "786.3636363636364 %\n",
      "787.5 %\n",
      "788.6363636363636 %\n",
      "789.7727272727273 %\n",
      "790.9090909090909 %\n",
      "792.0454545454546 %\n",
      "793.1818181818181 %\n",
      "794.3181818181819 %\n",
      "795.4545454545454 %\n",
      "796.5909090909091 %\n",
      "797.7272727272727 %\n",
      "798.8636363636364 %\n",
      "800.0 %\n",
      "801.1363636363636 %\n",
      "802.2727272727274 %\n",
      "803.4090909090909 %\n",
      "804.5454545454545 %\n",
      "805.6818181818181 %\n",
      "806.8181818181819 %\n",
      "807.9545454545455 %\n",
      "809.0909090909091 %\n",
      "810.2272727272726 %\n",
      "811.3636363636364 %\n",
      "812.5 %\n",
      "813.6363636363636 %\n",
      "814.7727272727274 %\n",
      "815.9090909090909 %\n",
      "817.0454545454545 %\n",
      "818.1818181818181 %\n",
      "819.3181818181819 %\n",
      "820.4545454545455 %\n",
      "821.5909090909091 %\n",
      "822.7272727272726 %\n",
      "823.8636363636364 %\n",
      "825.0 %\n",
      "826.1363636363636 %\n",
      "827.2727272727274 %\n",
      "828.4090909090909 %\n",
      "829.5454545454545 %\n",
      "830.6818181818181 %\n",
      "831.8181818181819 %\n",
      "832.9545454545455 %\n",
      "834.0909090909091 %\n",
      "835.2272727272726 %\n",
      "836.3636363636364 %\n",
      "837.5 %\n",
      "838.6363636363636 %\n",
      "839.7727272727274 %\n",
      "840.9090909090909 %\n",
      "842.0454545454545 %\n",
      "843.1818181818181 %\n",
      "844.3181818181819 %\n",
      "845.4545454545455 %\n",
      "846.5909090909091 %\n",
      "847.7272727272726 %\n",
      "848.8636363636364 %\n",
      "850.0 %\n",
      "851.1363636363636 %\n",
      "852.2727272727274 %\n",
      "853.4090909090909 %\n",
      "854.5454545454545 %\n",
      "855.6818181818181 %\n",
      "856.8181818181819 %\n",
      "857.9545454545455 %\n",
      "859.0909090909091 %\n",
      "860.2272727272726 %\n",
      "861.3636363636364 %\n",
      "862.5 %\n",
      "863.6363636363636 %\n",
      "864.7727272727274 %\n",
      "865.9090909090909 %\n",
      "867.0454545454545 %\n",
      "868.1818181818181 %\n",
      "869.3181818181819 %\n",
      "870.4545454545455 %\n",
      "871.5909090909091 %\n",
      "872.7272727272726 %\n",
      "873.8636363636364 %\n",
      "875.0 %\n",
      "876.1363636363636 %\n",
      "877.2727272727274 %\n",
      "878.4090909090909 %\n",
      "879.5454545454545 %\n",
      "880.6818181818181 %\n",
      "881.8181818181819 %\n",
      "882.9545454545455 %\n",
      "884.0909090909091 %\n",
      "885.2272727272726 %\n",
      "886.3636363636364 %\n",
      "887.5 %\n",
      "888.6363636363636 %\n",
      "889.7727272727274 %\n",
      "890.9090909090909 %\n",
      "892.0454545454545 %\n",
      "893.1818181818181 %\n",
      "894.3181818181819 %\n",
      "895.4545454545455 %\n",
      "896.5909090909091 %\n",
      "897.7272727272726 %\n",
      "898.8636363636364 %\n",
      "900.0 %\n",
      "901.1363636363636 %\n",
      "902.2727272727274 %\n",
      "903.4090909090909 %\n",
      "904.5454545454545 %\n",
      "905.6818181818181 %\n",
      "906.8181818181819 %\n",
      "907.9545454545455 %\n",
      "909.0909090909091 %\n",
      "910.2272727272726 %\n",
      "911.3636363636364 %\n",
      "912.5 %\n",
      "913.6363636363636 %\n",
      "914.7727272727274 %\n",
      "915.9090909090909 %\n",
      "917.0454545454545 %\n",
      "918.1818181818181 %\n",
      "919.3181818181819 %\n",
      "920.4545454545455 %\n",
      "921.5909090909091 %\n",
      "922.7272727272726 %\n",
      "923.8636363636364 %\n",
      "925.0 %\n",
      "926.1363636363636 %\n",
      "927.2727272727274 %\n",
      "928.4090909090909 %\n",
      "929.5454545454545 %\n",
      "930.6818181818181 %\n",
      "931.8181818181819 %\n",
      "932.9545454545455 %\n",
      "934.0909090909091 %\n",
      "935.2272727272726 %\n",
      "936.3636363636364 %\n",
      "937.5 %\n",
      "938.6363636363636 %\n",
      "939.7727272727274 %\n",
      "940.9090909090909 %\n",
      "942.0454545454545 %\n",
      "943.1818181818181 %\n",
      "944.3181818181819 %\n",
      "945.4545454545455 %\n",
      "946.5909090909091 %\n",
      "947.7272727272726 %\n",
      "948.8636363636364 %\n",
      "950.0 %\n",
      "951.1363636363636 %\n",
      "952.2727272727274 %\n",
      "953.4090909090909 %\n",
      "954.5454545454545 %\n",
      "955.6818181818181 %\n",
      "956.8181818181819 %\n",
      "957.9545454545455 %\n",
      "959.0909090909091 %\n",
      "960.2272727272726 %\n",
      "961.3636363636364 %\n",
      "962.5 %\n",
      "963.6363636363636 %\n",
      "964.7727272727274 %\n",
      "965.9090909090909 %\n",
      "967.0454545454545 %\n",
      "968.1818181818181 %\n",
      "969.3181818181819 %\n",
      "970.4545454545455 %\n",
      "971.5909090909091 %\n",
      "972.7272727272726 %\n",
      "973.8636363636364 %\n",
      "975.0 %\n",
      "976.1363636363636 %\n",
      "977.2727272727274 %\n",
      "978.4090909090909 %\n",
      "979.5454545454545 %\n",
      "980.6818181818181 %\n",
      "981.8181818181819 %\n",
      "982.9545454545455 %\n",
      "984.0909090909091 %\n",
      "985.2272727272726 %\n",
      "986.3636363636364 %\n",
      "987.5 %\n",
      "988.6363636363636 %\n",
      "989.7727272727274 %\n",
      "990.9090909090909 %\n",
      "992.0454545454545 %\n",
      "993.1818181818181 %\n",
      "994.3181818181819 %\n",
      "995.4545454545455 %\n",
      "996.5909090909091 %\n",
      "997.7272727272726 %\n",
      "998.8636363636364 %\n",
      "1000.0 %\n",
      "1001.1363636363636 %\n",
      "1002.2727272727274 %\n",
      "1003.4090909090909 %\n",
      "1004.5454545454545 %\n",
      "1005.6818181818181 %\n",
      "1006.8181818181819 %\n",
      "1007.9545454545455 %\n",
      "1009.0909090909091 %\n",
      "1010.2272727272726 %\n",
      "1011.3636363636364 %\n",
      "1012.5 %\n",
      "1013.6363636363636 %\n",
      "1014.7727272727274 %\n",
      "1015.9090909090909 %\n",
      "1017.0454545454545 %\n",
      "1018.1818181818181 %\n",
      "1019.3181818181819 %\n",
      "1020.4545454545455 %\n",
      "1021.5909090909091 %\n",
      "1022.7272727272726 %\n",
      "1023.8636363636364 %\n",
      "1025.0 %\n",
      "1026.1363636363637 %\n",
      "1027.2727272727273 %\n",
      "1028.4090909090908 %\n",
      "1029.5454545454545 %\n",
      "1030.6818181818182 %\n",
      "1031.8181818181818 %\n",
      "1032.9545454545455 %\n",
      "1034.0909090909092 %\n",
      "1035.2272727272727 %\n",
      "1036.3636363636363 %\n",
      "1037.5 %\n",
      "1038.6363636363637 %\n",
      "1039.7727272727273 %\n",
      "1040.9090909090908 %\n",
      "1042.0454545454545 %\n",
      "1043.1818181818182 %\n",
      "1044.3181818181818 %\n",
      "1045.4545454545455 %\n",
      "1046.5909090909092 %\n",
      "1047.7272727272727 %\n",
      "1048.8636363636363 %\n",
      "1050.0 %\n",
      "1051.1363636363637 %\n",
      "1052.2727272727273 %\n",
      "1053.4090909090908 %\n",
      "1054.5454545454545 %\n",
      "1055.6818181818182 %\n",
      "1056.8181818181818 %\n",
      "1057.9545454545455 %\n",
      "1059.0909090909092 %\n",
      "1060.2272727272727 %\n",
      "1061.3636363636363 %\n",
      "1062.5 %\n",
      "1063.6363636363637 %\n",
      "1064.7727272727273 %\n",
      "1065.9090909090908 %\n",
      "1067.0454545454545 %\n",
      "1068.1818181818182 %\n",
      "1069.3181818181818 %\n",
      "1070.4545454545455 %\n",
      "1071.5909090909092 %\n",
      "1072.7272727272727 %\n",
      "1073.8636363636363 %\n",
      "1075.0 %\n",
      "1076.1363636363637 %\n",
      "1077.2727272727273 %\n",
      "1078.4090909090908 %\n",
      "1079.5454545454545 %\n",
      "1080.6818181818182 %\n",
      "1081.8181818181818 %\n",
      "1082.9545454545455 %\n",
      "1084.0909090909092 %\n",
      "1085.2272727272727 %\n",
      "1086.3636363636363 %\n",
      "1087.5 %\n",
      "1088.6363636363637 %\n",
      "1089.7727272727273 %\n",
      "1090.9090909090908 %\n",
      "1092.0454545454545 %\n",
      "1093.1818181818182 %\n",
      "1094.3181818181818 %\n",
      "1095.4545454545455 %\n",
      "1096.5909090909092 %\n",
      "1097.7272727272727 %\n",
      "1098.8636363636363 %\n",
      "1100.0 %\n",
      "1101.1363636363637 %\n",
      "1102.2727272727273 %\n",
      "1103.4090909090908 %\n",
      "1104.5454545454545 %\n",
      "1105.6818181818182 %\n",
      "1106.8181818181818 %\n",
      "1107.9545454545455 %\n",
      "1109.0909090909092 %\n",
      "1110.2272727272727 %\n",
      "1111.3636363636363 %\n",
      "1112.5 %\n",
      "1113.6363636363637 %\n",
      "1114.7727272727273 %\n",
      "1115.9090909090908 %\n",
      "1117.0454545454545 %\n",
      "1118.1818181818182 %\n",
      "1119.3181818181818 %\n",
      "1120.4545454545455 %\n",
      "1121.5909090909092 %\n",
      "1122.7272727272727 %\n",
      "1123.8636363636363 %\n",
      "1125.0 %\n",
      "1126.1363636363637 %\n",
      "1127.2727272727273 %\n",
      "1128.4090909090908 %\n",
      "1129.5454545454545 %\n",
      "1130.6818181818182 %\n",
      "1131.8181818181818 %\n",
      "1132.9545454545455 %\n",
      "1134.0909090909092 %\n",
      "1135.2272727272727 %\n",
      "1136.3636363636363 %\n",
      "1137.5 %\n",
      "1138.6363636363637 %\n",
      "1139.7727272727273 %\n",
      "1140.9090909090908 %\n",
      "1142.0454545454545 %\n",
      "1143.1818181818182 %\n",
      "1144.3181818181818 %\n",
      "1145.4545454545455 %\n",
      "1146.5909090909092 %\n",
      "1147.7272727272727 %\n",
      "1148.8636363636363 %\n",
      "1150.0 %\n",
      "1151.1363636363637 %\n",
      "1152.2727272727273 %\n",
      "1153.4090909090908 %\n",
      "1154.5454545454545 %\n",
      "1155.6818181818182 %\n",
      "1156.8181818181818 %\n",
      "1157.9545454545455 %\n",
      "1159.0909090909092 %\n",
      "1160.2272727272727 %\n",
      "1161.3636363636363 %\n",
      "1162.5 %\n",
      "1163.6363636363637 %\n",
      "1164.7727272727273 %\n",
      "1165.9090909090908 %\n",
      "1167.0454545454545 %\n",
      "1168.1818181818182 %\n",
      "1169.3181818181818 %\n",
      "1170.4545454545455 %\n",
      "1171.5909090909092 %\n",
      "1172.7272727272727 %\n",
      "1173.8636363636363 %\n",
      "1175.0 %\n",
      "1176.1363636363637 %\n",
      "1177.2727272727273 %\n",
      "1178.4090909090908 %\n",
      "1179.5454545454545 %\n",
      "1180.6818181818182 %\n",
      "1181.8181818181818 %\n",
      "1182.9545454545455 %\n",
      "1184.0909090909092 %\n",
      "1185.2272727272727 %\n",
      "1186.3636363636363 %\n",
      "1187.5 %\n",
      "1188.6363636363637 %\n",
      "1189.7727272727273 %\n",
      "1190.9090909090908 %\n",
      "1192.0454545454545 %\n",
      "1193.1818181818182 %\n",
      "1194.3181818181818 %\n",
      "1195.4545454545455 %\n",
      "1196.5909090909092 %\n",
      "1197.7272727272727 %\n",
      "1198.8636363636363 %\n",
      "1200.0 %\n",
      "1201.1363636363637 %\n",
      "1202.2727272727273 %\n",
      "1203.4090909090908 %\n",
      "1204.5454545454545 %\n",
      "1205.6818181818182 %\n",
      "1206.8181818181818 %\n",
      "1207.9545454545455 %\n",
      "1209.0909090909092 %\n",
      "1210.2272727272727 %\n",
      "1211.3636363636363 %\n",
      "1212.5 %\n",
      "1213.6363636363637 %\n",
      "1214.7727272727273 %\n",
      "1215.9090909090908 %\n",
      "1217.0454545454545 %\n",
      "1218.1818181818182 %\n",
      "1219.3181818181818 %\n",
      "1220.4545454545455 %\n",
      "1221.5909090909092 %\n",
      "1222.7272727272727 %\n",
      "1223.8636363636363 %\n",
      "1225.0 %\n",
      "1226.1363636363637 %\n",
      "1227.2727272727273 %\n",
      "1228.4090909090908 %\n",
      "1229.5454545454545 %\n",
      "1230.6818181818182 %\n",
      "1231.8181818181818 %\n",
      "1232.9545454545455 %\n",
      "1234.0909090909092 %\n",
      "1235.2272727272727 %\n",
      "1236.3636363636363 %\n",
      "1237.5 %\n",
      "1238.6363636363637 %\n",
      "1239.7727272727273 %\n",
      "1240.9090909090908 %\n",
      "1242.0454545454545 %\n",
      "1243.1818181818182 %\n",
      "1244.3181818181818 %\n",
      "1245.4545454545455 %\n",
      "1246.5909090909092 %\n",
      "1247.7272727272727 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m axs[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mset_visible(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m fig\u001b[38;5;241m.\u001b[39mtight_layout(pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massets/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msave\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m ad\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mimread(path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massets/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vid_not_started:\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\figure.py:3272\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[1;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[0;32m   3268\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes:\n\u001b[0;32m   3269\u001b[0m         stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[0;32m   3270\u001b[0m             ax\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39m_cm_set(facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m-> 3272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mprint_figure(fname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\backend_bases.py:2338\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2335\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2336\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2337\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m-> 2338\u001b[0m         result \u001b[38;5;241m=\u001b[39m print_method(\n\u001b[0;32m   2339\u001b[0m             filename,\n\u001b[0;32m   2340\u001b[0m             facecolor\u001b[38;5;241m=\u001b[39mfacecolor,\n\u001b[0;32m   2341\u001b[0m             edgecolor\u001b[38;5;241m=\u001b[39medgecolor,\n\u001b[0;32m   2342\u001b[0m             orientation\u001b[38;5;241m=\u001b[39morientation,\n\u001b[0;32m   2343\u001b[0m             bbox_inches_restore\u001b[38;5;241m=\u001b[39m_bbox_inches_restore,\n\u001b[0;32m   2344\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2345\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\backend_bases.py:2204\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2200\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[0;32m   2201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m   2203\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[1;32m-> 2204\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: meth(\n\u001b[0;32m   2205\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m skip}))\n\u001b[0;32m   2206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:410\u001b[0m, in \u001b[0;36mdelete_parameter.<locals>.wrapper\u001b[1;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m     deprecation_addendum \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf any parameter follows \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m, they should be passed as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword, not positionally.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    403\u001b[0m     warn_deprecated(\n\u001b[0;32m    404\u001b[0m         since,\n\u001b[0;32m    405\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrepr\u001b[39m(name),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    408\u001b[0m                  \u001b[38;5;28;01melse\u001b[39;00m deprecation_addendum,\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39minner_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:520\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;129m@_api\u001b[39m\u001b[38;5;241m.\u001b[39mdelete_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    473\u001b[0m               metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:466\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[1;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[0;32m    468\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    469\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:408\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m RendererAgg\u001b[38;5;241m.\u001b[39mlock, \\\n\u001b[0;32m    406\u001b[0m      (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[0;32m    407\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[1;32m--> 408\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\artist.py:74\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 74\u001b[0m     result \u001b[38;5;241m=\u001b[39m draw(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[0;32m     76\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\figure.py:3069\u001b[0m, in \u001b[0;36mFigure.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3066\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m   3068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m-> 3069\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3072\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[0;32m   3073\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[1;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_base.py:3106\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3103\u001b[0m         a\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m   3104\u001b[0m     renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n\u001b[1;32m-> 3106\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3109\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   3110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[1;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\image.py:641\u001b[0m, in \u001b[0;36m_ImageBase.draw\u001b[1;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    639\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mdraw_image(gc, l, b, im, trans)\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 641\u001b[0m     im, l, b, trans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_magnification\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    644\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mdraw_image(gc, l, b, im)\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\image.py:949\u001b[0m, in \u001b[0;36mAxesImage.make_image\u001b[1;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[0;32m    946\u001b[0m transformed_bbox \u001b[38;5;241m=\u001b[39m TransformedBbox(bbox, trans)\n\u001b[0;32m    947\u001b[0m clip \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_clip_box() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mbbox) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_clip_on()\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mbbox)\n\u001b[1;32m--> 949\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_bbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmagnification\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsampled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munsampled\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\image.py:545\u001b[0m, in \u001b[0;36m_ImageBase._make_image\u001b[1;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;66;03m# Block the norm from sending an update signal during the\u001b[39;00m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;66;03m# temporary vmin/vmax change\u001b[39;00m\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mblocked(), \\\n\u001b[0;32m    544\u001b[0m          cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm, vmin\u001b[38;5;241m=\u001b[39ms_vmin, vmax\u001b[38;5;241m=\u001b[39ms_vmax):\n\u001b[1;32m--> 545\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresampled_masked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:  \u001b[38;5;66;03m# _interpolation_stage == 'rgba'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\colors.py:1331\u001b[0m, in \u001b[0;36mNormalize.__call__\u001b[1;34m(self, value, clip)\u001b[0m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1329\u001b[0m     clip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip\n\u001b[1;32m-> 1331\u001b[0m result, is_scalar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvmin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvmax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoscale_None(result)\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\colors.py:1307\u001b[0m, in \u001b[0;36mNormalize.process_value\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m   1305\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mgetmask(value)\n\u001b[0;32m   1306\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(value)\n\u001b[1;32m-> 1307\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, is_scalar\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\ma\\core.py:6635\u001b[0m, in \u001b[0;36marray\u001b[1;34m(data, dtype, copy, order, mask, fill_value, keep_mask, hard_mask, shrink, subok, ndmin)\u001b[0m\n\u001b[0;32m   6625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marray\u001b[39m(data, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   6626\u001b[0m           mask\u001b[38;5;241m=\u001b[39mnomask, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keep_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   6627\u001b[0m           hard_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, shrink\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ndmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m   6628\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6629\u001b[0m \u001b[38;5;124;03m    Shortcut to MaskedArray.\u001b[39;00m\n\u001b[0;32m   6630\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6633\u001b[0m \n\u001b[0;32m   6634\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6635\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMaskedArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6636\u001b[0m \u001b[43m                       \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6637\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mhard_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhard_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6638\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshrink\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshrink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\ma\\core.py:2820\u001b[0m, in \u001b[0;36mMaskedArray.__new__\u001b[1;34m(cls, data, mask, dtype, copy, subok, ndmin, fill_value, keep_mask, hard_mask, shrink, order)\u001b[0m\n\u001b[0;32m   2811\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2812\u001b[0m \u001b[38;5;124;03mCreate a new masked array from scratch.\u001b[39;00m\n\u001b[0;32m   2813\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2817\u001b[0m \n\u001b[0;32m   2818\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2819\u001b[0m \u001b[38;5;66;03m# Process data.\u001b[39;00m\n\u001b[1;32m-> 2820\u001b[0m _data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2821\u001b[0m \u001b[43m                 \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2822\u001b[0m _baseclass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_baseclass\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m(_data))\n\u001b[0;32m   2823\u001b[0m \u001b[38;5;66;03m# Check that we're not erasing the mask.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X,y=genDataSet(0,1,1,1,0) \n",
    "X=X[:,0:15]\n",
    "print(X.shape)\n",
    "Y_pred=ridge_model.predict(X)\n",
    "assert len(Y_pred)==len(X),\"Incorrect sizes\"+str(len(Y_pred))+\" \"+str(len(X))\n",
    "Y_pred[Y_pred<0.5]=0\n",
    "Y_pred[Y_pred>=0.5]=1\n",
    "Y_pred=Y_pred.reshape((len(Y_pred),2,2))\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "vid_not_started=True\n",
    "pathtosave=\"savedEdge.avi\"\n",
    "\n",
    "for _,i in enumerate(range(0,len(X),1)): #len(x)\n",
    "    ft1=get_foot_image(X[_])\n",
    "    pred=Y_pred[_]\n",
    "    axs[0].imshow(ft1.T,cmap='inferno')\n",
    "    axs[0].set_title(\"Left foot\")\n",
    "    axs[0].xaxis.set_visible(False)\n",
    "    axs[0].yaxis.set_visible(False)\n",
    "\n",
    "    axs[1].imshow(pred.T,cmap=\"gist_gray\")\n",
    "    axs[1].set_title(\"Prediction\")\n",
    "    axs[1].xaxis.set_visible(False)\n",
    "    axs[1].yaxis.set_visible(False)\n",
    "    fig.tight_layout(pad=0.05)\n",
    "    fig.savefig(path+\"assets/\"+\"save\"+\".png\")\n",
    "    ad=cv2.imread(path+\"assets/\"+\"save\"+\".png\")\n",
    "    if vid_not_started:\n",
    "        h1, w1 = ad.shape[:2]\n",
    "        out = cv2.VideoWriter(path+pathtosave,cv2.VideoWriter_fourcc(*'DIVX'), 15, (w1,h1))\n",
    "        vid_not_started=False\n",
    "    out.write(ad) #uncomment to record video\n",
    "    if _%5000==0:\n",
    "        print(_/len(X) *100,\"%\")\n",
    "out.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
